{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960786db-0311-42aa-b0ca-892c0b1d4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "        \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\",\n",
    "        \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "        \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\",\n",
    "        \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "        \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"co\",\n",
    "        \"con\", \"could\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\",\n",
    "        \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "        \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\",\n",
    "        \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\",\n",
    "        \"has\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
    "        \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\n",
    "        \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n",
    "        \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\",\n",
    "        \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"nevertheless\", \"next\", \"nine\", \"nobody\", \"now\", \"nowhere\",\n",
    "        \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
    "        \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n",
    "        \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
    "        \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\",\n",
    "        \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "        \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\",\n",
    "        \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\",\n",
    "        \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\",\n",
    "        \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
    "        \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\",\n",
    "        \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9827c547-7d44-4d16-b20c-81cae80eaea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def predict_classwise_stance_emergent(P_relatedness, P_stance, stance_labels):\n",
    "\n",
    "    P_related = torch.reshape(P_relatedness[:, 0], (-1, 1))\n",
    "    P_unrelated = torch.reshape(P_relatedness[:, 1], (-1, 1))\n",
    "    \n",
    "    tmp1 = P_stance[:,:3]\n",
    "    tmp2 = torch.reshape(torch.sum(tmp1,dim=1),[-1,1])\n",
    "    tmp3 = torch.cat([tmp2,tmp2,tmp2],dim=1)\n",
    "    tmp4 = torch.cat([P_related, P_related, P_related],dim=1)\n",
    "    tmp5 = torch.div(tmp1,tmp3)\n",
    "    tmp6 = tmp5*tmp4\n",
    "    prob = torch.cat([tmp6,P_unrelated],1)#tmp6\n",
    "    \n",
    "    target_labels = torch.argmax(torch.abs(stance_labels), 1)\n",
    "    predict_labels = torch.argmax(prob, 1)\n",
    "\n",
    "\n",
    "    \n",
    "    agree_true = 0\n",
    "    agree_total = 0\n",
    "    \n",
    "    disagree_true = 0\n",
    "    disagree_total = 0\n",
    "    \n",
    "    discuss_true = 0\n",
    "    discuss_total = 0\n",
    "    \n",
    "    unrelated_true = 0\n",
    "    unrelated_total = 0\n",
    "    \n",
    "    for idx, true_label in enumerate(target_labels):\n",
    "        predict_label = predict_labels[idx] \n",
    "        if true_label == 0:\n",
    "            agree_total += 1\n",
    "            if predict_label == 0:\n",
    "                agree_true += 1\n",
    "        elif true_label == 1:\n",
    "            disagree_total += 1\n",
    "            if predict_label == 1:\n",
    "                disagree_true += 1\n",
    "        elif true_label == 2:\n",
    "            discuss_total += 1\n",
    "            if predict_label == 2:\n",
    "                discuss_true += 1\n",
    "        elif true_label == 3:\n",
    "            unrelated_total += 1\n",
    "            if predict_label == 3:\n",
    "                unrelated_true += 1\n",
    "    accuracy_agree = 0\n",
    "    if agree_true != 0:\n",
    "        accuracy_agree = agree_true/agree_total\n",
    "        \n",
    "    accuracy_disagree = 0\n",
    "    if disagree_true != 0:\n",
    "        accuracy_disagree = disagree_true/disagree_total\n",
    "    \n",
    "    accuracy_discuss = 0\n",
    "    if discuss_true != 0:\n",
    "        accuracy_discuss = discuss_true/discuss_total\n",
    "        \n",
    "    accuracy_notrelated = 0\n",
    "    if unrelated_true != 0:\n",
    "        accuracy_notrelated = unrelated_true/unrelated_total\n",
    "        \n",
    "    true_predict_count = len((torch.eq(predict_labels, target_labels)).nonzero().flatten())\n",
    "    accuracy = true_predict_count / len(predict_labels)\n",
    "    \n",
    "    true_total = agree_true + disagree_true + discuss_true + unrelated_true\n",
    "    #print(\"*************\")\n",
    "    #print(\"Total True\")\n",
    "    #print(true_total)\n",
    "    \n",
    "    return accuracy, accuracy_agree, accuracy_disagree, accuracy_discuss, accuracy_notrelated, true_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ca074-c180-4ddf-99c6-873157732d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_train(df, dfVal, dfTest, lim_unigram):\n",
    "    \n",
    "    id_ref = {}\n",
    "    claims_t = []    \n",
    "    headlines_t = []\n",
    "    headlines_track_t = {}\n",
    "    \n",
    "    for index, instance in df.iterrows():\n",
    "        claim = instance['claimHeadline']\n",
    "\n",
    "    \n",
    "    for index, instance in df.iterrows():\n",
    "        claim = instance['claimHeadline']      \n",
    "        headline = instance['articleHeadline']\n",
    "             \n",
    "        if headline not in headlines_track_t:\n",
    "            headlines_t.append(headline)\n",
    "            headlines_track_t[headline] = 1\n",
    "        if claim not in claims_t:\n",
    "            claims_t.append(claim)\n",
    "            \n",
    "    claims_v = []\n",
    "    headlines_v = []\n",
    "    headlines_track_v = {}\n",
    "    for index, instance in dfVal.iterrows():\n",
    "        claim = instance['claimHeadline']        \n",
    "        headline = instance['articleHeadline']\n",
    "        \n",
    "        if headline not in headlines_track_v:\n",
    "            headlines_v.append(headline)\n",
    "            headlines_track_v[headline] = 1\n",
    "        if claim not in claims_v:\n",
    "            claims_v.append(claim)            \n",
    "\n",
    "      \n",
    "    ####################\n",
    "            \n",
    "    claims_test = []  \n",
    "    headlines_test = []\n",
    "    headlines_track_test = {}\n",
    "    for index, instance in dfTest.iterrows():\n",
    "        claim = instance['claimHeadline']\n",
    "        headline = instance['articleHeadline']\n",
    "            \n",
    "        if headline not in headlines_track_test:\n",
    "            headlines_test.append(headline)\n",
    "            headlines_track_test[headline] = 1\n",
    "        \n",
    "        if claim not in claims_test:\n",
    "            claims_test.append(claim)\n",
    "\n",
    "            \n",
    "        # Create reference dictionary\n",
    "    for i, elem in enumerate(headlines_t + claims_t):\n",
    "        id_ref[elem] = i\n",
    "\n",
    "        \n",
    "    # Create vectorizers and BOW and TF arrays for train set\n",
    "    bow_vectorizer = CountVectorizer(max_features=lim_unigram, stop_words=stop_words)\n",
    "    bow = bow_vectorizer.fit_transform(claims_t + headlines_t)  # Train set only\n",
    "    \n",
    "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
    "    tfreq = tfreq_vectorizer.transform(bow).toarray()  # Train set only\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=lim_unigram, stop_words=stop_words).\\\n",
    "        fit(headlines_t + claims_t + headlines_v + claims_v + headlines_test + claims_test)  # Train and test sets\n",
    "    \n",
    "    ####################\n",
    "    \n",
    "    \n",
    "    headline_tfidf_track = {}\n",
    "    claim_tfidf_track = {}\n",
    "    headline_all = []\n",
    "    claim_all = []\n",
    "    cos_track = {}\n",
    "    \n",
    "    train_set = []\n",
    "    print(len(df))\n",
    "    \n",
    "    for index, instance in df.iterrows():\n",
    "        headline = instance['articleHeadline']\n",
    "        claim = instance['claimHeadline']\n",
    "        \n",
    "        headline_all.append(headline)\n",
    "        claim_all.append(claim)\n",
    "        \n",
    "        headline_tf = tfreq[id_ref[headline]].reshape(1, -1)\n",
    "        claim_tf = tfreq[id_ref[claim]].reshape(1, -1)\n",
    "        \n",
    "        if headline not in headline_tfidf_track:\n",
    "            head_tfidf = tfidf_vectorizer.transform([headline]).toarray()\n",
    "            headline_tfidf_track[headline] = head_tfidf\n",
    "        else:\n",
    "            head_tfidf = headline_tfidf_track[headline]\n",
    "            \n",
    "        if claim not in claim_tfidf_track:\n",
    "            claim_tfidf = tfidf_vectorizer.transform([claim]).toarray()\n",
    "            claim_tfidf_track[claim] = claim_tfidf\n",
    "        else:\n",
    "            claim_tfidf = claim_tfidf_track[claim]\n",
    "            \n",
    "        if (headline, claim) not in cos_track:\n",
    "            tfidf_cos = cosine_similarity(head_tfidf, claim_tfidf)[0].reshape(1, 1)\n",
    "            cos_track[(headline, claim)] = tfidf_cos\n",
    "        else:\n",
    "            tfidf_cos = cos_track[(headline, claim)]\n",
    "\n",
    "        feat_vec = np.squeeze(np.c_[headline_tf, claim_tf, tfidf_cos])\n",
    "        train_set.append(feat_vec)\n",
    "\n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, headline_all, claim_all, 'features_fnc_onlytrain/train_overlap.npy')\n",
    "    X_refuting = gen_or_load_feats(refuting_features, headline_all, claim_all, 'features_fnc_onlytrain/train_refuting.npy')\n",
    "    X_polarity = gen_or_load_feats(polarity_features, headline_all, claim_all, 'features_fnc_onlytrain/train_polarity.npy')\n",
    "    X_hand = gen_or_load_feats(hand_features, headline_all, claim_all, 'features_fnc_onlytrain/train_hand.npy')\n",
    "    \n",
    "    train_features = np.squeeze(np.c_[X_refuting, X_polarity,X_overlap,X_hand])#\n",
    "    train_set = np.squeeze(np.c_[train_set,train_features])\n",
    "    ####### preprocessing\n",
    "    train_set = np.asarray(train_set)\n",
    "    train_mean = np.mean(train_set, axis = 0)\n",
    "    train_set = train_set-train_mean\n",
    "    \n",
    "    return train_set, train_mean, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4fdb8-9eb5-471a-8238-5d09dbff8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_val(dfVal, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m):\n",
    "    \n",
    "    headlines_track = {}\n",
    "    claims_track = {}\n",
    "    \n",
    "    headline_all = []\n",
    "    claim_all = []\n",
    "    cos_track = {}\n",
    "    \n",
    "    test_set = []\n",
    "    \n",
    "    for index, instance in dfVal.iterrows():\n",
    "        headline = instance['articleHeadline']\n",
    "        claim = instance['claimHeadline']\n",
    "        headline_all.append(headline)\n",
    "        claim_all.append(claim)\n",
    "        \n",
    "        if headline not in headlines_track:\n",
    "            head_bow = bow_vectorizer.transform([headline]).toarray()\n",
    "            headline_tf = tfreq_vectorizer.transform(head_bow).toarray()[0].reshape(1, -1)\n",
    "            headline_tfidf = tfidf_vectorizer.transform([headline]).toarray().reshape(1, -1)\n",
    "            headlines_track[headline] = (headline_tf, headline_tfidf)\n",
    "        else:\n",
    "            headline_tf = headlines_track[headline][0]\n",
    "            headline_tfidf = headlines_track[headline][1]\n",
    "            \n",
    "        if claim not in claims_track:\n",
    "            claim_bow = bow_vectorizer.transform([claim]).toarray()\n",
    "            claim_tf = tfreq_vectorizer.transform(claim_bow).toarray()[0].reshape(1, -1)\n",
    "            claim_tfidf = tfidf_vectorizer.transform([claim]).toarray().reshape(1, -1)\n",
    "            claims_track[headline] = (claim_tf, claim_tfidf)\n",
    "        else:\n",
    "            claim_tf = claims_track[claim][0]\n",
    "            claim_tfidf = claims_track[claim][1]\n",
    "            \n",
    "        if (headline, claim) not in cos_track:\n",
    "            tfidf_cos = cosine_similarity(headline_tfidf, claim_tfidf)[0].reshape(1, 1)\n",
    "            cos_track[(headline, claim)] = tfidf_cos\n",
    "        else:\n",
    "            tfidf_cos = cos_track[(headline, claim)]\n",
    "            \n",
    "        feat_vec = np.squeeze(np.c_[headline_tf, claim_tf, tfidf_cos])\n",
    "        test_set.append(feat_vec)\n",
    "        \n",
    "        \n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, headline_all, claim_all, 'features_fnc_onlytrain/val_overlap.npy')\n",
    "    X_refuting = gen_or_load_feats(refuting_features, headline_all, claim_all, 'features_fnc_onlytrain/val_refuting.npy')\n",
    "    X_polarity = gen_or_load_feats(polarity_features, headline_all, claim_all, 'features_fnc_onlytrain/val_polarity.npy')\n",
    "    X_hand = gen_or_load_feats(hand_features, headline_all, claim_all, 'features_fnc_onlytrain/val_hand.npy')\n",
    "    \n",
    "    test_features = np.squeeze(np.c_[X_refuting,X_polarity,X_overlap,X_hand])#\n",
    "    test_set = np.squeeze(np.c_[test_set, test_features])\n",
    "    ####### preprocessing\n",
    "    test_set = np.asarray(test_set)\n",
    "    test_set = (test_set-m)#/(std+0.0001)\n",
    "    \n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625786db-c379-45ee-ba7d-d20832c4850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_test(dfTest, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m):\n",
    "    \n",
    "    headlines_track = {}\n",
    "    claims_track = {}\n",
    "    \n",
    "    headline_all = []\n",
    "    claim_all = []\n",
    "    cos_track = {}\n",
    "    \n",
    "    test_set = []\n",
    "    \n",
    "    for index, instance in dfTest.iterrows():\n",
    "        headline = instance['articleHeadline']\n",
    "        claim = instance['claimHeadline']\n",
    "        headline_all.append(headline)\n",
    "        claim_all.append(claim)\n",
    "        \n",
    "        if headline not in headlines_track:\n",
    "            head_bow = bow_vectorizer.transform([headline]).toarray()\n",
    "            headline_tf = tfreq_vectorizer.transform(head_bow).toarray()[0].reshape(1, -1)\n",
    "            headline_tfidf = tfidf_vectorizer.transform([headline]).toarray().reshape(1, -1)\n",
    "            headlines_track[headline] = (headline_tf, headline_tfidf)\n",
    "        else:\n",
    "            headline_tf = headlines_track[headline][0]\n",
    "            headline_tfidf = headlines_track[headline][1]\n",
    "            \n",
    "        if claim not in claims_track:\n",
    "            claim_bow = bow_vectorizer.transform([claim]).toarray()\n",
    "            claim_tf = tfreq_vectorizer.transform(claim_bow).toarray()[0].reshape(1, -1)\n",
    "            claim_tfidf = tfidf_vectorizer.transform([claim]).toarray().reshape(1, -1)\n",
    "            claims_track[headline] = (claim_tf, claim_tfidf)\n",
    "        else:\n",
    "            claim_tf = claims_track[claim][0]\n",
    "            claim_tfidf = claims_track[claim][1]\n",
    "            \n",
    "        if (headline, claim) not in cos_track:\n",
    "            tfidf_cos = cosine_similarity(headline_tfidf, claim_tfidf)[0].reshape(1, 1)\n",
    "            cos_track[(headline, claim)] = tfidf_cos\n",
    "        else:\n",
    "            tfidf_cos = cos_track[(headline, claim)]\n",
    "            \n",
    "        feat_vec = np.squeeze(np.c_[headline_tf, claim_tf, tfidf_cos])\n",
    "        test_set.append(feat_vec)\n",
    "        \n",
    "        \n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, headline_all, claim_all, 'features_fnc_onlytrain/test_overlap.npy')\n",
    "    X_refuting = gen_or_load_feats(refuting_features, headline_all, claim_all, 'features_fnc_onlytrain/test_refuting.npy')\n",
    "    X_polarity = gen_or_load_feats(polarity_features, headline_all, claim_all, 'features_fnc_onlytrain/test_polarity.npy')\n",
    "    X_hand = gen_or_load_feats(hand_features, headline_all, claim_all, 'features_fnc_onlytrain/test_hand.npy')\n",
    "    \n",
    "    test_features = np.squeeze(np.c_[X_refuting,X_polarity,X_overlap,X_hand])#\n",
    "    test_set = np.squeeze(np.c_[test_set, test_features])\n",
    "    ####### preprocessing\n",
    "    test_set = np.asarray(test_set)\n",
    "    test_set = (test_set-m)#/(std+0.0001)\n",
    "    \n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b0607d-f12b-4732-8835-f06f767ff08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_stance_fnc(path):\n",
    "    df = pd.read_csv(path, delimiter='\\t', header = 0, names=['BodyID', 'claimHeadline', 'articleHeadline', 'stance'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0f9a41-e8aa-47a2-a69c-ee5d9dc7a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fnc(labels):\n",
    "    t_relatedness = []\n",
    "    t_stance = []\n",
    "    \n",
    "    t_mmd_symbol = []\n",
    "    t_mmd_symbol_ = []\n",
    "    \n",
    "    #print(labels.shape)\n",
    "    for label in labels:\n",
    "        if label == 3: #unrelated\n",
    "            t_relatedness.append([0,1])\n",
    "            t_stance.append([0,0,0,-1])\n",
    "            t_mmd_symbol.append(0)\n",
    "            t_mmd_symbol_.append(1)\n",
    "        elif label == 1: #disagree\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([0,1,0,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "        elif label == 0: #agree\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([1,0,0,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)       \n",
    "        elif label == 2: #discuss\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([0,0,1,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "\n",
    "    t_relatedness = torch.as_tensor(t_relatedness, dtype=torch.int32)\n",
    "    t_stance = torch.as_tensor(t_stance, dtype=torch.int32)\n",
    "    \n",
    "    t_mmd_symbol  = torch.as_tensor(t_mmd_symbol, dtype=torch.float32)\n",
    "    t_mmd_symbol_ = torch.as_tensor(t_mmd_symbol_, dtype=torch.float32)\n",
    "    \n",
    "    return t_relatedness, t_stance, t_mmd_symbol, t_mmd_symbol_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5127a012-afa6-4f23-baa3-177075b20f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fnc_test(labels):\n",
    "    t_relatedness = []\n",
    "    t_stance = []\n",
    "    \n",
    "    t_mmd_symbol = []\n",
    "    t_mmd_symbol_ = []\n",
    "    \n",
    "    #print(labels.shape)\n",
    "    for label in labels:\n",
    "        if label == 3: #unrelated\n",
    "            t_relatedness.append([0,1])\n",
    "            t_stance.append([0,0,0,1])\n",
    "            t_mmd_symbol.append(0)\n",
    "            t_mmd_symbol_.append(1)\n",
    "        elif label == 1: #disagree\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([0,1,0,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "        elif label == 0: #agree\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([1,0,0,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)       \n",
    "        elif label == 2: #discuss\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([0,0,1,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "\n",
    "    t_relatedness = torch.as_tensor(t_relatedness, dtype=torch.int32)\n",
    "    t_stance = torch.as_tensor(t_stance, dtype=torch.int32)\n",
    "    \n",
    "    t_mmd_symbol  = torch.as_tensor(t_mmd_symbol, dtype=torch.float32)\n",
    "    t_mmd_symbol_ = torch.as_tensor(t_mmd_symbol_, dtype=torch.float32)\n",
    "    \n",
    "    return t_relatedness, t_stance, t_mmd_symbol, t_mmd_symbol_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b4e2b5-9e00-4ad2-b611-455e45f51510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the datasets with the different fields.\n",
    "def generate_datasets_fnc(df):\n",
    "    \n",
    "    sentencesQuery = df.Q.values #claim in this case\n",
    "    labels = df.stance.values\n",
    "\n",
    "    sentencesCont = df.docCont.values\n",
    "\n",
    "    #*****Experiment 1a: Title and docCont: maxLen - 64 - different with the default case\n",
    "    sentencesQueryCont = concanListStringsLonger(sentencesQuery, sentencesCont)\n",
    "\n",
    "    return sentencesQueryCont, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b8ae1-2913-46f6-b232-b5d8148d79b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, RobertaModel, DistilBertModel\n",
    "class StanceDetectionUnigramClass(torch.nn.Module):\n",
    "    def __init__(self, datasetUsed):\n",
    "        super(StanceDetectionUnigramClass, self).__init__()\n",
    "        input_size = len(datasetUsed[0])\n",
    "        hidden_size_initial = 100\n",
    "        hidden_size = 100\n",
    "        mmd_size = 10\n",
    "        dropout_prob = 0.6\n",
    "        dropout_prob2 = 0.6\n",
    "        relatedness_size = 2\n",
    "        classes_size = 4\n",
    "        ideology_class_size = 3\n",
    "        #agreement_size = 3\n",
    "        #self.input_pl = BertForPreTraining.from_pretrained(modelUsed) #input\n",
    "        #self.input_pl = BertModel.from_pretrained(modelUsed)\n",
    "        #self.input_pl = RobertaModel.from_pretrained(modelUsed)\n",
    "        #self.input_pl = DistilBertModel.from_pretrained(modelUsed)\n",
    "        self.l1 = torch.nn.Linear(input_size, hidden_size_initial)\n",
    "        self.l2 = torch.nn.Linear(hidden_size_initial, hidden_size)\n",
    "        self.l3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn1_hidden = torch.nn.BatchNorm1d(hidden_size_initial, momentum=0.05)\n",
    "        self.bn2_hidden = torch.nn.BatchNorm1d(hidden_size, momentum=0.05)\n",
    "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout_prob2)\n",
    "\n",
    "        self.theta_d = torch.nn.Linear(hidden_size, mmd_size)\n",
    "        self.bn1_theta = torch.nn.BatchNorm1d(mmd_size, momentum=0.05)\n",
    "        \n",
    "        self.probability = torch.nn.Linear(hidden_size, relatedness_size)\n",
    "        self.output_prob = torch.nn.Softmax(dim = 1)\n",
    "        \n",
    "        self.stance = torch.nn.Linear(hidden_size + relatedness_size - 1, classes_size)\n",
    "        self.ideology = torch.nn.Linear(hidden_size + classes_size - 2, ideology_class_size)\n",
    "        \n",
    "        #for param in self.input_pl.embeddings.parameters():\n",
    "            #param.requires_grad = False\n",
    "        \n",
    "        #for param in self.input_pl[2][0:5].parameters():\n",
    "            #param.requires_grad = False\n",
    "\n",
    "        #self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, mmd_pl, mmd_pl_):\n",
    "        relatedness_size = 2\n",
    "        classes_size = 4\n",
    "        ideology_class_size = 3\n",
    "        \n",
    "        #hidden layer\n",
    "        hidden_state = self.l1(input_ids)\n",
    "        hidden_state_normalized = self.bn1_hidden(hidden_state)\n",
    "        hidden_state_normalized = torch.nn.ReLU()(hidden_state_normalized)\n",
    "        hidden_layer= self.dropout2(hidden_state_normalized)\n",
    "    \n",
    "        #mmd layer        \n",
    "        theta_d = self.theta_d(hidden_layer)\n",
    "        theta_d_normalized = self.bn1_theta(theta_d)\n",
    "        theta_d_normalized = torch.nn.ReLU()(theta_d_normalized)\n",
    "        theta_d_layer= self.dropout2(theta_d_normalized)\n",
    "        \n",
    "        \n",
    "        n1 = torch.sum(mmd_pl, dim = 0) + 1e-10\n",
    "        n2 = torch.sum(mmd_pl_, dim = 0)  + 1e-10\n",
    "        aa = torch.reshape(mmd_pl, (-1,1))\n",
    "        bb = torch.reshape(mmd_pl_, (-1,1))\n",
    "        \n",
    "        #calculate mmd_loss                  \n",
    "        d1 = torch.div(torch.sum(torch.mul(theta_d_layer, aa), dim=1), n1)\n",
    "        d2 = torch.div(torch.sum(torch.mul(theta_d_layer, bb), dim=1), n2)\n",
    "                             \n",
    "        mmd_loss = torch.sum(d1 - d2)\n",
    "\n",
    "        #probability layer\n",
    "        relatedness_state = self.probability(hidden_layer)\n",
    "        relatedness_flat = self.dropout2(relatedness_state)\n",
    "        \n",
    "        relatedness_flat_reshaped = torch.reshape(relatedness_flat, (-1, relatedness_size))\n",
    "        P_relatedness = self.output_prob(relatedness_flat_reshaped)\n",
    "        #P_relatedness = relatedness_flat_reshaped\n",
    "        \n",
    "        P_related = torch.reshape(P_relatedness[:, 0], (-1, 1))\n",
    "        P_unrelated = torch.reshape(P_relatedness[:, 1], (-1, 1))\n",
    "        \n",
    "        #stance layer\n",
    "        concat_fea = torch.cat([hidden_layer, P_related], dim = 1)\n",
    "        stance_state = self.stance(concat_fea) #batch size x classes_size\n",
    "        stance_flat = self.dropout2(stance_state) #batch size x classes_size\n",
    "        \n",
    "        stance_flat_reshaped = torch.reshape(stance_flat, (-1, classes_size))\n",
    "        P_stance = self.output_prob(stance_flat_reshaped) \n",
    "        \n",
    "\n",
    "        return mmd_loss, P_relatedness, P_stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee10c2c-088e-4cd1-9d59-e8da99285a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "def prepare_for_training_stance_emergent_unigram(instancesTrain, labelsTrain, instancesVal, labelsVal, modelUsed, batch_size=16, epochs = 50, num_warmup_steps=0, learning_rate=5e-5):\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "\n",
    "    from transformers import BertForSequenceClassification, AdamW, BertConfig, RobertaConfig, AutoModelWithLMHead\n",
    "    from transformers import DistilBertForSequenceClassification, RobertaForSequenceClassification\n",
    "    \n",
    "    from torch.utils.data import DataLoader, RandomSampler\n",
    "    \n",
    "    \n",
    "    t_train_relatedness, t_train_stance, t_train_mmd_symbol, t_train_mmd_symbol_ = preprocess_fnc(labelsTrain)\n",
    "    t_instancesTrain  = torch.as_tensor(instancesTrain, dtype=torch.float32)\n",
    "    datasetTrain = TensorDataset(t_instancesTrain, t_train_relatedness, t_train_stance, t_train_mmd_symbol, t_train_mmd_symbol_)\n",
    "\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "    t_val_relatedness, t_val_stance, t_val_mmd_symbol, t_val_mmd_symbol_ = preprocess_fnc_test(labelsVal)\n",
    "    t_instancesVal = torch.as_tensor(instancesVal, dtype=torch.float32)\n",
    "    datasetVal = TensorDataset(t_instancesVal, t_val_relatedness, t_val_stance, t_val_mmd_symbol, t_val_mmd_symbol_)\n",
    "    \n",
    "    \n",
    "    model = StanceDetectionUnigramClass(instancesTrain)\n",
    "    \n",
    "\n",
    "    #print(model)\n",
    "    # Tell pytorch to run this model on the GPU.\n",
    "    #model.cuda()\n",
    "\n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "    # I believe the 'W' stands for 'Weight Decay fix\"\n",
    "    \n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  betas=(0.9, 0.999), \n",
    "                  eps=1e-08, \n",
    "                  weight_decay=1e-5,\n",
    "                  correct_bias=True\n",
    "               )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "            datasetTrain,  # The training samples.\n",
    "            sampler =  RandomSampler(datasetTrain), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size., \n",
    "            num_workers=8\n",
    "        )\n",
    "    batch_size = batch_size\n",
    "\n",
    "\n",
    "    from transformers import get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "    # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "    # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "    # training data.\n",
    "    epochs = epochs\n",
    "\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    schedulerOld = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "    \n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps, num_cycles = 5)\n",
    "    \n",
    "    return model, datasetTrain, datasetVal, optimizer, schedulerOld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e698197-65b9-4193-9450-d2109be92e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "#from tensorboardX import SummaryWriter\n",
    "\n",
    "#import EarlyStopping\n",
    "def onlytrain_stance_emergent_unigram(model_save_path, model, tokenizer, datasetTrain, datasetVal, epochs, batch_size, optimizer, scheduler, patience, verbose, delta, seedVal, continue_train = False):\n",
    "    writer = SummaryWriter()\n",
    "    min_val_loss = 100\n",
    "    max_val_acc = 0\n",
    "    \n",
    "    relatedness_size = 2\n",
    "    classes_size = 4\n",
    "    loss_fct_relatedness = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    loss_fct_stance = torch.nn.CrossEntropyLoss()\n",
    "    #loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    alpha = 1.5\n",
    "    beta = 1e-3\n",
    "    theta = 0\n",
    "    gamma = 0\n",
    "    \n",
    "    batch_size_max_once = batch_size\n",
    "\n",
    "    if batch_size < batch_size_max_once:\n",
    "        batch_size_max_once = batch_size\n",
    "        \n",
    "    accumulation_steps = batch_size/batch_size_max_once\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    train_dataloader, validation_dataloader = return_batches_datasets(datasetTrain, datasetVal, batch_size_max_once)\n",
    "    \n",
    "    epoch_start = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        #multi-gpu\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            \n",
    "    print(device)\n",
    "    \n",
    "    \n",
    "            \n",
    "    if continue_train:\n",
    "        checkpoint = torch.load('./model_save/fnc/model_fncbert.t7')\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    \n",
    "     #pos_weight=torch.FloatTensor ([28.36 / 0.5090]\n",
    "    \n",
    "     #pos_weight = torch.tensor([1.0, 1.0, 1.0])\n",
    "     #pos_weight = pos_weight.to(device)\n",
    "     #criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    weights_ideology = torch.tensor([10, 5, 1.5]).to(device)   \n",
    "    weights_stance = torch.tensor([0.82, 0.85, 0.67, 0.3]).to(device) \n",
    "    loss_fct_relatedness_weighted = torch.nn.BCEWithLogitsLoss(pos_weight = weights_stance)\n",
    "    \n",
    "    # For each epoch...\n",
    "    batch_epoch_count = 1\n",
    "    for epoch_i in range(epoch_start, epoch_start + epochs):\n",
    "        \n",
    "        print(\"---------Epoch----------\" + str(epoch_i))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "    \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        #print(\"\")\n",
    "        #print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        #print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        # For each batch of training data...\n",
    "        mini_batch_avg_loss = 0\n",
    "        #train_size = len(train_dataloader)\n",
    "        \n",
    "        if batch_epoch_count % 500 == 0:\n",
    "            batch_size = batch_size*2\n",
    "            accumulation_steps = int(batch_size/batch_size_max_once)\n",
    "        batch_epoch_count = batch_epoch_count + 1\n",
    "\n",
    "        #train_size = len(train_dataloader) / float(accumulation_steps)\n",
    "        \n",
    "        print(\"Batch Size: \" + str(batch_size))\n",
    "        print(float(accumulation_steps))\n",
    "        \n",
    "        #print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            with torch.autograd.detect_anomaly():\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_relatedness = batch[1].to(device)\n",
    "                b_labels = batch[2].to(device)\n",
    "                b_mmd_symbol = batch[3].to(device)\n",
    "                b_mmd_symbol_ = batch[4].to(device)\n",
    "        \n",
    "            \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "            \n",
    "\n",
    "                mmd_loss, P_relatedness, P_stance = model(input_ids = b_input_ids, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "\n",
    "            \n",
    "                relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "                stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "\n",
    "            \n",
    "                loss = relatedness_loss + alpha * stance_loss - beta * mmd_loss\n",
    "                total_train_loss += loss.item()\n",
    "            \n",
    "            \n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                #for parameter in model.parameters(): print(parameter.grad.norm())\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "                \n",
    "                # Update parameters and take a step using the computed gradient.\n",
    "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "                # modified based on their gradients, the learning rate, etc.\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the learning rate.\n",
    "                scheduler.step()\n",
    "                \n",
    "                #for param_group in optimizer.param_groups:\n",
    "                #print(\"Learning Rate: \", optimizer.param_groups[\"lr\"])\n",
    "                \n",
    "                                \n",
    "                # Always clear any previously calculated gradients before performing a\n",
    "                # backward pass. PyTorch doesn't do this automatically because \n",
    "                # accumulating the gradients is \"convenient while training RNNs\". \n",
    "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                #total_train_loss = 0\n",
    "\n",
    "                #print(\"Iter: %02d, Loss: %4.4f\" % (step, mini_batch_avg_loss))                \n",
    "    \n",
    "        print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        #print(\"\")\n",
    "        print(\"  Average training loss: {0:.6f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "        model_save_state = {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "        \n",
    "        torch.save(model_save_state, model_save_path)\n",
    "\n",
    "    last_epoch = 1\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "    return training_stats, last_epoch, min_val_loss, max_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfec38f-5d24-4aee-8bdb-bf8597c31c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "#from tensorboardX import SummaryWriter\n",
    "\n",
    "#import EarlyStopping\n",
    "def train_stance_emergent_unigram(model_save_path, model, tokenizer, datasetTrain, datasetVal, epochs, batch_size, optimizer, scheduler, patience, verbose, delta, seedVal, continue_train = False):\n",
    "    writer = SummaryWriter()\n",
    "    min_val_loss = 100\n",
    "    \n",
    "    relatedness_size = 2\n",
    "    classes_size = 4\n",
    "    loss_fct_relatedness = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    loss_fct_stance = torch.nn.CrossEntropyLoss()\n",
    "    #loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    alpha = 1.5\n",
    "    beta = 1e-3\n",
    "    theta = 0\n",
    "    gamma = 0\n",
    "    \n",
    "    batch_size_max_once = batch_size\n",
    "\n",
    "    if batch_size < batch_size_max_once:\n",
    "        batch_size_max_once = batch_size\n",
    "        \n",
    "    accumulation_steps = batch_size/batch_size_max_once\n",
    "    \n",
    "    es = EarlyStopping(patience,verbose, delta)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    train_dataloader, validation_dataloader = return_batches_datasets(datasetTrain, datasetVal, batch_size_max_once)\n",
    "    \n",
    "    epoch_start = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        #multi-gpu\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            \n",
    "    print(device)\n",
    "    \n",
    "    \n",
    "            \n",
    "    if continue_train:\n",
    "        checkpoint = torch.load('./model_save/fnc/model_fncbert.t7')\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    \n",
    "     #pos_weight=torch.FloatTensor ([28.36 / 0.5090]\n",
    "    \n",
    "     #pos_weight = torch.tensor([1.0, 1.0, 1.0])\n",
    "     #pos_weight = pos_weight.to(device)\n",
    "     #criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    weights_ideology = torch.tensor([10, 5, 1.5]).to(device)   \n",
    "    weights_stance = torch.tensor([0.82, 0.85, 0.67, 0.3]).to(device) \n",
    "    loss_fct_relatedness_weighted = torch.nn.BCEWithLogitsLoss(pos_weight = weights_stance)\n",
    "    \n",
    "    # For each epoch...\n",
    "    batch_epoch_count = 1\n",
    "    for epoch_i in range(epoch_start, epoch_start + epochs):\n",
    "        \n",
    "        print(\"---------Epoch----------\" + str(epoch_i))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "    \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        #print(\"\")\n",
    "        #print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        #print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        # For each batch of training data...\n",
    "        mini_batch_avg_loss = 0\n",
    "        #train_size = len(train_dataloader)\n",
    "        \n",
    "        if batch_epoch_count % 500 == 0:\n",
    "            batch_size = batch_size*2\n",
    "            accumulation_steps = int(batch_size/batch_size_max_once)\n",
    "        batch_epoch_count = batch_epoch_count + 1\n",
    "\n",
    "        #train_size = len(train_dataloader) / float(accumulation_steps)\n",
    "        \n",
    "        print(\"Batch Size: \" + str(batch_size))\n",
    "        print(float(accumulation_steps))\n",
    "        \n",
    "        #print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            with torch.autograd.detect_anomaly():\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_relatedness = batch[1].to(device)\n",
    "                b_labels = batch[2].to(device)\n",
    "                b_mmd_symbol = batch[3].to(device)\n",
    "                b_mmd_symbol_ = batch[4].to(device)\n",
    "        \n",
    "            \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "            \n",
    "\n",
    "                mmd_loss, P_relatedness, P_stance = model(input_ids = b_input_ids, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "\n",
    "            \n",
    "                relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "                stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "\n",
    "            \n",
    "                loss = relatedness_loss + alpha * stance_loss - beta * mmd_loss\n",
    "                total_train_loss += loss.item()\n",
    "            \n",
    "            \n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                #for parameter in model.parameters(): print(parameter.grad.norm())\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "                \n",
    "                # Update parameters and take a step using the computed gradient.\n",
    "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "                # modified based on their gradients, the learning rate, etc.\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the learning rate.\n",
    "                scheduler.step()\n",
    "                \n",
    "                #for param_group in optimizer.param_groups:\n",
    "                #print(\"Learning Rate: \", optimizer.param_groups[\"lr\"])\n",
    "                \n",
    "                                \n",
    "                # Always clear any previously calculated gradients before performing a\n",
    "                # backward pass. PyTorch doesn't do this automatically because \n",
    "                # accumulating the gradients is \"convenient while training RNNs\". \n",
    "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                #total_train_loss = 0\n",
    "\n",
    "                #print(\"Iter: %02d, Loss: %4.4f\" % (step, mini_batch_avg_loss))                \n",
    "    \n",
    "        print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        #print(\"\")\n",
    "        print(\"  Average training loss: {0:.6f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        #print(\"\")\n",
    "        #print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy_stance = 0\n",
    "        total_eval_accuracy_ideology = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        \n",
    "        agree_test_accuracy = 0\n",
    "        disagree_test_accuracy = 0 \n",
    "        discuss_test_accuracy = 0 \n",
    "        unrelated_test_accuracy = 0\n",
    "        \n",
    "        con_test_acc = 0\n",
    "        lib_test_acc = 0\n",
    "        na_test_acc = 0\n",
    "        \n",
    "        total_true = 0\n",
    "        \n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "        \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_relatedness = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            b_mmd_symbol = batch[3].to(device)\n",
    "            b_mmd_symbol_ = batch[4].to(device)\n",
    "        \n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "            \n",
    "                mmd_loss, P_relatedness, P_stance = model(input_ids = b_input_ids, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "                \n",
    "                #CrossEntropy Loss\n",
    "                relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "                stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "\n",
    "                \n",
    "    \n",
    "                loss_val = relatedness_loss + alpha * stance_loss - beta * mmd_loss\n",
    "                total_eval_loss += loss_val.item()\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                P_relatedness = P_relatedness.to('cpu')\n",
    "                b_relatedness = b_relatedness.to('cpu')\n",
    "                P_stance = P_stance.to('cpu')\n",
    "                b_labels = b_labels.to('cpu')\n",
    "                \n",
    "\n",
    "                # Calculate the accuracy for this batch of test sentences, and\n",
    "                # accumulate it over all batches.\n",
    "                #total_eval_accuracy += predict(P_relatedness, P_stance, b_labels)\n",
    "                \n",
    "                \n",
    "                acc_list = predict_classwise_stance_emergent(P_relatedness, P_stance, b_labels)\n",
    "                total_eval_accuracy_stance += acc_list[0]\n",
    "                ###\n",
    "                agree_test_accuracy += acc_list[1]\n",
    "                disagree_test_accuracy += acc_list[2]\n",
    "                discuss_test_accuracy += acc_list[3]\n",
    "                unrelated_test_accuracy += acc_list[4]\n",
    "                total_true += acc_list[5]\n",
    "        \n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy_stance = total_eval_accuracy_stance / len(validation_dataloader)\n",
    "        print(\"Avg Val Accuracy Stance: {0:.6f}\".format(avg_val_accuracy_stance))\n",
    "        print(\"Total True\")\n",
    "        print(total_true)\n",
    "        print(\"*************\")\n",
    "        avg_val_agree_accuracy = agree_test_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Agree Accuracy: {0:.6f}\".format(avg_val_agree_accuracy))\n",
    "        avg_val_disagree_accuracy = disagree_test_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Disagree Accuracy: {0:.6f}\".format(avg_val_disagree_accuracy))\n",
    "        avg_val_discuss_accuracy = discuss_test_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Discuss Accuracy: {0:.6f}\".format(avg_val_discuss_accuracy))\n",
    "        avg_val_unrelated_accuracy = unrelated_test_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Unrelated Accuracy: {0:.6f}\".format(avg_val_unrelated_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        \n",
    "        if avg_val_loss < min_val_loss:\n",
    "            min_val_loss = avg_val_loss\n",
    "    \n",
    "        print(\"Avg Validation Loss: {0:.6f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Stance Accur.': avg_val_accuracy_stance,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        model_save_state = {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "    \n",
    "        es.__call__(avg_val_loss, avg_val_accuracy_stance, model_save_state, model_save_path, model, tokenizer)\n",
    "        last_epoch = epoch_i + 1\n",
    "        if es.early_stop == True:\n",
    "            break  # early stop criterion is met, we can stop now\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    \n",
    "    \n",
    "    min_val_loss = es.val_loss_min\n",
    "    max_val_acc = es.val_acc_max_stance\n",
    "\n",
    "    return training_stats, last_epoch, min_val_loss, max_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517111bb-7cb3-427b-95fb-72e843482844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "def run_test_stance(model_savepath, testData, stance_labels_Test, batch_size):    \n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #loss_fct = torch.nn.BCELoss()\n",
    "    loss_fct_relatedness = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    t_test_relatedness, t_test_stance, t_test_mmd_symbol, t_test_mmd_symbol_ = preprocess_fnc_test(stance_labels_Test)\n",
    "    t_instancesTest  = torch.as_tensor(testData, dtype=torch.float32)\n",
    "\n",
    "    \n",
    "\n",
    "    # Create the DataLoader.\n",
    "    prediction_data = TensorDataset(t_instancesTest, t_test_relatedness, t_test_stance, t_test_mmd_symbol, t_test_mmd_symbol_)\n",
    "    #prediction_data = TensorDataset(all_input_ids_Test, all_input_masks_Test, t_test_stance)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size, num_workers=8, drop_last=True)\n",
    "    \n",
    "    #model_current = 'bert-base-uncased'\n",
    "    model_current = 'roberta-large'\n",
    "    tokenizer = load_tokenizer(model_current)\n",
    "        \n",
    "    model = StanceDetectionUnigramClass(testData)\n",
    "    checkpoint = torch.load(model_savepath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])    \n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  betas=(0.9, 0.999), \n",
    "                  eps=1e-08, \n",
    "                  weight_decay=0,\n",
    "                  correct_bias=True\n",
    "    )\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    #model.cuda()\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_test_loss = 0.0\n",
    "    \n",
    "    total_test_accuracy = 0.0\n",
    "    ####\n",
    "    agree_test_accuracy = 0.0\n",
    "    disagree_test_accuracy = 0.0\n",
    "    discuss_test_accuracy = 0.0\n",
    "    unrelated_test_accuracy = 0.0\n",
    "    predictions , true_labels = [], []\n",
    "    \n",
    "    alpha = 1.5\n",
    "    theta = 1.1\n",
    "    beta = 1e-3\n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "      #Add batch to GPU\n",
    "        \n",
    "        #batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_relatedness = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_mmd_symbol = batch[3].to(device)\n",
    "        b_mmd_symbol_ = batch[4].to(device)\n",
    "  \n",
    "\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and \n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():         \n",
    "            # Forward pass, calculate logit predictions\n",
    "            \n",
    "            n1 = torch.sum(b_mmd_symbol, dim=0)\n",
    "            n2 = torch.sum(b_mmd_symbol_, dim=0)\n",
    "        \n",
    "            aa = torch.reshape(b_mmd_symbol, (-1,1))\n",
    "            bb = torch.reshape(b_mmd_symbol_, (-1,1))\n",
    "            \n",
    "            theta_d_layer, P_relatedness, P_stance = model(input_ids = b_input_ids, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "            #P_stance = model(input_ids = b_input_ids, attention_mask = b_input_mask)\n",
    "                \n",
    "            if n1 == 0:\n",
    "                d1 = torch.zeros(batch_size, 1, device = device)\n",
    "            else:\n",
    "                d1 = torch.div(torch.sum(theta_d_layer*aa, dim=1), n1)\n",
    "                \n",
    "            if n2 == 0:\n",
    "                d2 = torch.zeros(batch_size, 1, device = device)\n",
    "            else:\n",
    "                d2 = torch.div(torch.sum(theta_d_layer*bb, dim=1), n2)\n",
    "                    \n",
    "                    \n",
    "            mmd_loss = torch.sum(d1 - d2)\n",
    "            #mmd_loss = 0\n",
    "                \n",
    "            \n",
    "            relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "            #relatedness_loss = 0\n",
    "            stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "                \n",
    "    \n",
    "            loss_test = relatedness_loss + alpha * stance_loss - beta * mmd_loss\n",
    "            #loss_test = alpha * stance_loss\n",
    "            total_test_loss += loss_test.item()\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            P_relatedness = P_relatedness.to('cpu')\n",
    "            b_relatedness = b_relatedness.to('cpu')\n",
    "            P_stance = P_stance.to('cpu')\n",
    "            b_labels = b_labels.to('cpu')\n",
    "            \n",
    "            acc_list = predict_classwise_stance_emergent(P_relatedness, P_stance, b_labels)\n",
    "            total_test_accuracy += acc_list[0]\n",
    "            ###\n",
    "            agree_test_accuracy += acc_list[1]\n",
    "            disagree_test_accuracy += acc_list[2]\n",
    "            discuss_test_accuracy += acc_list[3]\n",
    "            unrelated_test_accuracy += acc_list[4]\n",
    "            \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_test_loss = total_test_loss / len(prediction_dataloader)\n",
    "    avg_test_accuracy = total_test_accuracy / len(prediction_dataloader)\n",
    "    \n",
    "    avg_agree_test_acc = agree_test_accuracy / len(prediction_dataloader)\n",
    "    avg_disagree_test_acc = disagree_test_accuracy / len(prediction_dataloader)\n",
    "    avg_discuss_test_acc = discuss_test_accuracy / len(prediction_dataloader)\n",
    "    avg_unrelated_test_acc = unrelated_test_accuracy / len(prediction_dataloader)\n",
    "\n",
    "    return avg_test_loss, avg_test_accuracy, avg_agree_test_acc, avg_disagree_test_acc, avg_discuss_test_acc, avg_unrelated_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51747dd-3ade-4c03-85e6-58bce3914873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wholeprocess_fnc_unigram(train_path, val_path, max_len, doc_stride, batch_size, num_warmup_steps, learning_rate, seedVal):\n",
    "    device = run_utils()\n",
    "    model_save_path = './model_save/emergent/model_emergentbert.t7'  \n",
    "    #model_fnc = './model_save/fnc/model_fnc.t7'    \n",
    "        \n",
    "    #model_current = 'distilbert-base-uncased'\n",
    "    #model_current = 'roberta-base'\n",
    "    model_current = 'bert-base-uncased'\n",
    "    #model_current = './models/tiny_bert/'\n",
    "    tokenizer = load_tokenizer(model_current)\n",
    "\n",
    "#--------------LOAD DATASETS--------------#\n",
    "    lim_unigram = 3000\n",
    "    \n",
    "    train_path = './dataset/fnc/train_fnc.csv'\n",
    "    val_path = './dataset/fnc/val_fnc.csv'\n",
    "    test_path = './dataset/fnc/test_fnc.csv'\n",
    "\n",
    "    df = load_dataset_stance_fnc(train_path)\n",
    "    dfVal = load_dataset_stance_fnc(val_path)\n",
    "    dfTest = load_dataset_stance_fnc(test_path)\n",
    "    \n",
    "    df = df.append(dfVal, ignore_index = True)\n",
    "    \n",
    "    # Report the number of sentences.\n",
    "    print('Number of training sentences: {:,}'.format(df.shape[0]))\n",
    "    print('Number of val sentences: {:,}'.format(dfVal.shape[0]))\n",
    "    print('Number of test sentences: {:,}'.format(dfTest.shape[0]))\n",
    "    \n",
    "    train_set, train_mean, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer = preprocess_data_train(df, dfVal, dfTest, lim_unigram)\n",
    "    val_set = preprocess_data_val(dfVal, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m = 0.0001)\n",
    "    test_set = preprocess_data_test(dfTest, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m = 0.0001)\n",
    "\n",
    "    labelsTrain = df['stance']\n",
    "    labelsVal = dfVal['stance']\n",
    "    labelsTest = dfTest['stance']\n",
    "    #--------------TRAINING-------------#\n",
    "    \n",
    "    model, train_dataloader, validation_dataloader, optimizer, scheduler = prepare_for_training_stance_emergent_unigram(train_set, labelsTrain, val_set, labelsVal, model_current, 16, epochs, num_warmup_steps, learning_rate)    \n",
    "    training_stats, last_epoch, min_val_loss, max_val_acc = onlytrain_stance_emergent_unigram (model_save_path, model, tokenizer, train_dataloader, validation_dataloader, epochs, batch_size, optimizer,\n",
    "                                                                          scheduler, patience, verbose, delta, seedVal, False)\n",
    "    \n",
    "    test_loss, test_acc, avg_agree_test_acc, avg_disagree_test_acc, avg_discuss_test_acc, avg_unrelated_test_acc = run_test_stance(model_save_path, test_set, labelsTest, batch_size)\n",
    "        \n",
    "    #df_stats = print_summary(training_stats)\n",
    "    #plot_results(df_stats, last_epoch)\n",
    "\n",
    "    print('Min Val Loss: ' + str(min_val_loss))\n",
    "    print('Max Val Acc: ' + str(max_val_acc))\n",
    "    \n",
    "    print('Test Loss: ' + str(test_loss))\n",
    "    print('Test Stance Acc: ' + str(test_acc))\n",
    "    \n",
    "    print('Test Agree Class Acc: ' + str(avg_agree_test_acc))\n",
    "    print('Test Disagree Class Acc: ' + str(avg_disagree_test_acc))\n",
    "    print('Test Discuss Class Acc: ' + str(avg_discuss_test_acc))\n",
    "    print('Test Unrelated Class Acc: ' + str(avg_unrelated_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6d3ed-7b80-4c1e-a727-2788a8fe0045",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install import_ipynb\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040c54eb-10df-40fc-8d69-9a56463b7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "import time\n",
    "from transformers import AutoModel\n",
    "from transformers import DistilBertModel\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import import_ipynb\n",
    "import feature_engineering\n",
    "from feature_engineering import *\n",
    "from Utils import *\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "#model = \"bert-base-uncased\"\n",
    "train_path = './dataset/fnc/train'\n",
    "val_path = './dataset/fnc/test'\n",
    "\n",
    "model_save_path = './model_save'\n",
    "\n",
    "\n",
    "max_len = 512\n",
    "doc_stride = 0\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "num_warmup_steps = 0\n",
    "learning_rate = 1e-5\n",
    "##-----Early Stopping\n",
    "patience = 10\n",
    "verbose = True\n",
    "delta = 0.000001\n",
    "\n",
    "value = randint(0, 100)\n",
    "seedVal = 7\n",
    "\n",
    "#seedVal = 40\n",
    "\n",
    "#0 - tinybert\n",
    "#1 - distilbert\n",
    "#2 - bert\n",
    "\n",
    "#bert_type = 2\n",
    "\n",
    "#create_determinism(seedVal)\n",
    "\n",
    "#folder_preparations()\n",
    "run_wholeprocess_fnc_unigram(train_path, val_path, max_len, doc_stride, batch_size, num_warmup_steps, learning_rate, seedVal)\n",
    "#create_more_notrel_docs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
