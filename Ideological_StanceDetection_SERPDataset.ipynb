{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c31a72-54b8-4044-87a2-f8265059fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_utils():\n",
    "    # Get the GPU device name.\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    # The device name should look like the following:\n",
    "    if device_name == '/device:GPU:0':\n",
    "        print('Found GPU at: {}'.format(device_name))\n",
    "    else:\n",
    "        raise SystemError('GPU device not found')\n",
    "\n",
    "    device = None\n",
    "    # If there's a GPU available...\n",
    "    if torch.cuda.is_available():    \n",
    "        # Tell PyTorch to use the GPU.    \n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    # If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def predict(P_ideology, ideology_labels):\n",
    "    predict_labels = torch.argmax(P_ideology, 1)\n",
    "    target_labels = torch.argmax(ideology_labels, 1)\n",
    "    \n",
    "\n",
    "    true_predict_count = len((torch.eq(predict_labels, target_labels)).nonzero().flatten())\n",
    "    accuracy = true_predict_count / len(predict_labels)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def predict_binary(P_ideology, ideology_labels):\n",
    "    predict_labels = np.round(P_ideology)\n",
    "    predict_labels = predict_labels.int()\n",
    "\n",
    "    true_predict_count = (torch.eq(predict_labels, ideology_labels)).sum()\n",
    "    true_predict_count = true_predict_count.numpy()\n",
    "    #print(true_predict_count)\n",
    "    \n",
    "    accuracy = true_predict_count / len(predict_labels)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def find_maxLen_doc(data, tokenizer):\n",
    "    max_len = 0\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "        input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        # Update the maximum sentence length.\n",
    "        max_len = max(max_len, len(input_ids))\n",
    "\n",
    "    print('Max sentence length: ', max_len)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def load_tokenizer(model):\n",
    "    tokenizer = None\n",
    "    from transformers import AutoTokenizer, DistilBertTokenizer, BertTokenizer, RobertaTokenizer, AutoModelWithLMHead\n",
    "    tokenizer = BertTokenizer.from_pretrained(model, do_lower_case=True)\n",
    "    #tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def load_dataset(path):\n",
    "    # Load the dataset into a pandas dataframe.\n",
    "    df = pd.read_csv(path, delimiter='\\t', header=0, names=['qID', 'q_ideology', 'ideology', 'stance', 'docCont', 'topic', 'Q', 'title'])       \n",
    "\n",
    "    df['docCont'] = df['docCont'].str.lower()\n",
    "    #df['topic'] = df['topic'].str.lower()\n",
    "    df['Q'] = df['Q'].str.lower()\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    \n",
    "    #df.insert(0, \"stanceStr\", df['stance'], True)\n",
    "    #df[\"stanceStr\"] = df[\"stanceStr\"].replace({1: \"Pro\", 0: \"Agst\"})\n",
    "    \n",
    "    print(\"Train\")\n",
    "    print (\"Con\", df[df.ideology == 0].shape[0])\n",
    "    print (\"Lib\", df[df.ideology == 1].shape[0])\n",
    "    print (\"Pro\", df[df.stance == 1].shape[0])\n",
    "    print (\"Against\", df[df.stance == 0].shape[0])\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_dataset_ambigious(path):\n",
    "    # Load the dataset into a pandas dataframe.\n",
    "    df = pd.read_csv(path, delimiter='\\t', header=0, names=['qID', 'docID', 'stance', 'Ambigious', 'ideology', 'docCont', 'Q', 'title'])       \n",
    "\n",
    "    df['docCont'] = df['docCont'].str.lower()\n",
    "    #df['topic'] = df['topic'].str.lower()\n",
    "    df['Q'] = df['Q'].str.lower()\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    \n",
    "    df = df.astype(str)\n",
    "    \n",
    "    #df.insert(0, \"stanceStr\", df['stance'], True)\n",
    "    #df[\"stanceStr\"] = df[\"stanceStr\"].replace({1: \"Pro\", 0: \"Agst\"})\n",
    "    \n",
    "    print(\"Train\")\n",
    "    print (\"Ambigious\", df[df.Ambigious == \"1\"].shape[0])\n",
    "    print (\"Non-ambigious\", df[df.Ambigious == \"0\"].shape[0])\n",
    "    \n",
    "    print(df.dtypes)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_dataset_first(path):\n",
    "    # Load the dataset into a pandas dataframe.\n",
    "    df = pd.read_csv(path, delimiter='\\t', header = 0, names=['qID', 'ideology', 'stance', 'docCont', 'topic', 'Q', 'title'])\n",
    "\n",
    "    df_q = pd.read_csv(path.replace('final.tsv', 'final_onlyqID.tsv'), delimiter='\\t', header=0, names=['qID', 'orientation'])\n",
    "    df_q[\"orientation\"] = df_q[\"orientation\"].replace({-1: \"Lib\", 1: \"Con\"})\n",
    "           \n",
    "    df = df.drop('qID', axis=1)\n",
    "    df.insert(0, \"qID\", df_q['qID'], True)\n",
    "    df.insert(1, \"q_ideology\", df_q['orientation'], True)\n",
    "  \n",
    "    df[\"stance\"] = df[\"stance\"].replace({\"-1\": 0, \"1\": 1, \"Pro\": 1, \"Agst\": 0})\n",
    "    \n",
    "    return df\n",
    "\n",
    "def sample_dataset_stance(df, seedVal):\n",
    "    #create_determinism(seedVal)\n",
    "    \n",
    "    df_A = df[df['Ambigious'] == \"1\"]\n",
    "    df_N = df[df['Ambigious'] == \"0\"]\n",
    "    \n",
    "    \n",
    "    df_new = df_A.append(df_N, ignore_index = True)\n",
    "\n",
    "    y_copy = df_new['Ambigious'].copy(deep=True)\n",
    "    X_copy = df_new.drop('Ambigious', axis=1).copy(deep=True)\n",
    "    \n",
    "    X = pd.DataFrame (columns=['qID', 'docID', 'stance', 'ideology', 'docCont', 'Q', 'title'])\n",
    "    y = pd.DataFrame (columns=['Ambigious'])\n",
    "    \n",
    "    X = X_copy\n",
    "    y = y_copy\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "    \n",
    "    print(len(X_train))\n",
    "    print(len(y_train))\n",
    "    print(len(X_test))\n",
    "    print(len(y_test))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, shuffle=True)\n",
    "    \n",
    "    X_train.insert(2, \"Ambigious\", y_train.values) \n",
    "    X_val.insert(2, \"Ambigious\", y_val.values) \n",
    "    X_test.insert(2, \"Ambigious\", y_test.values)\n",
    "    \n",
    "    \n",
    "    df_A = X_train[X_train['Ambigious'] == \"1\"]\n",
    "    df_N = X_train[X_train['Ambigious'] == \"0\"]\n",
    "    \n",
    "    \n",
    "    print(\"****Train****\")\n",
    "    print(\"Ambigious\", df_A.shape[0])\n",
    "    print(\"Not Ambigious\", df_N.shape[0])\n",
    "    \n",
    "    \n",
    "    df_A = X_test[X_test['Ambigious'] == \"1\"]\n",
    "    df_N = X_test[X_test['Ambigious'] == \"0\"]\n",
    "    \n",
    "    print(\"****Test****\")\n",
    "    print(\"Ambigious\", df_A.shape[0])\n",
    "    print(\"Not Ambigious\", df_N.shape[0])\n",
    "    \n",
    "    X_train.to_csv('./dataset/batches_cleaned/stance/train_serp.tsv', sep='\\t', index=False)\n",
    "    X_val.to_csv('./dataset/batches_cleaned/stance/val_serp.tsv', sep='\\t', index=False)\n",
    "    X_test.to_csv('./dataset/batches_cleaned/stance/test_serp.tsv', sep='\\t', index=False)\n",
    "\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "def merge_datasets(df, dfVal, dfTest):\n",
    "    from numpy import nan\n",
    "    df = df.append(dfVal, ignore_index = True)\n",
    "    df = df.append(dfTest, ignore_index = True)\n",
    "    \n",
    "    df.replace(\"\", nan, inplace=True)\n",
    "    df.replace(\" \", nan, inplace=True)\n",
    "    df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n",
    "    \n",
    "    dfLabel = df['ideology'].copy(deep=True)\n",
    "    df = df.drop('ideology', axis=1).copy(deep=True)\n",
    "    \n",
    "    return df, dfLabel\n",
    "\n",
    "def preprocess_dataset_new_ideology_latest(df_new, testPer, seedVal):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from pandas import DataFrame\n",
    "    \n",
    "    create_determinism(seedVal)\n",
    "    \n",
    "    #print(\"New dataset\")\n",
    "    #print(df_new['stance'].value_counts())\n",
    "\n",
    "    y = df_new['ideology'].copy(deep=True)\n",
    "    X = df_new.drop('ideology', axis=1).copy(deep=True)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testPer, shuffle=True, stratify=y)\n",
    "            \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def preprocess_dataset_new(df, dfLabels, testPer, seedVal):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from pandas import DataFrame\n",
    "    \n",
    "    create_determinism(seedVal)\n",
    "\n",
    "    df.insert(2, \"ideology\", dfLabels.values) \n",
    "    df = df.sort_values(by='Q')\n",
    "     \n",
    "    one_q_instances = []\n",
    "    all_q_instances = {}\n",
    "    \n",
    "    curr_q = df.Q.values[0]\n",
    "    for index, inst in df.iterrows():\n",
    "        if curr_q == inst['Q']:\n",
    "            one_q_instances.append(inst.values)\n",
    "        else:\n",
    "            all_q_instances[curr_q] = one_q_instances\n",
    "            one_q_instances = []\n",
    "            curr_q = inst['Q']\n",
    "            one_q_instances.append(inst.values)\n",
    "            \n",
    "    \n",
    "    X_train_allqueries = {}\n",
    "    X_test_allqueries = {}\n",
    "    y_train_allqueries = {}\n",
    "    y_test_allqueries = {}\n",
    "            \n",
    "    for query in all_q_instances:\n",
    "        this_query_instances = all_q_instances[query]\n",
    "        \n",
    "        df = DataFrame (this_query_instances, columns=['qID', 'q_ideology', 'stance', 'ideology', 'docCont', 'topic', 'Q', 'title'])\n",
    "        \n",
    "        y = df['ideology'].copy(deep=True)\n",
    "        X = df.drop('ideology', axis=1).copy(deep=True)\n",
    "    \n",
    "        X_train_allqueries[query] = []\n",
    "        y_train_allqueries[query] = []\n",
    "        X_test_allqueries[query] = []\n",
    "        y_test_allqueries[query] = []\n",
    "        \n",
    "        if len(X.index) > 1:\n",
    "        \n",
    "            con_count = len(df[df['ideology'] == 0])\n",
    "            lib_count = len(df[df['ideology'] == 1])\n",
    "\n",
    "            if con_count < 2 or lib_count < 2:\n",
    "                X_train_allqueries[query], X_test_allqueries[query], y_train_allqueries[query], y_test_allqueries[query] = train_test_split(X, y, test_size=testPer, shuffle=True)\n",
    "            else:\n",
    "                if((con_count + lib_count)*testPer > 1):\n",
    "                    X_train_allqueries[query], X_test_allqueries[query], y_train_allqueries[query], y_test_allqueries[query] = train_test_split(X, y, test_size=testPer, shuffle=True, stratify=y)\n",
    "                else:\n",
    "                    X_train_allqueries[query], X_test_allqueries[query], y_train_allqueries[query], y_test_allqueries[query] = train_test_split(X, y, test_size=testPer, shuffle=True)\n",
    "        else:\n",
    "            X_train_allqueries[query] = X\n",
    "            y_train_allqueries[query] = y\n",
    "    \n",
    "    X_train = pd.DataFrame(columns=['qID', 'q_ideology', 'stance', 'docCont', 'topic', 'Q', 'title'])\n",
    "    y_train = pd.DataFrame(columns=['ideology'])\n",
    "    \n",
    "    X_test = pd.DataFrame(columns=['qID', 'q_ideology', 'stance', 'docCont', 'topic', 'Q', 'title'])\n",
    "    y_test = pd.DataFrame(columns=['ideology'])\n",
    "    \n",
    "    for query in X_train_allqueries:\n",
    "        X_train = X_train.append(pd.DataFrame(X_train_allqueries[query]), ignore_index = True)\n",
    "        y_train = y_train.append(pd.DataFrame(y_train_allqueries[query]), ignore_index = True)\n",
    "    \n",
    "    for query in X_test_allqueries:\n",
    "        X_test = X_test.append(pd.DataFrame(X_test_allqueries[query]), ignore_index = True)\n",
    "        y_test = y_test.append(pd.DataFrame(y_test_allqueries[query]), ignore_index = True)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def create_train_val_test_split(trainpath, valpath, testpath, testPer, valPer, seedVal):\n",
    "    \n",
    "    df = load_dataset(\"./dataset/ideology/train_samples.tsv\")\n",
    "    dfVal = load_dataset(\"./dataset/ideology/val_samples.tsv\")\n",
    "    dfTest = load_dataset(\"./dataset/ideology/test_samples.tsv\")\n",
    "\n",
    "    dfComp, dfCompLabel = merge_datasets(df, dfVal, dfTest)\n",
    "    dfComp.insert(2, \"ideology\", dfCompLabel.values)\n",
    "    \n",
    "    df, dfLabel, dfTest, dfTestLabel = preprocess_dataset_new_ideology_latest(dfComp, 0.2, seedVal)\n",
    "    df.insert(2, \"ideology\", dfLabel.values)\n",
    "    \n",
    "    df, dfLabel, dfVal, dfValLabel = preprocess_dataset_new_ideology_latest(df, 0.2, seedVal)\n",
    "    \n",
    "    df.insert(2, \"ideology\", dfLabel.values)\n",
    "    dfVal.insert(2, \"ideology\", dfValLabel.values) \n",
    "    dfTest.insert(2, \"ideology\", dfTestLabel.values)\n",
    "    \n",
    "    df.to_csv('train_new.tsv', sep='\\t', index=False)\n",
    "    dfVal.to_csv('val_new.tsv', sep='\\t', index=False)\n",
    "    dfTest.to_csv('test_new.tsv', sep='\\t', index=False)\n",
    "\n",
    "def preprocess_ideologyOld(stance_labels, ideology_labels):\n",
    "    t_stance = []\n",
    "    t_ideology = []\n",
    "    \n",
    "    t_mmd_symbol = []\n",
    "    t_mmd_symbol_ = []\n",
    "\n",
    "    for idx, s_label in enumerate(stance_labels):\n",
    "        i_label = ideology_labels[idx]\n",
    "        if s_label == 1 and i_label == 0: #pro-con\n",
    "            t_stance.append([1,0])\n",
    "            t_ideology.append([0,1])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "        elif s_label == 1 and i_label == 1: #pro-lib\n",
    "            t_stance.append([1,0]) \n",
    "            t_ideology.append([1,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(1)\n",
    "        elif s_label == 0 and i_label == 0: #agst-con\n",
    "            t_stance.append([0,1])\n",
    "            t_ideology.append([0,1])\n",
    "            t_mmd_symbol.append(0)\n",
    "            t_mmd_symbol_.append(0)\n",
    "        else: #agst-lib\n",
    "            t_stance.append([0,1])\n",
    "            t_ideology.append([1,0])\n",
    "            t_mmd_symbol.append(0)\n",
    "            t_mmd_symbol_.append(1)\n",
    "            \n",
    "    \n",
    "    t_stance = torch.as_tensor(t_stance, dtype=torch.int32)\n",
    "    t_ideology = torch.as_tensor(t_ideology, dtype=torch.int32)\n",
    "    \n",
    "    t_mmd_symbol  = torch.as_tensor(t_mmd_symbol, dtype=torch.float32)\n",
    "    t_mmd_symbol_ = torch.as_tensor(t_mmd_symbol_, dtype=torch.float32)\n",
    "    \n",
    "    return t_stance, t_ideology, t_mmd_symbol, t_mmd_symbol_\n",
    "\n",
    "def preprocess_ideology_ambigious(ambigious_labels):\n",
    "    t_ideology = []\n",
    "\n",
    "    for idx, a_label in enumerate(ambigious_labels):\n",
    "        a_label = ambigious_labels[idx]\n",
    "        if a_label == \"0\": #con\n",
    "            t_ideology.append([0])\n",
    "        else:#lib\n",
    "            t_ideology.append([1])\n",
    "            \n",
    "    t_ideology = torch.as_tensor(t_ideology, dtype=torch.int32)\n",
    "    \n",
    "    return t_ideology\n",
    "\n",
    "def preprocess_ideology_new(stance_labels, ideology_labels):\n",
    "    t_ideology = []\n",
    "\n",
    "    for idx, s_label in enumerate(stance_labels):\n",
    "        i_label = ideology_labels[idx]\n",
    "        if i_label == 0: #con\n",
    "            t_ideology.append([0])\n",
    "        else:#lib\n",
    "            t_ideology.append([1])\n",
    "            \n",
    "    t_ideology = torch.as_tensor(t_ideology, dtype=torch.int32)\n",
    "    \n",
    "    return t_ideology\n",
    "\n",
    "def preprocess_ideology(stance_labels, ideology_labels):\n",
    "    t_ideology = []\n",
    "\n",
    "    for idx, s_label in enumerate(stance_labels):\n",
    "        i_label = ideology_labels[idx]\n",
    "        if s_label == 1 and i_label == 0: #pro-con\n",
    "            t_ideology.append([1,0])\n",
    "        elif s_label == 1 and i_label == 1: #pro-lib\n",
    "            t_ideology.append([1,1])\n",
    "        elif s_label == 0 and i_label == 0: #agst-con\n",
    "            t_ideology.append([0,0])\n",
    "        else: #agst-lib\n",
    "            t_ideology.append([0,1])\n",
    "            \n",
    "    t_ideology = torch.as_tensor(t_ideology, dtype=torch.int32)\n",
    "    \n",
    "    return t_ideology\n",
    "\n",
    "def concanListStringsLonger(list1, list2):\n",
    "    list3 = []\n",
    "    myLen1 = len(list1)\n",
    "    if myLen1 != len(list2):\n",
    "        print(\"Length - error\")\n",
    "    for idx in range(0, myLen1):\n",
    "        list3.append(list1[idx] + \" GIZEM \" + list2[idx])\n",
    "    return list3\n",
    "\n",
    "def concanListStrings(list1, list2):\n",
    "    list3 = []\n",
    "    new_labels = []\n",
    "    myLen1 = len(list1)\n",
    "    if myLen1 != len(list2):\n",
    "        print(\"Length - error\")\n",
    "    for idx in range(0, myLen1):\n",
    "        list3.append(list1[idx] + \" \" + list2[idx])\n",
    "        #list3.append(list1[idx] + \" \" + list2[idx][-512:])\n",
    "        #new_labels.append(labels[idx])\n",
    "        #new_labels.append(labels[idx])\n",
    "        \n",
    "    return list3\n",
    "\n",
    "def concanListStrings_sep(list1, list2):\n",
    "    list3 = []\n",
    "    myLen1 = len(list1)\n",
    "    if myLen1 != len(list2):\n",
    "        print(\"Length - error\")\n",
    "    for idx in range(0, myLen1):\n",
    "        list3.append(list1[idx] + \" [SEP] \" + str(list2[idx]))\n",
    "\n",
    "    return list3\n",
    "\n",
    "### Generate the datasets with the different fields.\n",
    "def generate_datasets_ambigious(df, tokenizer):\n",
    "\n",
    "    sentencesQuery= df.Q.values\n",
    "    sentencesTitle = df.title.values\n",
    "    sentencesCont = df.docCont.values\n",
    "\n",
    "    labels = df.Ambigious.values\n",
    "    \n",
    "    #print(stances[0:10])\n",
    "\n",
    "    sentencesQueryTitle = concanListStrings(sentencesQuery, sentencesTitle)\n",
    "    sentencesQueryTitleCont = concanListStringsLonger(sentencesQueryTitle, sentencesCont)\n",
    "\n",
    "    return sentencesQueryTitle, sentencesQueryTitleCont, labels\n",
    "\n",
    "\n",
    "### Generate the datasets with the different fields.\n",
    "def generate_datasets_ideology(df, tokenizer):\n",
    "\n",
    "    sentencesQuery= df.Q.values\n",
    "    sentencesQIdeology = df.q_ideology.values\n",
    "    sentencesTitle = df.title.values\n",
    "    sentencesCont = df.docCont.values\n",
    "\n",
    "    stances = df.stance.values\n",
    "    labels = df.ideology.values\n",
    "    \n",
    "    #print(stances[0:10])\n",
    "\n",
    "    sentencesQueryTitle = concanListStrings(sentencesQuery, sentencesTitle)\n",
    "    sentencesQueryTitleStance = concanListStrings(sentencesQueryTitle, stances)\n",
    "    sentencesQueryTitleCont = concanListStringsLonger(sentencesQueryTitle, sentencesCont)\n",
    "    sentencesQueryTitleStanceCont = concanListStringsLonger(sentencesQueryTitleStance, sentencesCont)\n",
    "\n",
    "    return sentencesQueryTitle, sentencesQueryTitleCont, sentencesQueryTitleStance, sentencesQueryTitleStanceCont, stances, labels\n",
    "\n",
    "def preprocessing_for_bert(tokenizer, docs, max_len, doc_stride):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    input_ids_last = []\n",
    "    attention_masks_last = []\n",
    "    \n",
    "    content_input_ids = {}\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in docs:\n",
    "        #print(sent)\n",
    "        #print(sentences[0])\n",
    "        #print(sentences[1])\n",
    "        \n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus (\n",
    "            sent,  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=max_len,                  # Max length to truncate/pad\n",
    "            #padding='longest',         # Pad sentence to max length\n",
    "            pad_to_max_length = True,\n",
    "            return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent['input_ids'])\n",
    "        attention_masks.append(encoded_sent['attention_mask'])\n",
    "        \n",
    "        # Print the original sentence.\n",
    "        #print(' Original: ', sent)\n",
    "\n",
    "        # Print the sentence split into tokens.\n",
    "        #print('Tokenized: ', input_ids)\n",
    "        \n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    # Print sentence 0, now as a list of IDs.\n",
    "    #print('Original: ', docs[0])\n",
    "    #print('Token IDs:', input_ids[0])\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "def transform_sequences_longer_ideology(tokenizer, docs, stanceLabels, ideologylabels, max_len, doc_stride):\n",
    "\n",
    "    special_tokens_count = 2 #[CLS] and [SEP]\n",
    "    # For every sentence...\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    stance_labels_Transformed = []\n",
    "    ideology_labels_Transformed = []\n",
    "\n",
    "    only_get_partial_text = False\n",
    "    if doc_stride == 0:\n",
    "        only_get_partial_text = True\n",
    "        \n",
    "    checked_doc_stride_thresh = doc_stride - special_tokens_count - 1\n",
    "        \n",
    "    allDocs_len = len(docs)\n",
    "    for doc_id in range(0, allDocs_len):\n",
    "        currDoc = docs[doc_id]\n",
    "        currStanceLabel = stanceLabels[doc_id]\n",
    "        currIdeologyLabel = ideologylabels[doc_id]\n",
    "        \n",
    "        my_idx = 0\n",
    "        if \"GIZEM\" in currDoc:\n",
    "            doc_splitted_tokens = currDoc.split(\" \")\n",
    "            my_idx = doc_splitted_tokens.index('GIZEM')\n",
    "        else:\n",
    "            doc_splitted_tokens = currDoc.split(\" \")\n",
    "        \n",
    "        #query\n",
    "        first_part_tokens = tokenizer.tokenize(' '.join(doc_splitted_tokens[0:my_idx]))\n",
    "        myTokens = tokenizer.tokenize(' '.join(doc_splitted_tokens[my_idx+1:]))\n",
    "        mytokens_maxlen = []\n",
    "\n",
    "        first_part_len = len(first_part_tokens)\n",
    "        cur_len = len(myTokens)\n",
    "        #longer than the max-len, use doc-stride\n",
    "        taken_len = max_len - first_part_len - special_tokens_count - 1\n",
    "        \n",
    "        if only_get_partial_text:\n",
    "            mytokens_maxlen.append(first_part_tokens + myTokens[0:taken_len])\n",
    "        else:\n",
    "            checked_thresh = max_len - first_part_len - special_tokens_count\n",
    "            if cur_len > checked_thresh:\n",
    "                #get first part len\n",
    "                while cur_len > checked_thresh:\n",
    "                    partialTokens = first_part_tokens + myTokens[0:taken_len]\n",
    "                    mytokens_maxlen.append(partialTokens)\n",
    "                    del myTokens[0:checked_doc_stride_thresh]\n",
    "                    cur_len = len(myTokens)\n",
    "                if cur_len > 0:\n",
    "                    mytokens_maxlen.append(first_part_tokens + myTokens)\n",
    "            else:\n",
    "                mytokens_maxlen.append(first_part_tokens + myTokens)\n",
    "\n",
    "        if len(mytokens_maxlen) == 1:\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                        currDoc,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "          # Add the encoded sentence to the list.    \n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "          # And its attention mask (simply differentiates padding from non-padding).\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "            stance_labels_Transformed.append(currStanceLabel)\n",
    "            ideology_labels_Transformed.append(currIdeologyLabel)\n",
    "        else:\n",
    "            for maxTokenList in mytokens_maxlen:\n",
    "                if len(maxTokenList) > 510:\n",
    "                    print(len(maxTokenList))\n",
    "          #   (4) Map tokens to their IDs.\n",
    "          #   (5) Pad or truncate the sentence to `max_length`\n",
    "          #   (6) Create attention masks for [PAD] tokens.\n",
    "                encoded_dict = tokenizer.encode_plus(\n",
    "                    maxTokenList,                      # Sentence to encode.\n",
    "                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                    max_length = max_len,           # Pad & truncate all sentences.\n",
    "                    pad_to_max_length = True,\n",
    "                    return_attention_mask = True,   # Construct attn. masks.\n",
    "                    return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    " \n",
    "                input_ids.append(encoded_dict['input_ids'])\n",
    "                attention_masks.append(encoded_dict['attention_mask'])\n",
    "                stance_labels_Transformed.append(currStanceLabel)\n",
    "                ideology_labels_Transformed.append(currIdeologyLabel)\n",
    "\n",
    "\n",
    "    all_input_ids = torch.cat(input_ids, dim=0)\n",
    "    all_input_mask = torch.cat(attention_masks, dim=0)\n",
    "    stance_labels = torch.tensor(stance_labels_Transformed)\n",
    "    ideology_labels = torch.tensor(ideology_labels_Transformed)\n",
    "    \n",
    "    print(all_input_ids.shape)\n",
    "\n",
    "    return all_input_ids, all_input_mask, stance_labels, ideology_labels\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, RobertaModel\n",
    "class IdeologyDetectionClass(torch.nn.Module):\n",
    "    def __init__(self, modelUsed):\n",
    "        super(IdeologyDetectionClass, self).__init__()\n",
    "        input_size = 768\n",
    "        hidden_size = 768\n",
    "        mmd_size = 10\n",
    "        dropout_prob = 0.5\n",
    "        relatedness_size = 2\n",
    "        classes_size = 2\n",
    "        #agreement_size = 3\n",
    "        \n",
    "        self.input_pl = BertModel.from_pretrained(modelUsed) #input\n",
    "        self.l1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.bn1_hidden = torch.nn.BatchNorm1d(hidden_size, momentum=0.05)\n",
    "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.stance = torch.nn.Linear(hidden_size, classes_size)\n",
    "        self.output_prob = torch.nn.Softmax(dim = 1)\n",
    "\n",
    "        #self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        relatedness_size = 2\n",
    "        classes_size = 1\n",
    "        \n",
    "        input_1 = self.input_pl(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        last_hidden_state_cls = input_1[0][:, 0, :]\n",
    "        \n",
    "        #hidden layer\n",
    "        hidden_state = self.l1(last_hidden_state_cls)\n",
    "        hidden_state_normalized = self.bn1_hidden(hidden_state)\n",
    "        hidden_state_normalized = self.relu(hidden_state_normalized)\n",
    "        hidden_layer= self.dropout(hidden_state_normalized)\n",
    "        \n",
    "        #mmd layer        \n",
    "        #theta_d = self.theta_d(hidden_layer)\n",
    "        ##theta_d_normalized = self.bn1_theta(theta_d)\n",
    "        #theta_d_normalized = torch.nn.ReLU()(theta_d_normalized)\n",
    "        #theta_d_layer= self.dropout(theta_d_normalized)\n",
    "\n",
    "        #probability layer\n",
    "        #relatedness_state = self.probability(hidden_layer)\n",
    "        #relatedness_flat = self.dropout(relatedness_state)\n",
    "        \n",
    "        #relatedness_flat_reshaped = torch.reshape(relatedness_flat, (-1, relatedness_size))\n",
    "        #P_relatedness = self.output_prob(relatedness_flat_reshaped)    \n",
    "        \n",
    "        #P_related = torch.reshape(P_relatedness[:, 0], (-1, 1))\n",
    "        #P_unrelated = torch.reshape(P_relatedness[:, 1], (-1, 1))\n",
    "        \n",
    "        stance_state = self.stance(hidden_layer) #batch size x classes_size\n",
    "        P_stance = self.output_prob(stance_state) \n",
    "\n",
    "        return P_stance\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.val_acc_max = -1\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, val_acc, model_save_state, model_save_path):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, val_acc, model_save_state, model_save_path)\n",
    "            self.val_acc_max = val_acc\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, val_acc, model_save_state, model_save_path)\n",
    "            self.val_acc_max = val_acc\n",
    "            self.counter = 0\n",
    "\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, val_loss, val_acc, model_save_state, model_save_path):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            print(f'Validation acc: ({self.val_acc_max:.6f} --> {val_acc:.6f}).  Saving model ...')\n",
    "        #torch.save(model.module.state_dict(), 'checkpoint.pt')\n",
    "        \n",
    "        torch.save(model_save_state, model_save_path)\n",
    "        \n",
    "        \n",
    "        #model.save_pretrained('model_save/')\n",
    "        #tokenizer.save_pretrained('model_save/')\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        #torch.save(model, './model_save/entire_model.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "#from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "def prepare_for_training_ambigious(input_idsTrain, attention_masksTrain, ideology_labels_Train, input_idsVal, attention_masksVal, ideology_labels_Val, modelUsed, batch_size=16, epochs = 50, num_warmup_steps=0, learning_rate=5e-5):\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "\n",
    "    from transformers import BertForSequenceClassification, AdamW, BertConfig, RobertaConfig, AutoModelWithLMHead\n",
    "    from transformers import DistilBertForSequenceClassification, RobertaForSequenceClassification\n",
    "    \n",
    "    from torch.utils.data import DataLoader, RandomSampler\n",
    "    \n",
    "    t_train_stance = preprocess_ideology_ambigious(ideology_labels_Train)\n",
    "    \n",
    "    datasetTrain = TensorDataset(input_idsTrain, attention_masksTrain, t_train_stance)\n",
    "\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "    t_val_stance  = preprocess_ideology_ambigious(ideology_labels_Val)\n",
    "    \n",
    "    \n",
    "    datasetVal = TensorDataset(input_idsVal, attention_masksVal, t_val_stance)\n",
    "    \n",
    "    model = IdeologyDetectionClass(modelUsed)\n",
    "\n",
    "    # Tell pytorch to run this model on the GPU.\n",
    "    model.cuda()\n",
    "\n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "    # I believe the 'W' stands for 'Weight Decay fix\"\n",
    "    \n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  betas=(0.9, 0.999), \n",
    "                  eps=1e-08, \n",
    "                  weight_decay=1e-3,\n",
    "                  correct_bias=True\n",
    "               )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "            datasetTrain,  # The training samples.\n",
    "            sampler =  RandomSampler(datasetTrain), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size., \n",
    "            num_workers=8\n",
    "        )\n",
    "    batch_size = batch_size\n",
    "\n",
    "\n",
    "    from transformers import get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "    # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "    # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "    # training data.\n",
    "    epochs = epochs\n",
    "\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    schedulerOld = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "    \n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps, num_cycles = 5)\n",
    "    \n",
    "    loss_fct = torch.nn.BCELoss()\n",
    "    return model, datasetTrain, datasetVal, optimizer, schedulerOld\n",
    "\n",
    "#from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "def prepare_for_trainingOld(input_idsTrain, attention_masksTrain, stance_labels_Train, ideology_labels_Train, input_idsVal, attention_masksVal, stance_labels_Val, ideology_labels_Val, modelUsed, batch_size=16, epochs = 50, num_warmup_steps=0, learning_rate=5e-5):\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "\n",
    "    from transformers import BertForSequenceClassification, AdamW, BertConfig, RobertaConfig, AutoModelWithLMHead\n",
    "    from transformers import DistilBertForSequenceClassification, RobertaForSequenceClassification\n",
    "    \n",
    "    from torch.utils.data import DataLoader, RandomSampler\n",
    "    \n",
    "    t_train_stance, t_train_ideology, t_train_mmd_symbol, t_train_mmd_symbol_ = preprocess_ideology(stance_labels_Train, ideology_labels_Train)\n",
    "    \n",
    "    datasetTrain = TensorDataset(input_idsTrain, attention_masksTrain, t_train_stance, t_train_ideology, t_train_mmd_symbol, t_train_mmd_symbol_)\n",
    "\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "    t_val_stance, t_val_ideology, t_val_mmd_symbol, t_val_mmd_symbol_  = preprocess_ideology(stance_labels_Val, ideology_labels_Val)\n",
    "    \n",
    "    \n",
    "    datasetVal = TensorDataset(input_idsVal, attention_masksVal, t_val_stance, t_val_ideology, t_val_mmd_symbol, t_val_mmd_symbol_)\n",
    "    \n",
    "    model = IdeologyDetectionClass(modelUsed)\n",
    "\n",
    "    # Tell pytorch to run this model on the GPU.\n",
    "    model.cuda()\n",
    "\n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "    # I believe the 'W' stands for 'Weight Decay fix\"\n",
    "    \n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  betas=(0.9, 0.999), \n",
    "                  eps=1e-08, \n",
    "                  weight_decay=1e-3,\n",
    "                  correct_bias=True\n",
    "               )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "            datasetTrain,  # The training samples.\n",
    "            sampler =  RandomSampler(datasetTrain), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size., \n",
    "            num_workers=8\n",
    "        )\n",
    "    batch_size = batch_size\n",
    "\n",
    "\n",
    "    from transformers import get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "    # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "    # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "    # training data.\n",
    "    epochs = epochs\n",
    "\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    schedulerOld = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "    \n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps, num_cycles = 5)\n",
    "    \n",
    "    loss_fct = torch.nn.BCELoss()\n",
    "    return model, datasetTrain, datasetVal, optimizer, schedulerOld, loss_fct\n",
    "\n",
    "    return model, datasetTrain, datasetVal, optimizer, schedulerOld\n",
    "\n",
    "def return_batches_datasets(datasetTrain, datasetVal, batch_size = 16):\n",
    "    from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "        \n",
    "    # Create the DataLoaders for our training and validation sets.\n",
    "    # We'll take training samples in random order. \n",
    "    train_dataloader = DataLoader(\n",
    "            datasetTrain,  # The training samples.\n",
    "            sampler =  RandomSampler(datasetTrain), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size., \n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "    # For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "    validation_dataloader = DataLoader(\n",
    "            datasetVal, # The validation samples.\n",
    "            sampler = SequentialSampler(datasetVal), # Pull out batches sequentially.\n",
    "            batch_size = batch_size, # Evaluate with this batch size.\n",
    "            num_workers=0\n",
    "        )\n",
    "    \n",
    "    \n",
    "    #validation_dataloader = DataLoader(\n",
    "    #        datasetVal, # The validation samples.\n",
    "    #        sampler = SequentialSampler(datasetVal), # Pull out batches sequentially.\n",
    "    #        batch_size = batch_size, # Evaluate with this batch size.\n",
    "    #        num_workers=0, drop_last=True\n",
    "    #)\n",
    "    \n",
    "    return train_dataloader, validation_dataloader\n",
    "\n",
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "#from tensorboardX import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#import EarlyStopping\n",
    "def train_stance_ideology_ambigious(train_nums, val_nums, train_nums_ideology, val_nums_ideology, model_save_path, \n",
    "                                    model, datasetTrain, datasetVal, epochs, batch_size, optimizer, scheduler, patience, verbose, delta, seedVal, continue_train = False):\n",
    "    \n",
    "    pro_val_num = val_nums[0]\n",
    "    agst_val_num = val_nums[1]\n",
    "    neut_val_num = val_nums[2] + 0.01\n",
    "    notrel_val_num = val_nums[3]\n",
    "    \n",
    "    stance_all_num = pro_val_num + agst_val_num + neut_val_num + notrel_val_num\n",
    "    \n",
    "    con_val_num = 0.1\n",
    "    lib_val_num = 0.1\n",
    "    na_val_num = 0.1\n",
    "    \n",
    "    con_train_num = 0.1\n",
    "    lib_train_num = 0.1\n",
    "    na_train_num = 0.1\n",
    "    \n",
    "    #con_train_num = train_nums_ideology[0]\n",
    "    #lib_train_num = train_nums_ideology[1]\n",
    "    #na_train_num = train_nums_ideology[2]\n",
    "    \n",
    "    my_max_train_stance = max(pro_val_num, agst_val_num, neut_val_num, notrel_val_num)\n",
    "    my_max_train = max(con_train_num, lib_train_num, na_train_num)\n",
    "    \n",
    "    #con_val_num = val_nums_ideology[0]\n",
    "    #lib_val_num = val_nums_ideology[1]\n",
    "    #na_val_num = val_nums_ideology[2]\n",
    "    \n",
    "    my_max = max(con_val_num, lib_val_num, na_val_num)\n",
    "    \n",
    "    ideology_all_num = con_val_num + lib_val_num + na_val_num\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    min_val_loss = 100\n",
    "    \n",
    "    relatedness_size = 2\n",
    "    classes_size = 4\n",
    "    loss_fct_relatedness = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    loss_fct_stance = torch.nn.CrossEntropyLoss()\n",
    "    #loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    alpha = 1.5\n",
    "    beta = 1e-3\n",
    "    theta = 0\n",
    "    gamma = 0\n",
    "    \n",
    "    batch_size_max_once = 16\n",
    "\n",
    "    if batch_size < batch_size_max_once:\n",
    "        batch_size_max_once = batch_size\n",
    "        \n",
    "    accumulation_steps = batch_size/batch_size_max_once\n",
    "    \n",
    "    es = EarlyStopping(patience,verbose, delta)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    train_dataloader, validation_dataloader = return_batches_datasets(datasetTrain, datasetVal, batch_size_max_once)\n",
    "    \n",
    "    epoch_start = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        #multi-gpu\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            \n",
    "    print(device)\n",
    "    \n",
    "    \n",
    "            \n",
    "    if continue_train:    \n",
    "        #'./model_save/fnc/model_emergentbert_epoch90_withoutsep_serp.t7'\n",
    "        checkpoint = torch.load(model_save_path)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    \n",
    "     #pos_weight=torch.FloatTensor ([28.36 / 0.5090]\n",
    "    \n",
    "     #pos_weight = torch.tensor([1.0, 1.0, 1.0])\n",
    "     #pos_weight = pos_weight.to(device)\n",
    "     #criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    weights_ideology = torch.tensor([my_max_train/con_train_num, my_max_train/lib_train_num, my_max_train/na_train_num]).to(device)   \n",
    "    weights_stance = torch.tensor([my_max_train_stance/pro_val_num, my_max_train_stance/agst_val_num, my_max_train_stance/neut_val_num, my_max_train_stance/notrel_val_num]).to(device) \n",
    "    loss_fct_relatedness_weighted = torch.nn.BCEWithLogitsLoss(pos_weight = weights_stance)\n",
    "    loss_fct_ideology_weighted = torch.nn.BCEWithLogitsLoss(pos_weight = weights_ideology)\n",
    "    \n",
    "    # For each epoch...\n",
    "    batch_epoch_count = 1\n",
    "    for epoch_i in range(epoch_start, epoch_start + epochs):\n",
    "        \n",
    "        print(\"---------Epoch----------\" + str(epoch_i))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "    \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        #print(\"\")\n",
    "        #print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        #print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        # For each batch of training data...\n",
    "        mini_batch_avg_loss = 0\n",
    "        #train_size = len(train_dataloader)\n",
    "        \n",
    "        if batch_epoch_count % 500 == 0:\n",
    "            batch_size = batch_size*2\n",
    "            accumulation_steps = int(batch_size/batch_size_max_once)\n",
    "        batch_epoch_count = batch_epoch_count + 1\n",
    "\n",
    "        #train_size = len(train_dataloader) / float(accumulation_steps)\n",
    "        \n",
    "        print(\"Batch Size: \" + str(batch_size))\n",
    "        print(float(accumulation_steps))\n",
    "        \n",
    "        #print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_relatedness = batch[2].to(device)\n",
    "            b_labels = batch[3].to(device)\n",
    "            b_mmd_symbol = batch[4].to(device)\n",
    "            b_mmd_symbol_ = batch[5].to(device)\n",
    "            b_existedstances = batch[6].to(device)\n",
    "            b_ideologies = batch[7].to(device)\n",
    "        \n",
    "            \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "\n",
    "            #mmd_loss, P_relatedness, P_stance, P_existedstance = model(input_ids = b_input_ids, attention_mask = b_input_mask, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "            P_stance = model(input_ids = b_input_ids, attention_mask = b_input_mask, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "                \n",
    "                \n",
    "                \n",
    "            #relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "            stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "            #existedstance_loss = loss_fct_relatedness(P_existedstance, b_existedstances.float())\n",
    "\n",
    "            \n",
    "            #loss = alpha * stance_loss + theta * existedstance_loss + beta * mmd_loss + relatedness_loss\n",
    "            loss = stance_loss\n",
    "            loss = loss / accumulation_steps \n",
    "            total_train_loss += loss.item()\n",
    "                \n",
    "            loss.backward()\n",
    "            if (step+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "                    \n",
    "\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                # This is to help prevent the \"exploding gradients\" problem.\n",
    "                    #torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "                    \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                \n",
    "                # Update parameters and take a step using the computed gradient.\n",
    "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "                # modified based on their gradients, the learning rate, etc.\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the learning rate.\n",
    "                scheduler.step()\n",
    "                \n",
    "                #for param_group in optimizer.param_groups:\n",
    "                #print(\"Learning Rate: \", optimizer.param_groups[\"lr\"])\n",
    "                \n",
    "                                \n",
    "                # Always clear any previously calculated gradients before performing a\n",
    "                # backward pass. PyTorch doesn't do this automatically because \n",
    "                # accumulating the gradients is \"convenient while training RNNs\". \n",
    "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()       \n",
    "    \n",
    "        print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader) * accumulation_steps\n",
    "        #avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        #print(\"\")\n",
    "        print(\"  Average training loss: {0:.6f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        #print(\"\")\n",
    "        #print(\"Running Validation...\")\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_true_eval_stance = 0\n",
    "        total_true_eval_ideology = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        \n",
    "        agree_val_true = 0\n",
    "        disagree_val_true = 0 \n",
    "        discuss_val_true = 0 \n",
    "        unrelated_val_true = 0\n",
    "        \n",
    "        con_val_true = 0\n",
    "        lib_val_true = 0\n",
    "        na_val_true = 0\n",
    "        \n",
    "        total_true = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "        \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_relatedness = batch[2].to(device)\n",
    "            b_labels = batch[3].to(device)\n",
    "            b_mmd_symbol = batch[4].to(device)\n",
    "            b_mmd_symbol_ = batch[5].to(device)\n",
    "            b_existedstances = batch[6].to(device)\n",
    "            b_ideologies = batch[7].to(device)\n",
    "        \n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "\n",
    "                #mmd_loss, P_relatedness, P_stance, P_existedstance = model(input_ids = b_input_ids, attention_mask = b_input_mask, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "                P_stance = model(input_ids = b_input_ids, attention_mask = b_input_mask, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "\n",
    "                \n",
    "                #CrossEntropy Loss\n",
    "                #relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "                stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "                #existedstance_loss = loss_fct_relatedness(P_existedstance, b_existedstances.float())\n",
    "                \n",
    "                #loss_val = alpha * stance_loss + beta * mmd_loss + relatedness_loss\n",
    "                loss_val = stance_loss\n",
    "                total_eval_loss += loss_val.item()\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                #P_relatedness = P_relatedness.to('cpu')\n",
    "                #b_relatedness = b_relatedness.to('cpu')\n",
    "                P_stance = P_stance.to('cpu')\n",
    "                b_labels = b_labels.to('cpu')\n",
    "                #P_existedstance = P_existedstance.to('cpu')\n",
    "                #b_existedstances = b_existedstances.to('cpu')\n",
    "                \n",
    "                \n",
    "\n",
    "                # Calculate the accuracy for this batch of test sentences, and\n",
    "                # accumulate it over all batches.\n",
    "                #total_eval_accuracy += predict(P_relatedness, P_stance, b_labels)\n",
    "\n",
    "                #acc_list = predict_classwise_stance_ideology(P_relatedness, P_stance, P_existedstance, b_labels)\n",
    "                acc_list = predict_classwise_stance_ideology_bert(P_stance, b_labels)\n",
    "                total_true_eval_stance += acc_list[0]\n",
    "                ###\n",
    "                agree_val_true += acc_list[1]\n",
    "                disagree_val_true += acc_list[2]\n",
    "                discuss_val_true += acc_list[3]\n",
    "                unrelated_val_true += acc_list[4]\n",
    "                \n",
    "                total_true_eval_ideology += acc_list[5]\n",
    "                con_val_true += acc_list[6]\n",
    "                lib_val_true += acc_list[7]\n",
    "                na_val_true += acc_list[8]\n",
    "                \n",
    "                predict_labels = acc_list[9]\n",
    "                \n",
    "                                \n",
    "                ##print(\"Batch Next\")\n",
    "                #for idx in range(0, len(P_stance)):\n",
    "                    \n",
    "                    #print(P_stance[idx], b_labels[idx], acc_list[9][idx]) \n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy_stance = total_true_eval_stance / stance_all_num\n",
    "        avg_val_accuracy_ideology = total_true_eval_ideology / ideology_all_num\n",
    "        print(\"Avg Val Accuracy Stance: {0:.6f}\".format(avg_val_accuracy_stance))\n",
    "        print(\"Avg Val Accuracy Ideology: {0:.6f}\".format(avg_val_accuracy_ideology))\n",
    "        print(\"Total True\")\n",
    "        print(total_true)\n",
    "        print(\"*************\")\n",
    "        avg_val_agree_accuracy = agree_val_true / pro_val_num\n",
    "        print(\"Avg Val Agree Accuracy: {0:.6f}\".format(avg_val_agree_accuracy))\n",
    "        avg_val_disagree_accuracy = disagree_val_true / agst_val_num\n",
    "        print(\"Avg Val Disagree Accuracy: {0:.6f}\".format(avg_val_disagree_accuracy))\n",
    "        avg_val_discuss_accuracy = discuss_val_true / neut_val_num\n",
    "        print(\"Avg Val Discuss Accuracy: {0:.6f}\".format(avg_val_discuss_accuracy))\n",
    "        avg_val_unrelated_accuracy = unrelated_val_true / notrel_val_num\n",
    "        print(\"Avg Val Unrelated Accuracy: {0:.6f}\".format(avg_val_unrelated_accuracy))\n",
    "        \n",
    "        relative_score = 0.25*avg_val_unrelated_accuracy + 0.75*(avg_val_agree_accuracy + avg_val_disagree_accuracy + avg_val_discuss_accuracy)/3\n",
    "        \n",
    "        print(\"*****************\")\n",
    "        print(\"Relative score: {0:.6f}\".format(relative_score))\n",
    "        print(\"*****************\")\n",
    "        print(\"-------------\")\n",
    "        avg_val_con_accuracy = con_val_true / con_val_num\n",
    "        print(\"Avg Val Con Accuracy: {0:.6f}\".format(avg_val_con_accuracy))\n",
    "        avg_lib_accuracy = lib_val_true / lib_val_num\n",
    "        print(\"Avg Val Lib Accuracy: {0:.6f}\".format(avg_lib_accuracy))\n",
    "        avg_na_discuss_accuracy = na_val_true / na_val_num\n",
    "        print(\"Avg Val NA Accuracy: {0:.6f}\".format(avg_na_discuss_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        \n",
    "        print(\"Total Validation loss\", total_eval_loss)\n",
    "        print(\"Len-validation loader\", len(validation_dataloader))\n",
    "    \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t1)\n",
    "        \n",
    "        if avg_val_loss < min_val_loss:\n",
    "            min_val_loss = avg_val_loss\n",
    "    \n",
    "        print(\"Avg Validation Loss: {0:.6f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        #avg_val_accuracy_ideology = 0\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Stance Accur.': avg_val_accuracy_stance,\n",
    "            'Valid. Ideology Accur.': avg_val_accuracy_ideology,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        model_save_state = {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "    \n",
    "        es.__call__(avg_val_loss, avg_val_accuracy_stance, avg_val_accuracy_ideology, model_save_state, model_save_path, model)\n",
    "        last_epoch = epoch_i + 1\n",
    "        if es.early_stop == True:\n",
    "            break  # early stop criterion is met, we can stop now\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    \n",
    "    \n",
    "    min_val_loss = es.val_loss_min\n",
    "    max_val_acc = es.val_acc_max_stance\n",
    "\n",
    "    return training_stats, last_epoch, min_val_loss, max_val_acc\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#import EarlyStopping\n",
    "def train_stance(model_save_path, model, tokenizer, datasetTrain, datasetVal, epochs, batch_size, optimizer, scheduler, patience, verbose, delta, seedVal, continue_train = False):\n",
    "    \n",
    "    #loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "    loss_fct = torch.nn.BCELoss()\n",
    "    create_determinism(seedVal)\n",
    "    \n",
    "    min_val_loss = 100\n",
    "    \n",
    "    relatedness_size = 2\n",
    "    classes_size = 2\n",
    "    \n",
    "    alpha = 1.3\n",
    "    theta = 0.8\n",
    "    beta = 1e-2\n",
    "    \n",
    "    batch_size_max_once = 16    \n",
    "    \n",
    "\n",
    "    if batch_size < batch_size_max_once:\n",
    "        batch_size_max_once = batch_size\n",
    "        \n",
    "    accumulation_steps = batch_size/batch_size_max_once\n",
    "    \n",
    "    es = EarlyStopping(patience,verbose, delta)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    train_dataloader, validation_dataloader = return_batches_datasets(datasetTrain, datasetVal, batch_size_max_once)\n",
    "    \n",
    "    epoch_start = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        #multi-gpu\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            \n",
    "    print(device)\n",
    "          \n",
    "    continue_train = False\n",
    "    if continue_train:\n",
    "        checkpoint = torch.load('models/2_a/')\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    # For each epoch...\n",
    "    batch_epoch_count = 1\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(\"---------Epoch----------\" + str(epoch_i))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "    \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        # For each batch of training data...\n",
    "        mini_batch_avg_loss = 0\n",
    "        \n",
    "        \n",
    "        if batch_epoch_count % 200 == 0:\n",
    "            batch_size = batch_size*2\n",
    "            accumulation_steps = int(batch_size/batch_size_max_once)\n",
    "        batch_epoch_count = batch_epoch_count + 1\n",
    "        \n",
    "        train_size = len(train_dataloader) / accumulation_steps\n",
    "        \n",
    "        print(\"Batch Size: \" + str(batch_size))\n",
    "        print(accumulation_steps)\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            #b_stance = batch[2].to(device)\n",
    "            b_ideology = batch[2].to(device)\n",
    "            #b_mmd_symbol = batch[4].to(device)\n",
    "            #b_mmd_symbol_ = batch[5].to(device)\n",
    "            \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "\n",
    "            P_ideology = model(input_ids = b_input_ids, attention_mask = b_input_mask)      \n",
    "            ideology_loss = loss_fct(P_ideology, b_ideology.float())\n",
    "\n",
    "            loss = ideology_loss\n",
    "\n",
    "            #loss = torch.sum(loss, dim=0)\n",
    "\n",
    "\n",
    "            # Accumulate the training loss over all of t0e batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            #loss_train = loss\n",
    "            loss_train = loss / accumulation_steps\n",
    "            # Calculate the average loss over all of the batches.\n",
    "            \n",
    "            #loss_length = torch.numel(loss_train)\n",
    "            #fill_length = batch_size_max_once-loss_length\n",
    "            #cat_tensor = torch.zeros(fill_length, device=device)\n",
    "\n",
    "            #if loss_length < batch_size_max_once:\n",
    "                #loss_train = torch.cat([loss_train, cat_tensor], dim=0)\n",
    "                \n",
    "            mini_batch_avg_loss += loss_train.item()\n",
    "            \n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss_train.backward()\n",
    "            if (step+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                # This is to help prevent the \"exploding gradients\" problem.\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                \n",
    "                # Update parameters and take a step using the computed gradient.\n",
    "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "                # modified based on their gradients, the learning rate, etc.\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the learning rate.\n",
    "                scheduler.step()\n",
    "                \n",
    "                #for param_group in optimizer.param_groups:\n",
    "                \n",
    "                                \n",
    "                # Always clear any previously calculated gradients before performing a\n",
    "                # backward pass. PyTorch doesn't do this automatically because \n",
    "                # accumulating the gradients is \"convenient while training RNNs\". \n",
    "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                #total_train_loss = 0           \n",
    "                \n",
    "                total_train_loss += mini_batch_avg_loss\n",
    "                mini_batch_avg_loss = 0\n",
    "    \n",
    "        print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        \n",
    "        avg_train_loss = total_train_loss / train_size\n",
    "    \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Average training loss: {0:.6f}\".format(avg_train_loss))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        total_eval_stanceloss = 0\n",
    "        total_eval_ideologicalloss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "        \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            #b_stance = batch[2].to(device)\n",
    "            b_ideology = batch[2].to(device)\n",
    "            #b_mmd_symbol = batch[4].to(device)\n",
    "            #b_mmd_symbol_ = batch[5].to(device)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                \n",
    "                \n",
    "                P_ideology = model(input_ids = b_input_ids, attention_mask = b_input_mask)\n",
    "\n",
    "                ideology_loss = loss_fct(P_ideology, b_ideology.float())\n",
    "\n",
    "                loss_val = ideology_loss\n",
    "                #loss_val = torch.sum(loss, dim=0).item()\n",
    "                \n",
    "                #logits = model(input_ids = b_input_ids,attention_mask=b_input_mask)\n",
    "                \n",
    "                #loss = loss_function(logits, b_labels)\n",
    "            \n",
    "                # Accumulate the validation loss.\n",
    "                total_eval_loss += loss_val.item()\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                P_ideology = P_ideology.to('cpu')\n",
    "                b_ideology = b_ideology.to('cpu')\n",
    "\n",
    "                # Calculate the accuracy for this batch of test sentences, and\n",
    "                # accumulate it over all batches.\n",
    "                total_eval_accuracy += predict_binary(P_ideology, b_ideology)\n",
    "        \n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Accuracy: {0:.6f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        \n",
    "        if avg_val_loss < min_val_loss:\n",
    "            min_val_loss = avg_val_loss\n",
    "    \n",
    "        print(\"Avg Validation Loss: {0:.6f}\".format(avg_val_loss))\n",
    "        #print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "    \n",
    "        model_save_state = {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "    \n",
    "        es.__call__(avg_val_loss, avg_val_accuracy, model_save_state, model_save_path)\n",
    "        last_epoch = epoch_i + 1\n",
    "        if es.early_stop == True:\n",
    "            break  # early stop criterion is met, we can stop now\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    \n",
    "    \n",
    "    min_val_loss = es.val_loss_min\n",
    "    max_val_acc = es.val_acc_max\n",
    "\n",
    "    return training_stats, last_epoch, min_val_loss, max_val_acc\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#import EarlyStopping\n",
    "def train_stance(model_save_path, model, tokenizer, datasetTrain, datasetVal, epochs, batch_size, optimizer, scheduler, patience, verbose, delta, seedVal, continue_train = False):\n",
    "    \n",
    "    #loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "    loss_fct = torch.nn.BCELoss()\n",
    "    create_determinism(seedVal)\n",
    "    \n",
    "    min_val_loss = 100\n",
    "    \n",
    "    relatedness_size = 2\n",
    "    classes_size = 2\n",
    "    \n",
    "    alpha = 1.3\n",
    "    theta = 0.8\n",
    "    beta = 1e-2\n",
    "    \n",
    "    batch_size_max_once = 16    \n",
    "    \n",
    "\n",
    "    if batch_size < batch_size_max_once:\n",
    "        batch_size_max_once = batch_size\n",
    "        \n",
    "    accumulation_steps = batch_size/batch_size_max_once\n",
    "    \n",
    "    es = EarlyStopping(patience,verbose, delta)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    train_dataloader, validation_dataloader = return_batches_datasets(datasetTrain, datasetVal, batch_size_max_once)\n",
    "    \n",
    "    epoch_start = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        #multi-gpu\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            \n",
    "    print(device)\n",
    "            \n",
    "    if continue_train:\n",
    "        checkpoint = torch.load('models/2_a/')\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    # For each epoch...\n",
    "    batch_epoch_count = 1\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(\"---------Epoch----------\" + str(epoch_i))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "    \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        # For each batch of training data...\n",
    "        mini_batch_avg_loss = 0\n",
    "        \n",
    "        \n",
    "        if batch_epoch_count % 200 == 0:\n",
    "            batch_size = batch_size*2\n",
    "            accumulation_steps = int(batch_size/batch_size_max_once)\n",
    "        batch_epoch_count = batch_epoch_count + 1\n",
    "        \n",
    "        train_size = len(train_dataloader) / accumulation_steps\n",
    "        \n",
    "        print(\"Batch Size: \" + str(batch_size))\n",
    "        print(accumulation_steps)\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            #b_stance = batch[2].to(device)\n",
    "            b_ideology = batch[2].to(device)\n",
    "            #b_mmd_symbol = batch[4].to(device)\n",
    "            #b_mmd_symbol_ = batch[5].to(device)\n",
    "            \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "\n",
    "            P_ideology = model(input_ids = b_input_ids, attention_mask = b_input_mask)      \n",
    "            ideology_loss = loss_fct(P_ideology, b_ideology.float())\n",
    "\n",
    "            loss = ideology_loss\n",
    "\n",
    "            #loss = torch.sum(loss, dim=0)\n",
    "\n",
    "\n",
    "            # Accumulate the training loss over all of t0e batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            #loss_train = loss\n",
    "            loss_train = loss / accumulation_steps\n",
    "            # Calculate the average loss over all of the batches.\n",
    "            \n",
    "            #loss_length = torch.numel(loss_train)\n",
    "            #fill_length = batch_size_max_once-loss_length\n",
    "            #cat_tensor = torch.zeros(fill_length, device=device)\n",
    "\n",
    "            #if loss_length < batch_size_max_once:\n",
    "                #loss_train = torch.cat([loss_train, cat_tensor], dim=0)\n",
    "                \n",
    "            mini_batch_avg_loss += loss_train.item()\n",
    "            \n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss_train.backward()\n",
    "            if (step+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                # This is to help prevent the \"exploding gradients\" problem.\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                \n",
    "                # Update parameters and take a step using the computed gradient.\n",
    "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "                # modified based on their gradients, the learning rate, etc.\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the learning rate.\n",
    "                scheduler.step()\n",
    "                \n",
    "                #for param_group in optimizer.param_groups:\n",
    "                \n",
    "                                \n",
    "                # Always clear any previously calculated gradients before performing a\n",
    "                # backward pass. PyTorch doesn't do this automatically because \n",
    "                # accumulating the gradients is \"convenient while training RNNs\". \n",
    "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                #total_train_loss = 0           \n",
    "                \n",
    "                total_train_loss += mini_batch_avg_loss\n",
    "                mini_batch_avg_loss = 0\n",
    "    \n",
    "        print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        \n",
    "        avg_train_loss = total_train_loss / train_size\n",
    "    \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Average training loss: {0:.6f}\".format(avg_train_loss))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        total_eval_stanceloss = 0\n",
    "        total_eval_ideologicalloss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "        \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            #b_stance = batch[2].to(device)\n",
    "            b_ideology = batch[2].to(device)\n",
    "            #b_mmd_symbol = batch[4].to(device)\n",
    "            #b_mmd_symbol_ = batch[5].to(device)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                \n",
    "                \n",
    "                P_ideology = model(input_ids = b_input_ids, attention_mask = b_input_mask)\n",
    "\n",
    "                ideology_loss = loss_fct(P_ideology, b_ideology.float())\n",
    "\n",
    "                loss_val = ideology_loss\n",
    "                #loss_val = torch.sum(loss, dim=0).item()\n",
    "                \n",
    "                #logits = model(input_ids = b_input_ids,attention_mask=b_input_mask)\n",
    "                \n",
    "                #loss = loss_function(logits, b_labels)\n",
    "            \n",
    "                # Accumulate the validation loss.\n",
    "                total_eval_loss += loss_val.item()\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                P_ideology = P_ideology.to('cpu')\n",
    "                b_ideology = b_ideology.to('cpu')\n",
    "\n",
    "                # Calculate the accuracy for this batch of test sentences, and\n",
    "                # accumulate it over all batches.\n",
    "                total_eval_accuracy += predict_binary(P_ideology, b_ideology)\n",
    "        \n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Accuracy: {0:.6f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        \n",
    "        if avg_val_loss < min_val_loss:\n",
    "            min_val_loss = avg_val_loss\n",
    "    \n",
    "        print(\"Avg Validation Loss: {0:.6f}\".format(avg_val_loss))\n",
    "        #print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "    \n",
    "        model_save_state = {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "    \n",
    "        es.__call__(avg_val_loss, avg_val_accuracy, model_save_state, model_save_path)\n",
    "        last_epoch = epoch_i + 1\n",
    "        if es.early_stop == True:\n",
    "            break  # early stop criterion is met, we can stop now\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    \n",
    "    \n",
    "    min_val_loss = es.val_loss_min\n",
    "    max_val_acc = es.val_acc_max\n",
    "\n",
    "    return training_stats, last_epoch, min_val_loss, max_val_acc\n",
    "\n",
    "def print_summary(training_stats):\n",
    "    # Display floats with two decimal places.\n",
    "    pd.set_option('precision', 4)\n",
    "    \n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "\n",
    "    # Create a DataFrame from our training statistics.\n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "    # Use the 'epoch' as the row index.\n",
    "    df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "    # A hack to force the column headers to wrap.\n",
    "    #df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "\n",
    "    # Display the table.\n",
    "    return df_stats\n",
    "\n",
    "def plot_results(df_stats, last_epoch):\n",
    "    # Use plot styling from seaborn.\n",
    "    sns.set(style='darkgrid')\n",
    "\n",
    "    # Increase the plot size and font size.\n",
    "    sns.set(font_scale=1.5)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "    \n",
    "    plot1 = plt.figure(1)\n",
    "    \n",
    "    plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training_Loss\")\n",
    "    plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Val_Loss\")\n",
    "\n",
    "    # Label the plot.\n",
    "    plt.title(\"Training & Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    #plt.autoscale(enable=True, axis='x')\n",
    "\n",
    "    x_ticks = []\n",
    "    for currEpoch in range(1, last_epoch+1):\n",
    "        x_ticks.append(currEpoch)\n",
    "    #plt.xticks(x_ticks)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def run_wholeprocess_fnc(tokenizer, model_current, train_path, val_path, max_len, doc_stride, batch_size, num_warmup_steps, learning_rate, seedVal):\n",
    "\n",
    "#--------------LOAD DATASETS--------------#\n",
    "\n",
    "    model_save_path = './models/BERT_SERP/bert_titleonly_ambigious'  \n",
    "    df = load_dataset_ambigious(\"./dataset/batches_cleaned/stance/FullDataset_16.09.2021.tsv\")\n",
    "    \n",
    "    #trainpath = \"./dataset/ideology/train_new.tsv\"\n",
    "    #valPath = \"./dataset/ideology/val_new.tsv\"\n",
    "    #testPath = \"./dataset/ideology/test_new.tsv\"\n",
    "    \n",
    "    trainPer = 0.8\n",
    "    valPer = 0.2\n",
    "    testPer = 0.2\n",
    "    \n",
    "    df, dfVal, dfTest = sample_dataset_stance(df, seedVal)\n",
    "    \n",
    "    #create_new_splits_and_writethem_to_csvfiles\n",
    "    #create_train_val_test_split(trainpath, valPath, testPath, testPer, valPer, seedVal)\n",
    "    \n",
    "    ##df = load_dataset(trainpath)\n",
    "    #dfVal = load_dataset(valPath)\n",
    "    #dfTest = load_dataset(testPath)\n",
    "    \n",
    "    # Report the number of sentences.\n",
    "    print('Number of training sentences: {:,}'.format(df.shape[0]))\n",
    "    print('Number of val sentences: {:,}'.format(dfVal.shape[0]))\n",
    "    print('Number of test sentences: {:,}'.format(dfTest.shape[0]))\n",
    "\n",
    "    sentencesQueryTitle_Train = []\n",
    "    sentencesQueryTitleCont_Train = []\n",
    "    stances_Train = []\n",
    "    labels_Train = []\n",
    "    \n",
    "    #--------------DATASETS-------------#\n",
    "\n",
    "    #sentencesQueryTitle_Train, sentencesQueryTitleCont_Train, sentencesQueryTitleStance_Train, sentencesQueryTitleStanceCont_Train, stances_Train, labels_Train = generate_datasets_ideology (df, tokenizer)\n",
    "    sentencesQueryTitle_Train, sentencesQueryTitleCont_Train, labels_Train = generate_datasets_ambigious(df, tokenizer)\n",
    "    \n",
    "    \n",
    "    \n",
    "    sentencesQueryTitle_Val = []\n",
    "    sentencesQueryTitleCont_Val = []\n",
    "    stances_Val = []\n",
    "    labels_Val = []\n",
    "\n",
    " \n",
    "    #sentencesQueryTitle_Val, sentencesQueryTitleCont_Val, sentencesQueryTitleStance_Val, sentencesQueryTitleStanceCont_Val, stances_Val, labels_Val = generate_datasets_ideology (dfVal, tokenizer)\n",
    "    sentencesQueryTitle_Val, sentencesQueryTitleCont_Val, labels_Val = generate_datasets_ambigious(dfVal, tokenizer)\n",
    "    \n",
    "    sentencesQueryTitle_Test = []\n",
    "    sentencesQueryTitleCont_Test = []\n",
    "    stances_Test = []\n",
    "    labels_Test = []\n",
    "   \n",
    "    #sentencesQueryTitle_Test, sentencesQueryTitleCont_Test, sentencesQueryTitleStance_Test, sentencesQueryTitleStanceCont_Test, stances_Test, labels_Test = generate_datasets_ideology (dfTest, tokenizer)\n",
    "    sentencesQueryTitle_Test, sentencesQueryTitleCont_Test, labels_Test = generate_datasets_ambigious(dfTest, tokenizer)\n",
    "    \n",
    "    print(sentencesQueryTitle_Train[0])\n",
    "\n",
    "    #--------------DATASETS-------------#\n",
    "\n",
    "    all_input_ids_Train, all_input_masks_Train  = preprocessing_for_bert(tokenizer, sentencesQueryTitleCont_Train, max_len, doc_stride)\n",
    "    all_input_ids_Val, all_input_masks_Val  = preprocessing_for_bert(tokenizer, sentencesQueryTitleCont_Val, max_len, doc_stride)\n",
    "    all_input_ids_Test, all_input_masks_Test  = preprocessing_for_bert(tokenizer, sentencesQueryTitleCont_Test, max_len, doc_stride)\n",
    "    \n",
    "    #all_input_ids_Train, all_input_masks_Train, stance_labels_Train, ideology_labels_Train = transform_sequences_longer_ideology(tokenizer, sentencesQueryTitleCont_Train, stances_Train, labels_Train, max_len, doc_stride) #train\n",
    "    #all_input_ids_Val, all_input_masks_Val, stance_labels_Val, ideology_labels_Val = transform_sequences_longer_ideology(tokenizer, sentencesQueryTitleCont_Val, stances_Val, labels_Val, max_len, doc_stride) #val\n",
    "    #all_input_ids_Test, all_input_masks_Test, stance_labels_Test, ideology_labels_Test = transform_sequences_longer_ideology(tokenizer, sentencesQueryTitleCont_Test, stances_Test, labels_Test, max_len, doc_stride) #test\n",
    "\n",
    "    model, datasetTrain, datasetVal, optimizer, scheduler = prepare_for_training_ambigious(all_input_ids_Train, all_input_masks_Train, labels_Train, all_input_ids_Val,\n",
    "                                                                                                               all_input_masks_Val, labels_Val, model_current, batch_size, epochs, num_warmup_steps, learning_rate)    \n",
    "    training_stats, last_epoch, min_val_loss, max_val_acc = train_stance(model_save_path, model, tokenizer, datasetTrain, datasetVal, epochs, batch_size, optimizer,\n",
    "                                                                          scheduler, patience, verbose, delta, seedVal)\n",
    "    \n",
    "    \n",
    "    avg_test_loss, avg_test_acc = run_test_ideology(model_save_path, all_input_ids_Test, all_input_masks_Test, stance_labels_Test, labels_Test, batch_size)\n",
    "    df_stats = print_summary(training_stats)\n",
    "    plot_results(df_stats, last_epoch)\n",
    "    \n",
    "    #--------------TRAINING-------------#\n",
    "    batch_size_cuda = 16\n",
    "    if batch_size < 16:\n",
    "        batch_size_cuda = batch_size\n",
    "        \n",
    "    num_iterations = 5\n",
    "    total_val_loss = 0.0\n",
    "    total_val_acc = 0.0\n",
    "    total_test_loss = 0.0\n",
    "    total_test_acc = 0.0\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        value = randint(0, 100)\n",
    "        seedVal = value\n",
    "        print(\"******************\")\n",
    "        print(\"This is the iteration \" + str(i))\n",
    "        \n",
    "        model_save_path = \"model_save/ideology/model_news2a_qtitle.t7\" + str(i)\n",
    "\n",
    "        model, datasetTrain, datasetVal, optimizer, scheduler = prepare_for_training(all_input_ids_Train, all_input_masks_Train, stance_labels_Train, ideology_labels_Train, all_input_ids_Val,\n",
    "                                                                                                               all_input_masks_Val, stance_labels_Val, ideology_labels_Val, model_current, batch_size_cuda, epochs, num_warmup_steps, learning_rate)    \n",
    "        training_stats, last_epoch, min_val_loss, max_val_acc = train_stance (model_save_path, model, tokenizer, datasetTrain, datasetVal, epochs, batch_size, optimizer,\n",
    "                                                                          scheduler, patience, verbose, delta, seedVal)\n",
    "\n",
    "        avg_test_loss, avg_test_acc = run_test_ideology(model_save_path, all_input_ids_Test, all_input_masks_Test, stance_labels_Test, ideology_labels_Test, batch_size)\n",
    "        df_stats = print_summary(training_stats)\n",
    "        plot_results(df_stats, last_epoch)\n",
    "        \n",
    "        total_val_loss += min_val_loss\n",
    "        total_val_acc += max_val_acc\n",
    "        total_test_loss += avg_test_loss\n",
    "        total_test_acc += avg_test_acc\n",
    "\n",
    "        print('Min Val Loss: ' + str(min_val_loss))\n",
    "        print('Max Val Acc: ' + str(max_val_acc))\n",
    "        print('Test Loss: ' + str(avg_test_loss))\n",
    "        print('Test Acc: ' + str(avg_test_acc))\n",
    "        \n",
    "        \n",
    "    print(\"******************\")\n",
    "    print('Avg Min Val Loss: ' + str(total_val_loss/num_iterations))\n",
    "    print('Avg Max Val Acc: ' + str(total_val_acc/num_iterations))\n",
    "    print('Avg Test Loss: ' + str(total_test_loss/num_iterations))\n",
    "    print('Avg Test Acc: ' + str(total_test_acc/num_iterations))\n",
    "    \n",
    "    \n",
    "    #model_to_save.save_pretrained('model_save')\n",
    "    #tokenizer.save_pretrained('model_save')\n",
    "\n",
    "    # Good practice: save your training arguments together with the trained model\n",
    "    #torch.save(args, os.path.join('model_save', 'training_args.bin'))\n",
    "    #model_args = str(max_len) + '_' + str(doc_stride) + '_' + str(batch_size) + \"_\" + str(learning_rate) + \"_warmup\" + str(num_warmup_steps) + \"_seedVal\" + str(seedVal)\n",
    "    #model_path = model_save_path + '/model_' + model_args\n",
    "    #model.save_pretrained(model_save_path)\n",
    "    #torch.save(model.state_dict(), model_path)\n",
    "\n",
    "def create_determinism(seedVal):\n",
    "    import os\n",
    "    torch.manual_seed(seedVal)\n",
    "    torch.cuda.manual_seed_all(seedVal)  \n",
    "    torch.cuda.manual_seed(seedVal)\n",
    "    np.random.seed(seedVal)\n",
    "    random.seed(seedVal)\n",
    "    #os.environ['PYTHONHASHSEED'] = str(seedVal)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    return avg_test_loss, avg_test_accuracy\n",
    "\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "def run_test_stance(model_savepath, all_input_ids_Test, all_input_masks_Test, stance_labels_Test, ideology_labels_Test, batch_size = 16):\n",
    "    #loss_fct = torch.nn.BCELoss()\n",
    "    loss_fct_relatedness = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    t_test_relatedness, t_test_stance, t_test_mmd_symbol, t_test_mmd_symbol_ = preprocess_fnc(stance_labels_Test)\n",
    "    # Create the DataLoader.\n",
    "    prediction_data = TensorDataset(all_input_ids_Test, all_input_masks_Test, t_test_relatedness, t_test_stance, t_test_mmd_symbol, t_test_mmd_symbol_)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size, num_workers=0)\n",
    "    \n",
    "    model_current = 'bert-base-uncased'\n",
    "    tokenizer = load_tokenizer(model_current)\n",
    "        \n",
    "    model = StanceDetectionClass(model_current)\n",
    "    checkpoint = torch.load(model_savepath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])    \n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  betas=(0.9, 0.999), \n",
    "                  eps=1e-08, \n",
    "                  weight_decay=1e-5,\n",
    "                  correct_bias=True\n",
    "    )\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    #model.cuda()\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_test_loss = 0.0\n",
    "    \n",
    "    total_test_accuracy = 0.0\n",
    "    predictions , true_labels = [], []\n",
    "    \n",
    "    alpha = 1.3\n",
    "    theta = 0.8\n",
    "    beta = 1e-3\n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "      #Add batch to GPU\n",
    "        \n",
    "        #batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_relatedness = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "        b_mmd_symbol = batch[4].to(device)\n",
    "        b_mmd_symbol_ = batch[5].to(device)\n",
    "  \n",
    "        # Telling the model not to compute or store gradients, saving memory and \n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():         \n",
    "            # Forward pass, calculate logit predictions\n",
    "            \n",
    "            n1 = torch.sum(b_mmd_symbol, dim=0)\n",
    "            n2 = torch.sum(b_mmd_symbol_, dim=0)\n",
    "        \n",
    "            aa = torch.reshape(b_mmd_symbol, (-1,1))\n",
    "            bb = torch.reshape(b_mmd_symbol_, (-1,1))\n",
    "            \n",
    "            theta_d_layer, P_relatedness, P_stance = model(input_ids = b_input_ids, attention_mask = b_input_mask)\n",
    "                \n",
    "            if n1 == 0:\n",
    "                d1 = torch.zeros(batch_size, 1, device = device)\n",
    "            else:\n",
    "                d1 = torch.div(torch.sum(theta_d_layer*aa, dim=1), n1)\n",
    "                \n",
    "            if n2 == 0:\n",
    "                d2 = torch.zeros(batch_size, 1, device = device)\n",
    "            else:\n",
    "                d2 = torch.div(torch.sum(theta_d_layer*bb, dim=1), n2)\n",
    "                    \n",
    "                    \n",
    "            mmd_loss = torch.sum(d1 - d2)\n",
    "                \n",
    "            \n",
    "            relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "            stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "                \n",
    "    \n",
    "            loss_test = relatedness_loss + alpha * stance_loss - beta * mmd_loss\n",
    "            total_test_loss += loss_test.item()\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            P_relatedness = P_relatedness.to('cpu')\n",
    "            b_relatedness = b_relatedness.to('cpu')\n",
    "            P_stance = P_stance.to('cpu')\n",
    "            b_labels = b_labels.to('cpu')\n",
    "\n",
    "            total_test_accuracy += predict(P_relatedness, P_stance, b_labels)\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_test_loss = total_test_loss / len(prediction_dataloader)\n",
    "    avg_test_accuracy = total_test_accuracy / len(prediction_dataloader)\n",
    "\n",
    "    return avg_test_loss, avg_test_accuracy\n",
    "\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "def run_test_ideology(model_save_path, all_input_ids_Test, all_input_masks_Test, stance_labels_Test, ideology_labels_Test, batch_size = 16):\n",
    "\n",
    "    t_ideology_labels_test = preprocess_ideology_new(stance_labels_Test, ideology_labels_Test)\n",
    "    # Create the DataLoader.\n",
    "    prediction_data = TensorDataset(all_input_ids_Test, all_input_masks_Test, t_ideology_labels_test)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "    loss_fct = torch.nn.BCELoss()\n",
    "    \n",
    "    model_current = 'bert-base-uncased'\n",
    "    tokenizer = load_tokenizer(model_current)\n",
    "        \n",
    "    model = IdeologyDetectionClass(model_current)\n",
    "    checkpoint = torch.load(model_save_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])    \n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  betas=(0.9, 0.999), \n",
    "                  eps=1e-08, \n",
    "                  weight_decay=1e-5,\n",
    "                  correct_bias=True\n",
    "    )\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "\n",
    "    model.to(device)\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_test_loss = 0.0\n",
    "    \n",
    "    total_test_accuracy = 0.0\n",
    "    predictions , true_labels = [], []\n",
    "    \n",
    "    alpha = 1.3\n",
    "    theta = 0.8\n",
    "    beta = 1e-3\n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "      #Add batch to GPU\n",
    "\n",
    "        #batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_ideologylabels = batch[2].to(device)\n",
    "  \n",
    "        # Telling the model not to compute or store gradients, saving memory and \n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():         \n",
    "            \n",
    "            \n",
    "            # Forward pass, calculate logit predictions\n",
    "            P_ideology = model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "            ideology_loss = loss_fct(P_ideology, b_ideologylabels.float())\n",
    "\n",
    "            loss = ideology_loss\n",
    "\n",
    "            #logits = outputs[0]\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            P_ideology = P_ideology.detach().cpu()\n",
    "            t_ideology_labels_test = b_ideologylabels.to('cpu')\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            total_test_loss += loss.item()\n",
    "            total_test_accuracy += predict_binary(P_ideology, t_ideology_labels_test)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_test_loss = total_test_loss / len(prediction_dataloader)\n",
    "    \n",
    "    avg_test_accuracy = total_test_accuracy / len(prediction_dataloader)\n",
    "  \n",
    "            # Store predictions and true labels\n",
    "            #predictions.append(logits)\n",
    "            #true_labels.append(label_ids)\n",
    "    #print('Test Accuracy', avg_test_accuracy)\n",
    "\n",
    "    return avg_test_loss, avg_test_accuracy\n",
    "\n",
    "### import os\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "from transformers import AutoModel\n",
    "from transformers import DistilBertModel\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "#model = \"bert-base-uncased\"\n",
    "train_path = './dataset/fnc/train'\n",
    "val_path = './dataset/fnc/test'\n",
    "\n",
    "model_save_path = './model_save/'\n",
    "model_name = 'querytitle_model_base_9'\n",
    "\n",
    "#device = run_utils()\n",
    "        \n",
    "model_base = 'bert-base-uncased'\n",
    "model_roberta = \"roberta-base\"\n",
    "model_finetuned = './models/2_a/'\n",
    "model_finetuned2 = './models/2_b/'\n",
    "model_tiny_bert = './models/tiny_bert/'\n",
    "    \n",
    "model_current = model_base\n",
    "tokenizer = load_tokenizer(model_current)\n",
    "\n",
    "max_len = 512\n",
    "doc_stride = 128\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "num_warmup_steps = 10\n",
    "learning_rate = 2e-6\n",
    "\n",
    "##-----Early Stopping\n",
    "patience = 4000\n",
    "verbose = True\n",
    "delta = 0.000001\n",
    "seedVal = 20\n",
    "\n",
    "train_flag = True\n",
    "\n",
    "if train_flag:\n",
    "    run_wholeprocess_fnc(tokenizer, model_current, train_path, val_path, max_len, doc_stride, batch_size, num_warmup_steps, learning_rate,seedVal)  \n",
    "else:\n",
    "    avg_test_loss, avg_test_acc = run_onlytest_ideology(tokenizer, model_save_path + model_name, torch.nn.BCELoss(), device)\n",
    "    print(avg_test_loss, avg_test_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_utils():\n",
    "    # Get the GPU device name.\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    # The device name should look like the following:\n",
    "    if device_name == '/device:GPU:0':\n",
    "        print('Found GPU at: {}'.format(device_name))\n",
    "    else:\n",
    "        raise SystemError('GPU device not found')\n",
    "\n",
    "    device = None\n",
    "    # If there's a GPU available...\n",
    "    if torch.cuda.is_available():    \n",
    "        # Tell PyTorch to use the GPU.    \n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    # If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def predict(P_ideology, ideology_labels):\n",
    "    predict_labels = torch.argmax(P_ideology, 1)\n",
    "    target_labels = torch.argmax(ideology_labels, 1)\n",
    "    \n",
    "\n",
    "    true_predict_count = len((torch.eq(predict_labels, target_labels)).nonzero().flatten())\n",
    "    accuracy = true_predict_count / len(predict_labels)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def predict_binary(P_ideology, ideology_labels):\n",
    "    predict_labels = np.round(P_ideology)\n",
    "    predict_labels = predict_labels.int()\n",
    "\n",
    "    true_predict_count = (torch.eq(predict_labels, ideology_labels)).sum()\n",
    "    true_predict_count = true_predict_count.numpy()\n",
    "    #print(true_predict_count)\n",
    "    \n",
    "    accuracy = true_predict_count / len(predict_labels)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def find_maxLen_doc(data, tokenizer):\n",
    "    max_len = 0\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "        input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        # Update the maximum sentence length.\n",
    "        max_len = max(max_len, len(input_ids))\n",
    "\n",
    "    print('Max sentence length: ', max_len)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def load_tokenizer(model):\n",
    "    tokenizer = None\n",
    "    from transformers import AutoTokenizer, DistilBertTokenizer, BertTokenizer, RobertaTokenizer, AutoModelWithLMHead\n",
    "    tokenizer = BertTokenizer.from_pretrained(model, do_lower_case=True)\n",
    "    #tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def load_dataset(path):\n",
    "    # Load the dataset into a pandas dataframe.\n",
    "    df = pd.read_csv(path, delimiter='\\t', header=0, names=['qID', 'q_ideology', 'ideology', 'stance', 'docCont', 'topic', 'Q', 'title'])       \n",
    "\n",
    "    df['docCont'] = df['docCont'].str.lower()\n",
    "    #df['topic'] = df['topic'].str.lower()\n",
    "    df['Q'] = df['Q'].str.lower()\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    \n",
    "    #df.insert(0, \"stanceStr\", df['stance'], True)\n",
    "    #df[\"stanceStr\"] = df[\"stanceStr\"].replace({1: \"Pro\", 0: \"Agst\"})\n",
    "    \n",
    "    print(\"Train\")\n",
    "    print (\"Con\", df[df.ideology == 0].shape[0])\n",
    "    print (\"Lib\", df[df.ideology == 1].shape[0])\n",
    "    print (\"Pro\", df[df.stance == 1].shape[0])\n",
    "    print (\"Against\", df[df.stance == 0].shape[0])\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_dataset_ambigious(path):\n",
    "    # Load the dataset into a pandas dataframe.\n",
    "    df = pd.read_csv(path, delimiter='\\t', header=0, names=['qID', 'docID', 'stance', 'Ambigious', 'ideology', 'docCont', 'Q', 'title'])       \n",
    "\n",
    "    df['docCont'] = df['docCont'].str.lower()\n",
    "    #df['topic'] = df['topic'].str.lower()\n",
    "    df['Q'] = df['Q'].str.lower()\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    \n",
    "    df = df.astype(str)\n",
    "    \n",
    "    #df.insert(0, \"stanceStr\", df['stance'], True)\n",
    "    #df[\"stanceStr\"] = df[\"stanceStr\"].replace({1: \"Pro\", 0: \"Agst\"})\n",
    "    \n",
    "    print(\"Train\")\n",
    "    print (\"Ambigious\", df[df.Ambigious == \"1\"].shape[0])\n",
    "    print (\"Non-ambigious\", df[df.Ambigious == \"0\"].shape[0])\n",
    "    \n",
    "    print(df.dtypes)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_dataset_first(path):\n",
    "    # Load the dataset into a pandas dataframe.\n",
    "    df = pd.read_csv(path, delimiter='\\t', header = 0, names=['qID', 'ideology', 'stance', 'docCont', 'topic', 'Q', 'title'])\n",
    "\n",
    "    df_q = pd.read_csv(path.replace('final.tsv', 'final_onlyqID.tsv'), delimiter='\\t', header=0, names=['qID', 'orientation'])\n",
    "    df_q[\"orientation\"] = df_q[\"orientation\"].replace({-1: \"Lib\", 1: \"Con\"})\n",
    "           \n",
    "    df = df.drop('qID', axis=1)\n",
    "    df.insert(0, \"qID\", df_q['qID'], True)\n",
    "    df.insert(1, \"q_ideology\", df_q['orientation'], True)\n",
    "  \n",
    "    df[\"stance\"] = df[\"stance\"].replace({\"-1\": 0, \"1\": 1, \"Pro\": 1, \"Agst\": 0})\n",
    "    \n",
    "    return df\n",
    "\n",
    "def sample_dataset_stance(df, seedVal):\n",
    "    #create_determinism(seedVal)\n",
    "    \n",
    "    df_A = df[df['Ambigious'] == \"1\"]\n",
    "    df_N = df[df['Ambigious'] == \"0\"]\n",
    "    \n",
    "    \n",
    "    df_new = df_A.append(df_N, ignore_index = True)\n",
    "\n",
    "    y_copy = df_new['Ambigious'].copy(deep=True)\n",
    "    X_copy = df_new.drop('Ambigious', axis=1).copy(deep=True)\n",
    "    \n",
    "    X = pd.DataFrame (columns=['qID', 'docID', 'stance', 'ideology', 'docCont', 'Q', 'title'])\n",
    "    y = pd.DataFrame (columns=['Ambigious'])\n",
    "    \n",
    "    X = X_copy\n",
    "    y = y_copy\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "    \n",
    "    print(len(X_train))\n",
    "    print(len(y_train))\n",
    "    print(len(X_test))\n",
    "    print(len(y_test))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, shuffle=True)\n",
    "    \n",
    "    X_train.insert(2, \"Ambigious\", y_train.values) \n",
    "    X_val.insert(2, \"Ambigious\", y_val.values) \n",
    "    X_test.insert(2, \"Ambigious\", y_test.values)\n",
    "    \n",
    "    \n",
    "    df_A = X_train[X_train['Ambigious'] == \"1\"]\n",
    "    df_N = X_train[X_train['Ambigious'] == \"0\"]\n",
    "    \n",
    "    \n",
    "    print(\"****Train****\")\n",
    "    print(\"Ambigious\", df_A.shape[0])\n",
    "    print(\"Not Ambigious\", df_N.shape[0])\n",
    "    \n",
    "    \n",
    "    df_A = X_test[X_test['Ambigious'] == \"1\"]\n",
    "    df_N = X_test[X_test['Ambigious'] == \"0\"]\n",
    "    \n",
    "    print(\"****Test****\")\n",
    "    print(\"Ambigious\", df_A.shape[0])\n",
    "    print(\"Not Ambigious\", df_N.shape[0])\n",
    "    \n",
    "    X_train.to_csv('./dataset/batches_cleaned/stance/train_serp.tsv', sep='\\t', index=False)\n",
    "    X_val.to_csv('./dataset/batches_cleaned/stance/val_serp.tsv', sep='\\t', index=False)\n",
    "    X_test.to_csv('./dataset/batches_cleaned/stance/test_serp.tsv', sep='\\t', index=False)\n",
    "\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "def merge_datasets(df, dfVal, dfTest):\n",
    "    from numpy import nan\n",
    "    df = df.append(dfVal, ignore_index = True)\n",
    "    df = df.append(dfTest, ignore_index = True)\n",
    "    \n",
    "    df.replace(\"\", nan, inplace=True)\n",
    "    df.replace(\" \", nan, inplace=True)\n",
    "    df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n",
    "    \n",
    "    dfLabel = df['ideology'].copy(deep=True)\n",
    "    df = df.drop('ideology', axis=1).copy(deep=True)\n",
    "    \n",
    "    return df, dfLabel\n",
    "\n",
    "def preprocess_dataset_new_ideology_latest(df_new, testPer, seedVal):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from pandas import DataFrame\n",
    "    \n",
    "    create_determinism(seedVal)\n",
    "    \n",
    "    #print(\"New dataset\")\n",
    "    #print(df_new['stance'].value_counts())\n",
    "\n",
    "    y = df_new['ideology'].copy(deep=True)\n",
    "    X = df_new.drop('ideology', axis=1).copy(deep=True)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testPer, shuffle=True, stratify=y)\n",
    "            \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def preprocess_dataset_new(df, dfLabels, testPer, seedVal):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from pandas import DataFrame\n",
    "    \n",
    "    create_determinism(seedVal)\n",
    "\n",
    "    df.insert(2, \"ideology\", dfLabels.values) \n",
    "    df = df.sort_values(by='Q')\n",
    "     \n",
    "    one_q_instances = []\n",
    "    all_q_instances = {}\n",
    "    \n",
    "    curr_q = df.Q.values[0]\n",
    "    for index, inst in df.iterrows():\n",
    "        if curr_q == inst['Q']:\n",
    "            one_q_instances.append(inst.values)\n",
    "        else:\n",
    "            all_q_instances[curr_q] = one_q_instances\n",
    "            one_q_instances = []\n",
    "            curr_q = inst['Q']\n",
    "            one_q_instances.append(inst.values)\n",
    "            \n",
    "    \n",
    "    X_train_allqueries = {}\n",
    "    X_test_allqueries = {}\n",
    "    y_train_allqueries = {}\n",
    "    y_test_allqueries = {}\n",
    "            \n",
    "    for query in all_q_instances:\n",
    "        this_query_instances = all_q_instances[query]\n",
    "        \n",
    "        df = DataFrame (this_query_instances, columns=['qID', 'q_ideology', 'stance', 'ideology', 'docCont', 'topic', 'Q', 'title'])\n",
    "        \n",
    "        y = df['ideology'].copy(deep=True)\n",
    "        X = df.drop('ideology', axis=1).copy(deep=True)\n",
    "    \n",
    "        X_train_allqueries[query] = []\n",
    "        y_train_allqueries[query] = []\n",
    "        X_test_allqueries[query] = []\n",
    "        y_test_allqueries[query] = []\n",
    "        \n",
    "        if len(X.index) > 1:\n",
    "        \n",
    "            con_count = len(df[df['ideology'] == 0])\n",
    "            lib_count = len(df[df['ideology'] == 1])\n",
    "\n",
    "            if con_count < 2 or lib_count < 2:\n",
    "                X_train_allqueries[query], X_test_allqueries[query], y_train_allqueries[query], y_test_allqueries[query] = train_test_split(X, y, test_size=testPer, shuffle=True)\n",
    "            else:\n",
    "                if((con_count + lib_count)*testPer > 1):\n",
    "                    X_train_allqueries[query], X_test_allqueries[query], y_train_allqueries[query], y_test_allqueries[query] = train_test_split(X, y, test_size=testPer, shuffle=True, stratify=y)\n",
    "                else:\n",
    "                    X_train_allqueries[query], X_test_allqueries[query], y_train_allqueries[query], y_test_allqueries[query] = train_test_split(X, y, test_size=testPer, shuffle=True)\n",
    "        else:\n",
    "            X_train_allqueries[query] = X\n",
    "            y_train_allqueries[query] = y\n",
    "    \n",
    "    X_train = pd.DataFrame(columns=['qID', 'q_ideology', 'stance', 'docCont', 'topic', 'Q', 'title'])\n",
    "    y_train = pd.DataFrame(columns=['ideology'])\n",
    "    \n",
    "    X_test = pd.DataFrame(columns=['qID', 'q_ideology', 'stance', 'docCont', 'topic', 'Q', 'title'])\n",
    "    y_test = pd.DataFrame(columns=['ideology'])\n",
    "    \n",
    "    for query in X_train_allqueries:\n",
    "        X_train = X_train.append(pd.DataFrame(X_train_allqueries[query]), ignore_index = True)\n",
    "        y_train = y_train.append(pd.DataFrame(y_train_allqueries[query]), ignore_index = True)\n",
    "    \n",
    "    for query in X_test_allqueries:\n",
    "        X_test = X_test.append(pd.DataFrame(X_test_allqueries[query]), ignore_index = True)\n",
    "        y_test = y_test.append(pd.DataFrame(y_test_allqueries[query]), ignore_index = True)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def create_train_val_test_split(trainpath, valpath, testpath, testPer, valPer, seedVal):\n",
    "    \n",
    "    df = load_dataset(\"./dataset/ideology/train_samples.tsv\")\n",
    "    dfVal = load_dataset(\"./dataset/ideology/val_samples.tsv\")\n",
    "    dfTest = load_dataset(\"./dataset/ideology/test_samples.tsv\")\n",
    "\n",
    "    dfComp, dfCompLabel = merge_datasets(df, dfVal, dfTest)\n",
    "    dfComp.insert(2, \"ideology\", dfCompLabel.values)\n",
    "    \n",
    "    df, dfLabel, dfTest, dfTestLabel = preprocess_dataset_new_ideology_latest(dfComp, 0.2, seedVal)\n",
    "    df.insert(2, \"ideology\", dfLabel.values)\n",
    "    \n",
    "    df, dfLabel, dfVal, dfValLabel = preprocess_dataset_new_ideology_latest(df, 0.2, seedVal)\n",
    "    \n",
    "    df.insert(2, \"ideology\", dfLabel.values)\n",
    "    dfVal.insert(2, \"ideology\", dfValLabel.values) \n",
    "    dfTest.insert(2, \"ideology\", dfTestLabel.values)\n",
    "    \n",
    "    df.to_csv('train_new.tsv', sep='\\t', index=False)\n",
    "    dfVal.to_csv('val_new.tsv', sep='\\t', index=False)\n",
    "    dfTest.to_csv('test_new.tsv', sep='\\t', index=False)\n",
    "\n",
    "def preprocess_ideologyOld(stance_labels, ideology_labels):\n",
    "    t_stance = []\n",
    "    t_ideology = []\n",
    "    \n",
    "    t_mmd_symbol = []\n",
    "    t_mmd_symbol_ = []\n",
    "\n",
    "    for idx, s_label in enumerate(stance_labels):\n",
    "        i_label = ideology_labels[idx]\n",
    "        if s_label == 1 and i_label == 0: #pro-con\n",
    "            t_stance.append([1,0])\n",
    "            t_ideology.append([0,1])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "        elif s_label == 1 and i_label == 1: #pro-lib\n",
    "            t_stance.append([1,0]) \n",
    "            t_ideology.append([1,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(1)\n",
    "        elif s_label == 0 and i_label == 0: #agst-con\n",
    "            t_stance.append([0,1])\n",
    "            t_ideology.append([0,1])\n",
    "            t_mmd_symbol.append(0)\n",
    "            t_mmd_symbol_.append(0)\n",
    "        else: #agst-lib\n",
    "            t_stance.append([0,1])\n",
    "            t_ideology.append([1,0])\n",
    "            t_mmd_symbol.append(0)\n",
    "            t_mmd_symbol_.append(1)\n",
    "            \n",
    "    \n",
    "    t_stance = torch.as_tensor(t_stance, dtype=torch.int32)\n",
    "    t_ideology = torch.as_tensor(t_ideology, dtype=torch.int32)\n",
    "    \n",
    "    t_mmd_symbol  = torch.as_tensor(t_mmd_symbol, dtype=torch.float32)\n",
    "    t_mmd_symbol_ = torch.as_tensor(t_mmd_symbol_, dtype=torch.float32)\n",
    "    \n",
    "    return t_stance, t_ideology, t_mmd_symbol, t_mmd_symbol_\n",
    "\n",
    "def preprocess_ideology_ambigious(ambigious_labels):\n",
    "    t_ideology = []\n",
    "\n",
    "    for idx, a_label in enumerate(ambigious_labels):\n",
    "        a_label = ambigious_labels[idx]\n",
    "        if a_label == \"0\": #con\n",
    "            t_ideology.append([0])\n",
    "        else:#lib\n",
    "            t_ideology.append([1])\n",
    "            \n",
    "    t_ideology = torch.as_tensor(t_ideology, dtype=torch.int32)\n",
    "    \n",
    "    return t_ideology\n",
    "\n",
    "def preprocess_ideology_new(stance_labels, ideology_labels):\n",
    "    t_ideology = []\n",
    "\n",
    "    for idx, s_label in enumerate(stance_labels):\n",
    "        i_label = ideology_labels[idx]\n",
    "        if i_label == 0: #con\n",
    "            t_ideology.append([0])\n",
    "        else:#lib\n",
    "            t_ideology.append([1])\n",
    "            \n",
    "    t_ideology = torch.as_tensor(t_ideology, dtype=torch.int32)\n",
    "    \n",
    "    return t_ideology\n",
    "\n",
    "def preprocess_ideology(stance_labels, ideology_labels):\n",
    "    t_ideology = []\n",
    "\n",
    "    for idx, s_label in enumerate(stance_labels):\n",
    "        i_label = ideology_labels[idx]\n",
    "        if s_label == 1 and i_label == 0: #pro-con\n",
    "            t_ideology.append([1,0])\n",
    "        elif s_label == 1 and i_label == 1: #pro-lib\n",
    "            t_ideology.append([1,1])\n",
    "        elif s_label == 0 and i_label == 0: #agst-con\n",
    "            t_ideology.append([0,0])\n",
    "        else: #agst-lib\n",
    "            t_ideology.append([0,1])\n",
    "            \n",
    "    t_ideology = torch.as_tensor(t_ideology, dtype=torch.int32)\n",
    "    \n",
    "    return t_ideology\n",
    "\n",
    "def concanListStringsLonger(list1, list2):\n",
    "    list3 = []\n",
    "    myLen1 = len(list1)\n",
    "    if myLen1 != len(list2):\n",
    "        print(\"Length - error\")\n",
    "    for idx in range(0, myLen1):\n",
    "        list3.append(list1[idx] + \" GIZEM \" + list2[idx])\n",
    "    return list3\n",
    "\n",
    "def concanListStrings(list1, list2):\n",
    "    list3 = []\n",
    "    new_labels = []\n",
    "    myLen1 = len(list1)\n",
    "    if myLen1 != len(list2):\n",
    "        print(\"Length - error\")\n",
    "    for idx in range(0, myLen1):\n",
    "        list3.append(list1[idx] + \" \" + list2[idx])\n",
    "        #list3.append(list1[idx] + \" \" + list2[idx][-512:])\n",
    "        #new_labels.append(labels[idx])\n",
    "        #new_labels.append(labels[idx])\n",
    "        \n",
    "    return list3\n",
    "\n",
    "def concanListStrings_sep(list1, list2):\n",
    "    list3 = []\n",
    "    myLen1 = len(list1)\n",
    "    if myLen1 != len(list2):\n",
    "        print(\"Length - error\")\n",
    "    for idx in range(0, myLen1):\n",
    "        list3.append(list1[idx] + \" [SEP] \" + str(list2[idx]))\n",
    "\n",
    "    return list3\n",
    "\n",
    "### Generate the datasets with the different fields.\n",
    "def generate_datasets_ambigious(df, tokenizer):\n",
    "\n",
    "    sentencesQuery= df.Q.values\n",
    "    sentencesTitle = df.title.values\n",
    "    sentencesCont = df.docCont.values\n",
    "\n",
    "    labels = df.Ambigious.values\n",
    "    \n",
    "    #print(stances[0:10])\n",
    "\n",
    "    sentencesQueryTitle = concanListStrings(sentencesQuery, sentencesTitle)\n",
    "    sentencesQueryTitleCont = concanListStringsLonger(sentencesQueryTitle, sentencesCont)\n",
    "\n",
    "    return sentencesQueryTitle, sentencesQueryTitleCont, labels\n",
    "\n",
    "\n",
    "### Generate the datasets with the different fields.\n",
    "def generate_datasets_ideology(df, tokenizer):\n",
    "\n",
    "    sentencesQuery= df.Q.values\n",
    "    sentencesQIdeology = df.q_ideology.values\n",
    "    sentencesTitle = df.title.values\n",
    "    sentencesCont = df.docCont.values\n",
    "\n",
    "    stances = df.stance.values\n",
    "    labels = df.ideology.values\n",
    "    \n",
    "    #print(stances[0:10])\n",
    "\n",
    "    sentencesQueryTitle = concanListStrings(sentencesQuery, sentencesTitle)\n",
    "    sentencesQueryTitleStance = concanListStrings(sentencesQueryTitle, stances)\n",
    "    sentencesQueryTitleCont = concanListStringsLonger(sentencesQueryTitle, sentencesCont)\n",
    "    sentencesQueryTitleStanceCont = concanListStringsLonger(sentencesQueryTitleStance, sentencesCont)\n",
    "\n",
    "    return sentencesQueryTitle, sentencesQueryTitleCont, sentencesQueryTitleStance, sentencesQueryTitleStanceCont, stances, labels\n",
    "\n",
    "def preprocessing_for_bert(tokenizer, docs, max_len, doc_stride):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    input_ids_last = []\n",
    "    attention_masks_last = []\n",
    "    \n",
    "    content_input_ids = {}\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in docs:\n",
    "        #print(sent)\n",
    "        #print(sentences[0])\n",
    "        #print(sentences[1])\n",
    "        \n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus (\n",
    "            sent,  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=max_len,                  # Max length to truncate/pad\n",
    "            #padding='longest',         # Pad sentence to max length\n",
    "            pad_to_max_length = True,\n",
    "            return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent['input_ids'])\n",
    "        attention_masks.append(encoded_sent['attention_mask'])\n",
    "        \n",
    "        # Print the original sentence.\n",
    "        #print(' Original: ', sent)\n",
    "\n",
    "        # Print the sentence split into tokens.\n",
    "        #print('Tokenized: ', input_ids)\n",
    "        \n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    # Print sentence 0, now as a list of IDs.\n",
    "    #print('Original: ', docs[0])\n",
    "    #print('Token IDs:', input_ids[0])\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "def transform_sequences_longer_ideology(tokenizer, docs, stanceLabels, ideologylabels, max_len, doc_stride):\n",
    "\n",
    "    special_tokens_count = 2 #[CLS] and [SEP]\n",
    "    # For every sentence...\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    stance_labels_Transformed = []\n",
    "    ideology_labels_Transformed = []\n",
    "\n",
    "    only_get_partial_text = False\n",
    "    if doc_stride == 0:\n",
    "        only_get_partial_text = True\n",
    "        \n",
    "    checked_doc_stride_thresh = doc_stride - special_tokens_count - 1\n",
    "        \n",
    "    allDocs_len = len(docs)\n",
    "    for doc_id in range(0, allDocs_len):\n",
    "        currDoc = docs[doc_id]\n",
    "        currStanceLabel = stanceLabels[doc_id]\n",
    "        currIdeologyLabel = ideologylabels[doc_id]\n",
    "        \n",
    "        my_idx = 0\n",
    "        if \"GIZEM\" in currDoc:\n",
    "            doc_splitted_tokens = currDoc.split(\" \")\n",
    "            my_idx = doc_splitted_tokens.index('GIZEM')\n",
    "        else:\n",
    "            doc_splitted_tokens = currDoc.split(\" \")\n",
    "        \n",
    "        #query\n",
    "        first_part_tokens = tokenizer.tokenize(' '.join(doc_splitted_tokens[0:my_idx]))\n",
    "        myTokens = tokenizer.tokenize(' '.join(doc_splitted_tokens[my_idx+1:]))\n",
    "        mytokens_maxlen = []\n",
    "\n",
    "        first_part_len = len(first_part_tokens)\n",
    "        cur_len = len(myTokens)\n",
    "        #longer than the max-len, use doc-stride\n",
    "        taken_len = max_len - first_part_len - special_tokens_count - 1\n",
    "        \n",
    "        if only_get_partial_text:\n",
    "            mytokens_maxlen.append(first_part_tokens + myTokens[0:taken_len])\n",
    "        else:\n",
    "            checked_thresh = max_len - first_part_len - special_tokens_count\n",
    "            if cur_len > checked_thresh:\n",
    "                #get first part len\n",
    "                while cur_len > checked_thresh:\n",
    "                    partialTokens = first_part_tokens + myTokens[0:taken_len]\n",
    "                    mytokens_maxlen.append(partialTokens)\n",
    "                    del myTokens[0:checked_doc_stride_thresh]\n",
    "                    cur_len = len(myTokens)\n",
    "                if cur_len > 0:\n",
    "                    mytokens_maxlen.append(first_part_tokens + myTokens)\n",
    "            else:\n",
    "                mytokens_maxlen.append(first_part_tokens + myTokens)\n",
    "\n",
    "        if len(mytokens_maxlen) == 1:\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                        currDoc,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "          # Add the encoded sentence to the list.    \n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "          # And its attention mask (simply differentiates padding from non-padding).\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "            stance_labels_Transformed.append(currStanceLabel)\n",
    "            ideology_labels_Transformed.append(currIdeologyLabel)\n",
    "        else:\n",
    "            for maxTokenList in mytokens_maxlen:\n",
    "                if len(maxTokenList) > 510:\n",
    "                    print(len(maxTokenList))\n",
    "          #   (4) Map tokens to their IDs.\n",
    "          #   (5) Pad or truncate the sentence to `max_length`\n",
    "          #   (6) Create attention masks for [PAD] tokens.\n",
    "                encoded_dict = tokenizer.encode_plus(\n",
    "                    maxTokenList,                      # Sentence to encode.\n",
    "                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                    max_length = max_len,           # Pad & truncate all sentences.\n",
    "                    pad_to_max_length = True,\n",
    "                    return_attention_mask = True,   # Construct attn. masks.\n",
    "                    return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    " \n",
    "                input_ids.append(encoded_dict['input_ids'])\n",
    "                attention_masks.append(encoded_dict['attention_mask'])\n",
    "                stance_labels_Transformed.append(currStanceLabel)\n",
    "                ideology_labels_Transformed.append(currIdeologyLabel)\n",
    "\n",
    "\n",
    "    all_input_ids = torch.cat(input_ids, dim=0)\n",
    "    all_input_mask = torch.cat(attention_masks, dim=0)\n",
    "    stance_labels = torch.tensor(stance_labels_Transformed)\n",
    "    ideology_labels = torch.tensor(ideology_labels_Transformed)\n",
    "    \n",
    "    print(all_input_ids.shape)\n",
    "\n",
    "    return all_input_ids, all_input_mask, stance_labels, ideology_labels\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, RobertaModel\n",
    "class IdeologyDetectionClass(torch.nn.Module):\n",
    "    def __init__(self, modelUsed):\n",
    "        super(IdeologyDetectionClass, self).__init__()\n",
    "        input_size = 768\n",
    "        hidden_size = 768\n",
    "        mmd_size = 10\n",
    "        dropout_prob = 0.5\n",
    "        relatedness_size = 2\n",
    "        classes_size = 2\n",
    "        #agreement_size = 3\n",
    "        \n",
    "        self.input_pl = BertModel.from_pretrained(modelUsed) #input\n",
    "        self.l1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.bn1_hidden = torch.nn.BatchNorm1d(hidden_size, momentum=0.05)\n",
    "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.stance = torch.nn.Linear(hidden_size, classes_size)\n",
    "        self.output_prob = torch.nn.Softmax(dim = 1)\n",
    "\n",
    "        #self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        relatedness_size = 2\n",
    "        classes_size = 1\n",
    "        \n",
    "        input_1 = self.input_pl(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        last_hidden_state_cls = input_1[0][:, 0, :]\n",
    "        \n",
    "        #hidden layer\n",
    "        hidden_state = self.l1(last_hidden_state_cls)\n",
    "        hidden_state_normalized = self.bn1_hidden(hidden_state)\n",
    "        hidden_state_normalized = self.relu(hidden_state_normalized)\n",
    "        hidden_layer= self.dropout(hidden_state_normalized)\n",
    "        \n",
    "        #mmd layer        \n",
    "        #theta_d = self.theta_d(hidden_layer)\n",
    "        ##theta_d_normalized = self.bn1_theta(theta_d)\n",
    "        #theta_d_normalized = torch.nn.ReLU()(theta_d_normalized)\n",
    "        #theta_d_layer= self.dropout(theta_d_normalized)\n",
    "\n",
    "        #probability layer\n",
    "        #relatedness_state = self.probability(hidden_layer)\n",
    "        #relatedness_flat = self.dropout(relatedness_state)\n",
    "        \n",
    "        #relatedness_flat_reshaped = torch.reshape(relatedness_flat, (-1, relatedness_size))\n",
    "        #P_relatedness = self.output_prob(relatedness_flat_reshaped)    \n",
    "        \n",
    "        #P_related = torch.reshape(P_relatedness[:, 0], (-1, 1))\n",
    "        #P_unrelated = torch.reshape(P_relatedness[:, 1], (-1, 1))\n",
    "        \n",
    "        stance_state = self.stance(hidden_layer) #batch size x classes_size\n",
    "        P_stance = self.output_prob(stance_state) \n",
    "\n",
    "        return P_stance\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.val_acc_max = -1\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, val_acc, model_save_state, model_save_path):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, val_acc, model_save_state, model_save_path)\n",
    "            self.val_acc_max = val_acc\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, val_acc, model_save_state, model_save_path)\n",
    "            self.val_acc_max = val_acc\n",
    "            self.counter = 0\n",
    "\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, val_loss, val_acc, model_save_state, model_save_path):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            print(f'Validation acc: ({self.val_acc_max:.6f} --> {val_acc:.6f}).  Saving model ...')\n",
    "        #torch.save(model.module.state_dict(), 'checkpoint.pt')\n",
    "        \n",
    "        torch.save(model_save_state, model_save_path)\n",
    "        \n",
    "        \n",
    "        #model.save_pretrained('model_save/')\n",
    "        #tokenizer.save_pretrained('model_save/')\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        #torch.save(model, './model_save/entire_model.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "#from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "def prepare_for_training_ambigious(input_idsTrain, attention_masksTrain, ideology_labels_Train, input_idsVal, attention_masksVal, ideology_labels_Val, modelUsed, batch_size=16, epochs = 50, num_warmup_steps=0, learning_rate=5e-5):\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "\n",
    "    from transformers import BertForSequenceClassification, AdamW, BertConfig, RobertaConfig, AutoModelWithLMHead\n",
    "    from transformers import DistilBertForSequenceClassification, RobertaForSequenceClassification\n",
    "    \n",
    "    from torch.utils.data import DataLoader, RandomSampler\n",
    "    \n",
    "    t_train_stance = preprocess_ideology_ambigious(ideology_labels_Train)\n",
    "    \n",
    "    datasetTrain = TensorDataset(input_idsTrain, attention_masksTrain, t_train_stance)\n",
    "\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "    t_val_stance  = preprocess_ideology_ambigious(ideology_labels_Val)\n",
    "    \n",
    "    \n",
    "    datasetVal = TensorDataset(input_idsVal, attention_masksVal, t_val_stance)\n",
    "    \n",
    "    model = IdeologyDetectionClass(modelUsed)\n",
    "\n",
    "    # Tell pytorch to run this model on the GPU.\n",
    "    model.cuda()\n",
    "\n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "    # I believe the 'W' stands for 'Weight Decay fix\"\n",
    "    \n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  betas=(0.9, 0.999), \n",
    "                  eps=1e-08, \n",
    "                  weight_decay=1e-3,\n",
    "                  correct_bias=True\n",
    "               )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "            datasetTrain,  # The training samples.\n",
    "            sampler =  RandomSampler(datasetTrain), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size., \n",
    "            num_workers=8\n",
    "        )\n",
    "    batch_size = batch_size\n",
    "\n",
    "\n",
    "    from transformers import get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "    # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "    # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "    # training data.\n",
    "    epochs = epochs\n",
    "\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    schedulerOld = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "    \n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps, num_cycles = 5)\n",
    "    \n",
    "    loss_fct = torch.nn.BCELoss()\n",
    "    return model, datasetTrain, datasetVal, optimizer, schedulerOld\n",
    "\n",
    "#from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "def prepare_for_trainingOld(input_idsTrain, attention_masksTrain, stance_labels_Train, ideology_labels_Train, input_idsVal, attention_masksVal, stance_labels_Val, ideology_labels_Val, modelUsed, batch_size=16, epochs = 50, num_warmup_steps=0, learning_rate=5e-5):\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "\n",
    "    from transformers import BertForSequenceClassification, AdamW, BertConfig, RobertaConfig, AutoModelWithLMHead\n",
    "    from transformers import DistilBertForSequenceClassification, RobertaForSequenceClassification\n",
    "    \n",
    "    from torch.utils.data import DataLoader, RandomSampler\n",
    "    \n",
    "    t_train_stance, t_train_ideology, t_train_mmd_symbol, t_train_mmd_symbol_ = preprocess_ideology(stance_labels_Train, ideology_labels_Train)\n",
    "    \n",
    "    datasetTrain = TensorDataset(input_idsTrain, attention_masksTrain, t_train_stance, t_train_ideology, t_train_mmd_symbol, t_train_mmd_symbol_)\n",
    "\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "    t_val_stance, t_val_ideology, t_val_mmd_symbol, t_val_mmd_symbol_  = preprocess_ideology(stance_labels_Val, ideology_labels_Val)\n",
    "    \n",
    "    \n",
    "    datasetVal = TensorDataset(input_idsVal, attention_masksVal, t_val_stance, t_val_ideology, t_val_mmd_symbol, t_val_mmd_symbol_)\n",
    "    \n",
    "    model = IdeologyDetectionClass(modelUsed)\n",
    "\n",
    "    # Tell pytorch to run this model on the GPU.\n",
    "    model.cuda()\n",
    "\n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "    # I believe the 'W' stands for 'Weight Decay fix\"\n",
    "    \n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  betas=(0.9, 0.999), \n",
    "                  eps=1e-08, \n",
    "                  weight_decay=1e-3,\n",
    "                  correct_bias=True\n",
    "               )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "            datasetTrain,  # The training samples.\n",
    "            sampler =  RandomSampler(datasetTrain), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size., \n",
    "            num_workers=8\n",
    "        )\n",
    "    batch_size = batch_size\n",
    "\n",
    "\n",
    "    from transformers import get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "    # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "    # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "    # training data.\n",
    "    epochs = epochs\n",
    "\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    schedulerOld = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "    \n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps, num_cycles = 5)\n",
    "    \n",
    "    loss_fct = torch.nn.BCELoss()\n",
    "    return model, datasetTrain, datasetVal, optimizer, schedulerOld, loss_fct\n",
    "\n",
    "    return model, datasetTrain, datasetVal, optimizer, schedulerOld\n",
    "\n",
    "def return_batches_datasets(datasetTrain, datasetVal, batch_size = 16):\n",
    "    from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "        \n",
    "    # Create the DataLoaders for our training and validation sets.\n",
    "    # We'll take training samples in random order. \n",
    "    train_dataloader = DataLoader(\n",
    "            datasetTrain,  # The training samples.\n",
    "            sampler =  RandomSampler(datasetTrain), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size., \n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "    # For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "    validation_dataloader = DataLoader(\n",
    "            datasetVal, # The validation samples.\n",
    "            sampler = SequentialSampler(datasetVal), # Pull out batches sequentially.\n",
    "            batch_size = batch_size, # Evaluate with this batch size.\n",
    "            num_workers=0\n",
    "        )\n",
    "    \n",
    "    \n",
    "    #validation_dataloader = DataLoader(\n",
    "    #        datasetVal, # The validation samples.\n",
    "    #        sampler = SequentialSampler(datasetVal), # Pull out batches sequentially.\n",
    "    #        batch_size = batch_size, # Evaluate with this batch size.\n",
    "    #        num_workers=0, drop_last=True\n",
    "    #)\n",
    "    \n",
    "    return train_dataloader, validation_dataloader\n",
    "\n",
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "#from tensorboardX import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#import EarlyStopping\n",
    "def train_stance_ideology_ambigious(train_nums, val_nums, train_nums_ideology, val_nums_ideology, model_save_path, \n",
    "                                    model, datasetTrain, datasetVal, epochs, batch_size, optimizer, scheduler, patience, verbose, delta, seedVal, continue_train = False):\n",
    "    \n",
    "    pro_val_num = val_nums[0]\n",
    "    agst_val_num = val_nums[1]\n",
    "    neut_val_num = val_nums[2] + 0.01\n",
    "    notrel_val_num = val_nums[3]\n",
    "    \n",
    "    stance_all_num = pro_val_num + agst_val_num + neut_val_num + notrel_val_num\n",
    "    \n",
    "    con_val_num = 0.1\n",
    "    lib_val_num = 0.1\n",
    "    na_val_num = 0.1\n",
    "    \n",
    "    con_train_num = 0.1\n",
    "    lib_train_num = 0.1\n",
    "    na_train_num = 0.1\n",
    "    \n",
    "    #con_train_num = train_nums_ideology[0]\n",
    "    #lib_train_num = train_nums_ideology[1]\n",
    "    #na_train_num = train_nums_ideology[2]\n",
    "    \n",
    "    my_max_train_stance = max(pro_val_num, agst_val_num, neut_val_num, notrel_val_num)\n",
    "    my_max_train = max(con_train_num, lib_train_num, na_train_num)\n",
    "    \n",
    "    #con_val_num = val_nums_ideology[0]\n",
    "    #lib_val_num = val_nums_ideology[1]\n",
    "    #na_val_num = val_nums_ideology[2]\n",
    "    \n",
    "    my_max = max(con_val_num, lib_val_num, na_val_num)\n",
    "    \n",
    "    ideology_all_num = con_val_num + lib_val_num + na_val_num\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    min_val_loss = 100\n",
    "    \n",
    "    relatedness_size = 2\n",
    "    classes_size = 4\n",
    "    loss_fct_relatedness = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    loss_fct_stance = torch.nn.CrossEntropyLoss()\n",
    "    #loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    alpha = 1.5\n",
    "    beta = 1e-3\n",
    "    theta = 0\n",
    "    gamma = 0\n",
    "    \n",
    "    batch_size_max_once = 16\n",
    "\n",
    "    if batch_size < batch_size_max_once:\n",
    "        batch_size_max_once = batch_size\n",
    "        \n",
    "    accumulation_steps = batch_size/batch_size_max_once\n",
    "    \n",
    "    es = EarlyStopping(patience,verbose, delta)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    train_dataloader, validation_dataloader = return_batches_datasets(datasetTrain, datasetVal, batch_size_max_once)\n",
    "    \n",
    "    epoch_start = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        #multi-gpu\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            \n",
    "    print(device)\n",
    "    \n",
    "    \n",
    "            \n",
    "    if continue_train:    \n",
    "        #'./model_save/fnc/model_emergentbert_epoch90_withoutsep_serp.t7'\n",
    "        checkpoint = torch.load(model_save_path)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    \n",
    "     #pos_weight=torch.FloatTensor ([28.36 / 0.5090]\n",
    "    \n",
    "     #pos_weight = torch.tensor([1.0, 1.0, 1.0])\n",
    "     #pos_weight = pos_weight.to(device)\n",
    "     #criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    weights_ideology = torch.tensor([my_max_train/con_train_num, my_max_train/lib_train_num, my_max_train/na_train_num]).to(device)   \n",
    "    weights_stance = torch.tensor([my_max_train_stance/pro_val_num, my_max_train_stance/agst_val_num, my_max_train_stance/neut_val_num, my_max_train_stance/notrel_val_num]).to(device) \n",
    "    loss_fct_relatedness_weighted = torch.nn.BCEWithLogitsLoss(pos_weight = weights_stance)\n",
    "    loss_fct_ideology_weighted = torch.nn.BCEWithLogitsLoss(pos_weight = weights_ideology)\n",
    "    \n",
    "    # For each epoch...\n",
    "    batch_epoch_count = 1\n",
    "    for epoch_i in range(epoch_start, epoch_start + epochs):\n",
    "        \n",
    "        print(\"---------Epoch----------\" + str(epoch_i))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "    \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        #print(\"\")\n",
    "        #print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        #print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        # For each batch of training data...\n",
    "        mini_batch_avg_loss = 0\n",
    "        #train_size = len(train_dataloader)\n",
    "        \n",
    "        if batch_epoch_count % 500 == 0:\n",
    "            batch_size = batch_size*2\n",
    "            accumulation_steps = int(batch_size/batch_size_max_once)\n",
    "        batch_epoch_count = batch_epoch_count + 1\n",
    "\n",
    "        #train_size = len(train_dataloader) / float(accumulation_steps)\n",
    "        \n",
    "        print(\"Batch Size: \" + str(batch_size))\n",
    "        print(float(accumulation_steps))\n",
    "        \n",
    "        #print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_relatedness = batch[2].to(device)\n",
    "            b_labels = batch[3].to(device)\n",
    "            b_mmd_symbol = batch[4].to(device)\n",
    "            b_mmd_symbol_ = batch[5].to(device)\n",
    "            b_existedstances = batch[6].to(device)\n",
    "            b_ideologies = batch[7].to(device)\n",
    "        \n",
    "            \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "\n",
    "            #mmd_loss, P_relatedness, P_stance, P_existedstance = model(input_ids = b_input_ids, attention_mask = b_input_mask, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "            P_stance = model(input_ids = b_input_ids, attention_mask = b_input_mask, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "                \n",
    "                \n",
    "                \n",
    "            #relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "            stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "            #existedstance_loss = loss_fct_relatedness(P_existedstance, b_existedstances.float())\n",
    "\n",
    "            \n",
    "            #loss = alpha * stance_loss + theta * existedstance_loss + beta * mmd_loss + relatedness_loss\n",
    "            loss = stance_loss\n",
    "            loss = loss / accumulation_steps \n",
    "            total_train_loss += loss.item()\n",
    "                \n",
    "            loss.backward()\n",
    "            if (step+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "                    \n",
    "\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                # This is to help prevent the \"exploding gradients\" problem.\n",
    "                    #torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "                    \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                \n",
    "                # Update parameters and take a step using the computed gradient.\n",
    "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "                # modified based on their gradients, the learning rate, etc.\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the learning rate.\n",
    "                scheduler.step()\n",
    "                \n",
    "                #for param_group in optimizer.param_groups:\n",
    "                #print(\"Learning Rate: \", optimizer.param_groups[\"lr\"])\n",
    "                \n",
    "                                \n",
    "                # Always clear any previously calculated gradients before performing a\n",
    "                # backward pass. PyTorch doesn't do this automatically because \n",
    "                # accumulating the gradients is \"convenient while training RNNs\". \n",
    "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()       \n",
    "    \n",
    "        print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader) * accumulation_steps\n",
    "        #avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        #print(\"\")\n",
    "        print(\"  Average training loss: {0:.6f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        #print(\"\")\n",
    "        #print(\"Running Validation...\")\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_true_eval_stance = 0\n",
    "        total_true_eval_ideology = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        \n",
    "        agree_val_true = 0\n",
    "        disagree_val_true = 0 \n",
    "        discuss_val_true = 0 \n",
    "        unrelated_val_true = 0\n",
    "        \n",
    "        con_val_true = 0\n",
    "        lib_val_true = 0\n",
    "        na_val_true = 0\n",
    "        \n",
    "        total_true = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "        \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_relatedness = batch[2].to(device)\n",
    "            b_labels = batch[3].to(device)\n",
    "            b_mmd_symbol = batch[4].to(device)\n",
    "            b_mmd_symbol_ = batch[5].to(device)\n",
    "            b_existedstances = batch[6].to(device)\n",
    "            b_ideologies = batch[7].to(device)\n",
    "        \n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "\n",
    "                #mmd_loss, P_relatedness, P_stance, P_existedstance = model(input_ids = b_input_ids, attention_mask = b_input_mask, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "                P_stance = model(input_ids = b_input_ids, attention_mask = b_input_mask, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "\n",
    "                \n",
    "                #CrossEntropy Loss\n",
    "                #relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "                stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "                #existedstance_loss = loss_fct_relatedness(P_existedstance, b_existedstances.float())\n",
    "                \n",
    "                #loss_val = alpha * stance_loss + beta * mmd_loss + relatedness_loss\n",
    "                loss_val = stance_loss\n",
    "                total_eval_loss += loss_val.item()\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                #P_relatedness = P_relatedness.to('cpu')\n",
    "                #b_relatedness = b_relatedness.to('cpu')\n",
    "                P_stance = P_stance.to('cpu')\n",
    "                b_labels = b_labels.to('cpu')\n",
    "                #P_existedstance = P_existedstance.to('cpu')\n",
    "                #b_existedstances = b_existedstances.to('cpu')\n",
    "                \n",
    "                \n",
    "\n",
    "                # Calculate the accuracy for this batch of test sentences, and\n",
    "                # accumulate it over all batches.\n",
    "                #total_eval_accuracy += predict(P_relatedness, P_stance, b_labels)\n",
    "\n",
    "                #acc_list = predict_classwise_stance_ideology(P_relatedness, P_stance, P_existedstance, b_labels)\n",
    "                acc_list = predict_classwise_stance_ideology_bert(P_stance, b_labels)\n",
    "                total_true_eval_stance += acc_list[0]\n",
    "                ###\n",
    "                agree_val_true += acc_list[1]\n",
    "                disagree_val_true += acc_list[2]\n",
    "                discuss_val_true += acc_list[3]\n",
    "                unrelated_val_true += acc_list[4]\n",
    "                \n",
    "                total_true_eval_ideology += acc_list[5]\n",
    "                con_val_true += acc_list[6]\n",
    "                lib_val_true += acc_list[7]\n",
    "                na_val_true += acc_list[8]\n",
    "                \n",
    "                predict_labels = acc_list[9]\n",
    "                \n",
    "                                \n",
    "                ##print(\"Batch Next\")\n",
    "                #for idx in range(0, len(P_stance)):\n",
    "                    \n",
    "                    #print(P_stance[idx], b_labels[idx], acc_list[9][idx]) \n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy_stance = total_true_eval_stance / stance_all_num\n",
    "        avg_val_accuracy_ideology = total_true_eval_ideology / ideology_all_num\n",
    "        print(\"Avg Val Accuracy Stance: {0:.6f}\".format(avg_val_accuracy_stance))\n",
    "        print(\"Avg Val Accuracy Ideology: {0:.6f}\".format(avg_val_accuracy_ideology))\n",
    "        print(\"Total True\")\n",
    "        print(total_true)\n",
    "        print(\"*************\")\n",
    "        avg_val_agree_accuracy = agree_val_true / pro_val_num\n",
    "        print(\"Avg Val Agree Accuracy: {0:.6f}\".format(avg_val_agree_accuracy))\n",
    "        avg_val_disagree_accuracy = disagree_val_true / agst_val_num\n",
    "        print(\"Avg Val Disagree Accuracy: {0:.6f}\".format(avg_val_disagree_accuracy))\n",
    "        avg_val_discuss_accuracy = discuss_val_true / neut_val_num\n",
    "        print(\"Avg Val Discuss Accuracy: {0:.6f}\".format(avg_val_discuss_accuracy))\n",
    "        avg_val_unrelated_accuracy = unrelated_val_true / notrel_val_num\n",
    "        print(\"Avg Val Unrelated Accuracy: {0:.6f}\".format(avg_val_unrelated_accuracy))\n",
    "        \n",
    "        relative_score = 0.25*avg_val_unrelated_accuracy + 0.75*(avg_val_agree_accuracy + avg_val_disagree_accuracy + avg_val_discuss_accuracy)/3\n",
    "        \n",
    "        print(\"*****************\")\n",
    "        print(\"Relative score: {0:.6f}\".format(relative_score))\n",
    "        print(\"*****************\")\n",
    "        print(\"-------------\")\n",
    "        avg_val_con_accuracy = con_val_true / con_val_num\n",
    "        print(\"Avg Val Con Accuracy: {0:.6f}\".format(avg_val_con_accuracy))\n",
    "        avg_lib_accuracy = lib_val_true / lib_val_num\n",
    "        print(\"Avg Val Lib Accuracy: {0:.6f}\".format(avg_lib_accuracy))\n",
    "        avg_na_discuss_accuracy = na_val_true / na_val_num\n",
    "        print(\"Avg Val NA Accuracy: {0:.6f}\".format(avg_na_discuss_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        \n",
    "        print(\"Total Validation loss\", total_eval_loss)\n",
    "        print(\"Len-validation loader\", len(validation_dataloader))\n",
    "    \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t1)\n",
    "        \n",
    "        if avg_val_loss < min_val_loss:\n",
    "            min_val_loss = avg_val_loss\n",
    "    \n",
    "        print(\"Avg Validation Loss: {0:.6f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        #avg_val_accuracy_ideology = 0\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Stance Accur.': avg_val_accuracy_stance,\n",
    "            'Valid. Ideology Accur.': avg_val_accuracy_ideology,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        model_save_state = {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "    \n",
    "        es.__call__(avg_val_loss, avg_val_accuracy_stance, avg_val_accuracy_ideology, model_save_state, model_save_path, model)\n",
    "        last_epoch = epoch_i + 1\n",
    "        if es.early_stop == True:\n",
    "            break  # early stop criterion is met, we can stop now\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    \n",
    "    \n",
    "    min_val_loss = es.val_loss_min\n",
    "    max_val_acc = es.val_acc_max_stance\n",
    "\n",
    "    return training_stats, last_epoch, min_val_loss, max_val_acc\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#import EarlyStopping\n",
    "def train_stance(model_save_path, model, tokenizer, datasetTrain, datasetVal, epochs, batch_size, optimizer, scheduler, patience, verbose, delta, seedVal, continue_train = False):\n",
    "    \n",
    "    #loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "    loss_fct = torch.nn.BCELoss()\n",
    "    create_determinism(seedVal)\n",
    "    \n",
    "    min_val_loss = 100\n",
    "    \n",
    "    relatedness_size = 2\n",
    "    classes_size = 2\n",
    "    \n",
    "    alpha = 1.3\n",
    "    theta = 0.8\n",
    "    beta = 1e-2\n",
    "    \n",
    "    batch_size_max_once = 16    \n",
    "    \n",
    "\n",
    "    if batch_size < batch_size_max_once:\n",
    "        batch_size_max_once = batch_size\n",
    "        \n",
    "    accumulation_steps = batch_size/batch_size_max_once\n",
    "    \n",
    "    es = EarlyStopping(patience,verbose, delta)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    train_dataloader, validation_dataloader = return_batches_datasets(datasetTrain, datasetVal, batch_size_max_once)\n",
    "    \n",
    "    epoch_start = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        #multi-gpu\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            \n",
    "    print(device)\n",
    "          \n",
    "    continue_train = False\n",
    "    if continue_train:\n",
    "        checkpoint = torch.load('models/2_a/')\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    # For each epoch...\n",
    "    batch_epoch_count = 1\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(\"---------Epoch----------\" + str(epoch_i))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "    \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        # For each batch of training data...\n",
    "        mini_batch_avg_loss = 0\n",
    "        \n",
    "        \n",
    "        if batch_epoch_count % 200 == 0:\n",
    "            batch_size = batch_size*2\n",
    "            accumulation_steps = int(batch_size/batch_size_max_once)\n",
    "        batch_epoch_count = batch_epoch_count + 1\n",
    "        \n",
    "        train_size = len(train_dataloader) / accumulation_steps\n",
    "        \n",
    "        print(\"Batch Size: \" + str(batch_size))\n",
    "        print(accumulation_steps)\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            #b_stance = batch[2].to(device)\n",
    "            b_ideology = batch[2].to(device)\n",
    "            #b_mmd_symbol = batch[4].to(device)\n",
    "            #b_mmd_symbol_ = batch[5].to(device)\n",
    "            \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "\n",
    "            P_ideology = model(input_ids = b_input_ids, attention_mask = b_input_mask)      \n",
    "            ideology_loss = loss_fct(P_ideology, b_ideology.float())\n",
    "\n",
    "            loss = ideology_loss\n",
    "\n",
    "            #loss = torch.sum(loss, dim=0)\n",
    "\n",
    "\n",
    "            # Accumulate the training loss over all of t0e batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            #loss_train = loss\n",
    "            loss_train = loss / accumulation_steps\n",
    "            # Calculate the average loss over all of the batches.\n",
    "            \n",
    "            #loss_length = torch.numel(loss_train)\n",
    "            #fill_length = batch_size_max_once-loss_length\n",
    "            #cat_tensor = torch.zeros(fill_length, device=device)\n",
    "\n",
    "            #if loss_length < batch_size_max_once:\n",
    "                #loss_train = torch.cat([loss_train, cat_tensor], dim=0)\n",
    "                \n",
    "            mini_batch_avg_loss += loss_train.item()\n",
    "            \n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss_train.backward()\n",
    "            if (step+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                # This is to help prevent the \"exploding gradients\" problem.\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                \n",
    "                # Update parameters and take a step using the computed gradient.\n",
    "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "                # modified based on their gradients, the learning rate, etc.\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the learning rate.\n",
    "                scheduler.step()\n",
    "                \n",
    "                #for param_group in optimizer.param_groups:\n",
    "                \n",
    "                                \n",
    "                # Always clear any previously calculated gradients before performing a\n",
    "                # backward pass. PyTorch doesn't do this automatically because \n",
    "                # accumulating the gradients is \"convenient while training RNNs\". \n",
    "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                #total_train_loss = 0           \n",
    "                \n",
    "                total_train_loss += mini_batch_avg_loss\n",
    "                mini_batch_avg_loss = 0\n",
    "    \n",
    "        print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        \n",
    "        avg_train_loss = total_train_loss / train_size\n",
    "    \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Average training loss: {0:.6f}\".format(avg_train_loss))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        total_eval_stanceloss = 0\n",
    "        total_eval_ideologicalloss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "        \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            #b_stance = batch[2].to(device)\n",
    "            b_ideology = batch[2].to(device)\n",
    "            #b_mmd_symbol = batch[4].to(device)\n",
    "            #b_mmd_symbol_ = batch[5].to(device)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                \n",
    "                \n",
    "                P_ideology = model(input_ids = b_input_ids, attention_mask = b_input_mask)\n",
    "\n",
    "                ideology_loss = loss_fct(P_ideology, b_ideology.float())\n",
    "\n",
    "                loss_val = ideology_loss\n",
    "                #loss_val = torch.sum(loss, dim=0).item()\n",
    "                \n",
    "                #logits = model(input_ids = b_input_ids,attention_mask=b_input_mask)\n",
    "                \n",
    "                #loss = loss_function(logits, b_labels)\n",
    "            \n",
    "                # Accumulate the validation loss.\n",
    "                total_eval_loss += loss_val.item()\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                P_ideology = P_ideology.to('cpu')\n",
    "                b_ideology = b_ideology.to('cpu')\n",
    "\n",
    "                # Calculate the accuracy for this batch of test sentences, and\n",
    "                # accumulate it over all batches.\n",
    "                total_eval_accuracy += predict_binary(P_ideology, b_ideology)\n",
    "        \n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Accuracy: {0:.6f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        \n",
    "        if avg_val_loss < min_val_loss:\n",
    "            min_val_loss = avg_val_loss\n",
    "    \n",
    "        print(\"Avg Validation Loss: {0:.6f}\".format(avg_val_loss))\n",
    "        #print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "    \n",
    "        model_save_state = {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "    \n",
    "        es.__call__(avg_val_loss, avg_val_accuracy, model_save_state, model_save_path)\n",
    "        last_epoch = epoch_i + 1\n",
    "        if es.early_stop == True:\n",
    "            break  # early stop criterion is met, we can stop now\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    \n",
    "    \n",
    "    min_val_loss = es.val_loss_min\n",
    "    max_val_acc = es.val_acc_max\n",
    "\n",
    "    return training_stats, last_epoch, min_val_loss, max_val_acc\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#import EarlyStopping\n",
    "def train_stance(model_save_path, model, tokenizer, datasetTrain, datasetVal, epochs, batch_size, optimizer, scheduler, patience, verbose, delta, seedVal, continue_train = False):\n",
    "    \n",
    "    #loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "    loss_fct = torch.nn.BCELoss()\n",
    "    create_determinism(seedVal)\n",
    "    \n",
    "    min_val_loss = 100\n",
    "    \n",
    "    relatedness_size = 2\n",
    "    classes_size = 2\n",
    "    \n",
    "    alpha = 1.3\n",
    "    theta = 0.8\n",
    "    beta = 1e-2\n",
    "    \n",
    "    batch_size_max_once = 16    \n",
    "    \n",
    "\n",
    "    if batch_size < batch_size_max_once:\n",
    "        batch_size_max_once = batch_size\n",
    "        \n",
    "    accumulation_steps = batch_size/batch_size_max_once\n",
    "    \n",
    "    es = EarlyStopping(patience,verbose, delta)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    train_dataloader, validation_dataloader = return_batches_datasets(datasetTrain, datasetVal, batch_size_max_once)\n",
    "    \n",
    "    epoch_start = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        #multi-gpu\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            \n",
    "    print(device)\n",
    "            \n",
    "    if continue_train:\n",
    "        checkpoint = torch.load('models/2_a/')\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    # For each epoch...\n",
    "    batch_epoch_count = 1\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(\"---------Epoch----------\" + str(epoch_i))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "    \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        # For each batch of training data...\n",
    "        mini_batch_avg_loss = 0\n",
    "        \n",
    "        \n",
    "        if batch_epoch_count % 200 == 0:\n",
    "            batch_size = batch_size*2\n",
    "            accumulation_steps = int(batch_size/batch_size_max_once)\n",
    "        batch_epoch_count = batch_epoch_count + 1\n",
    "        \n",
    "        train_size = len(train_dataloader) / accumulation_steps\n",
    "        \n",
    "        print(\"Batch Size: \" + str(batch_size))\n",
    "        print(accumulation_steps)\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            #b_stance = batch[2].to(device)\n",
    "            b_ideology = batch[2].to(device)\n",
    "            #b_mmd_symbol = batch[4].to(device)\n",
    "            #b_mmd_symbol_ = batch[5].to(device)\n",
    "            \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "\n",
    "            P_ideology = model(input_ids = b_input_ids, attention_mask = b_input_mask)      \n",
    "            ideology_loss = loss_fct(P_ideology, b_ideology.float())\n",
    "\n",
    "            loss = ideology_loss\n",
    "\n",
    "            #loss = torch.sum(loss, dim=0)\n",
    "\n",
    "\n",
    "            # Accumulate the training loss over all of t0e batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            #loss_train = loss\n",
    "            loss_train = loss / accumulation_steps\n",
    "            # Calculate the average loss over all of the batches.\n",
    "            \n",
    "            #loss_length = torch.numel(loss_train)\n",
    "            #fill_length = batch_size_max_once-loss_length\n",
    "            #cat_tensor = torch.zeros(fill_length, device=device)\n",
    "\n",
    "            #if loss_length < batch_size_max_once:\n",
    "                #loss_train = torch.cat([loss_train, cat_tensor], dim=0)\n",
    "                \n",
    "            mini_batch_avg_loss += loss_train.item()\n",
    "            \n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss_train.backward()\n",
    "            if (step+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                # This is to help prevent the \"exploding gradients\" problem.\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                \n",
    "                # Update parameters and take a step using the computed gradient.\n",
    "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "                # modified based on their gradients, the learning rate, etc.\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the learning rate.\n",
    "                scheduler.step()\n",
    "                \n",
    "                #for param_group in optimizer.param_groups:\n",
    "                \n",
    "                                \n",
    "                # Always clear any previously calculated gradients before performing a\n",
    "                # backward pass. PyTorch doesn't do this automatically because \n",
    "                # accumulating the gradients is \"convenient while training RNNs\". \n",
    "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                #total_train_loss = 0           \n",
    "                \n",
    "                total_train_loss += mini_batch_avg_loss\n",
    "                mini_batch_avg_loss = 0\n",
    "    \n",
    "        print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        \n",
    "        avg_train_loss = total_train_loss / train_size\n",
    "    \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Average training loss: {0:.6f}\".format(avg_train_loss))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        total_eval_stanceloss = 0\n",
    "        total_eval_ideologicalloss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "        \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            #b_stance = batch[2].to(device)\n",
    "            b_ideology = batch[2].to(device)\n",
    "            #b_mmd_symbol = batch[4].to(device)\n",
    "            #b_mmd_symbol_ = batch[5].to(device)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                \n",
    "                \n",
    "                P_ideology = model(input_ids = b_input_ids, attention_mask = b_input_mask)\n",
    "\n",
    "                ideology_loss = loss_fct(P_ideology, b_ideology.float())\n",
    "\n",
    "                loss_val = ideology_loss\n",
    "                #loss_val = torch.sum(loss, dim=0).item()\n",
    "                \n",
    "                #logits = model(input_ids = b_input_ids,attention_mask=b_input_mask)\n",
    "                \n",
    "                #loss = loss_function(logits, b_labels)\n",
    "            \n",
    "                # Accumulate the validation loss.\n",
    "                total_eval_loss += loss_val.item()\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                P_ideology = P_ideology.to('cpu')\n",
    "                b_ideology = b_ideology.to('cpu')\n",
    "\n",
    "                # Calculate the accuracy for this batch of test sentences, and\n",
    "                # accumulate it over all batches.\n",
    "                total_eval_accuracy += predict_binary(P_ideology, b_ideology)\n",
    "        \n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Accuracy: {0:.6f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        \n",
    "        if avg_val_loss < min_val_loss:\n",
    "            min_val_loss = avg_val_loss\n",
    "    \n",
    "        print(\"Avg Validation Loss: {0:.6f}\".format(avg_val_loss))\n",
    "        #print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "    \n",
    "        model_save_state = {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "    \n",
    "        es.__call__(avg_val_loss, avg_val_accuracy, model_save_state, model_save_path)\n",
    "        last_epoch = epoch_i + 1\n",
    "        if es.early_stop == True:\n",
    "            break  # early stop criterion is met, we can stop now\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    \n",
    "    \n",
    "    min_val_loss = es.val_loss_min\n",
    "    max_val_acc = es.val_acc_max\n",
    "\n",
    "    return training_stats, last_epoch, min_val_loss, max_val_acc\n",
    "\n",
    "def print_summary(training_stats):\n",
    "    # Display floats with two decimal places.\n",
    "    pd.set_option('precision', 4)\n",
    "    \n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "\n",
    "    # Create a DataFrame from our training statistics.\n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "    # Use the 'epoch' as the row index.\n",
    "    df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "    # A hack to force the column headers to wrap.\n",
    "    #df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "\n",
    "    # Display the table.\n",
    "    return df_stats\n",
    "\n",
    "def plot_results(df_stats, last_epoch):\n",
    "    # Use plot styling from seaborn.\n",
    "    sns.set(style='darkgrid')\n",
    "\n",
    "    # Increase the plot size and font size.\n",
    "    sns.set(font_scale=1.5)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "    \n",
    "    plot1 = plt.figure(1)\n",
    "    \n",
    "    plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training_Loss\")\n",
    "    plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Val_Loss\")\n",
    "\n",
    "    # Label the plot.\n",
    "    plt.title(\"Training & Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    #plt.autoscale(enable=True, axis='x')\n",
    "\n",
    "    x_ticks = []\n",
    "    for currEpoch in range(1, last_epoch+1):\n",
    "        x_ticks.append(currEpoch)\n",
    "    #plt.xticks(x_ticks)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def run_wholeprocess_fnc(tokenizer, model_current, train_path, val_path, max_len, doc_stride, batch_size, num_warmup_steps, learning_rate, seedVal):\n",
    "\n",
    "#--------------LOAD DATASETS--------------#\n",
    "\n",
    "    model_save_path = './models/BERT_SERP/bert_titleonly_ambigious'  \n",
    "    df = load_dataset_ambigious(\"./dataset/batches_cleaned/stance/FullDataset_16.09.2021.tsv\")\n",
    "    \n",
    "    #trainpath = \"./dataset/ideology/train_new.tsv\"\n",
    "    #valPath = \"./dataset/ideology/val_new.tsv\"\n",
    "    #testPath = \"./dataset/ideology/test_new.tsv\"\n",
    "    \n",
    "    trainPer = 0.8\n",
    "    valPer = 0.2\n",
    "    testPer = 0.2\n",
    "    \n",
    "    df, dfVal, dfTest = sample_dataset_stance(df, seedVal)\n",
    "    \n",
    "    #create_new_splits_and_writethem_to_csvfiles\n",
    "    #create_train_val_test_split(trainpath, valPath, testPath, testPer, valPer, seedVal)\n",
    "    \n",
    "    ##df = load_dataset(trainpath)\n",
    "    #dfVal = load_dataset(valPath)\n",
    "    #dfTest = load_dataset(testPath)\n",
    "    \n",
    "    # Report the number of sentences.\n",
    "    print('Number of training sentences: {:,}'.format(df.shape[0]))\n",
    "    print('Number of val sentences: {:,}'.format(dfVal.shape[0]))\n",
    "    print('Number of test sentences: {:,}'.format(dfTest.shape[0]))\n",
    "\n",
    "    sentencesQueryTitle_Train = []\n",
    "    sentencesQueryTitleCont_Train = []\n",
    "    stances_Train = []\n",
    "    labels_Train = []\n",
    "    \n",
    "    #--------------DATASETS-------------#\n",
    "\n",
    "    #sentencesQueryTitle_Train, sentencesQueryTitleCont_Train, sentencesQueryTitleStance_Train, sentencesQueryTitleStanceCont_Train, stances_Train, labels_Train = generate_datasets_ideology (df, tokenizer)\n",
    "    sentencesQueryTitle_Train, sentencesQueryTitleCont_Train, labels_Train = generate_datasets_ambigious(df, tokenizer)\n",
    "    \n",
    "    \n",
    "    \n",
    "    sentencesQueryTitle_Val = []\n",
    "    sentencesQueryTitleCont_Val = []\n",
    "    stances_Val = []\n",
    "    labels_Val = []\n",
    "\n",
    " \n",
    "    #sentencesQueryTitle_Val, sentencesQueryTitleCont_Val, sentencesQueryTitleStance_Val, sentencesQueryTitleStanceCont_Val, stances_Val, labels_Val = generate_datasets_ideology (dfVal, tokenizer)\n",
    "    sentencesQueryTitle_Val, sentencesQueryTitleCont_Val, labels_Val = generate_datasets_ambigious(dfVal, tokenizer)\n",
    "    \n",
    "    sentencesQueryTitle_Test = []\n",
    "    sentencesQueryTitleCont_Test = []\n",
    "    stances_Test = []\n",
    "    labels_Test = []\n",
    "   \n",
    "    #sentencesQueryTitle_Test, sentencesQueryTitleCont_Test, sentencesQueryTitleStance_Test, sentencesQueryTitleStanceCont_Test, stances_Test, labels_Test = generate_datasets_ideology (dfTest, tokenizer)\n",
    "    sentencesQueryTitle_Test, sentencesQueryTitleCont_Test, labels_Test = generate_datasets_ambigious(dfTest, tokenizer)\n",
    "    \n",
    "    print(sentencesQueryTitle_Train[0])\n",
    "\n",
    "    #--------------DATASETS-------------#\n",
    "\n",
    "    all_input_ids_Train, all_input_masks_Train  = preprocessing_for_bert(tokenizer, sentencesQueryTitleCont_Train, max_len, doc_stride)\n",
    "    all_input_ids_Val, all_input_masks_Val  = preprocessing_for_bert(tokenizer, sentencesQueryTitleCont_Val, max_len, doc_stride)\n",
    "    all_input_ids_Test, all_input_masks_Test  = preprocessing_for_bert(tokenizer, sentencesQueryTitleCont_Test, max_len, doc_stride)\n",
    "    \n",
    "    #all_input_ids_Train, all_input_masks_Train, stance_labels_Train, ideology_labels_Train = transform_sequences_longer_ideology(tokenizer, sentencesQueryTitleCont_Train, stances_Train, labels_Train, max_len, doc_stride) #train\n",
    "    #all_input_ids_Val, all_input_masks_Val, stance_labels_Val, ideology_labels_Val = transform_sequences_longer_ideology(tokenizer, sentencesQueryTitleCont_Val, stances_Val, labels_Val, max_len, doc_stride) #val\n",
    "    #all_input_ids_Test, all_input_masks_Test, stance_labels_Test, ideology_labels_Test = transform_sequences_longer_ideology(tokenizer, sentencesQueryTitleCont_Test, stances_Test, labels_Test, max_len, doc_stride) #test\n",
    "\n",
    "    model, datasetTrain, datasetVal, optimizer, scheduler = prepare_for_training_ambigious(all_input_ids_Train, all_input_masks_Train, labels_Train, all_input_ids_Val,\n",
    "                                                                                                               all_input_masks_Val, labels_Val, model_current, batch_size, epochs, num_warmup_steps, learning_rate)    \n",
    "    training_stats, last_epoch, min_val_loss, max_val_acc = train_stance(model_save_path, model, tokenizer, datasetTrain, datasetVal, epochs, batch_size, optimizer,\n",
    "                                                                          scheduler, patience, verbose, delta, seedVal)\n",
    "    \n",
    "    \n",
    "    avg_test_loss, avg_test_acc = run_test_ideology(model_save_path, all_input_ids_Test, all_input_masks_Test, stance_labels_Test, labels_Test, batch_size)\n",
    "    df_stats = print_summary(training_stats)\n",
    "    plot_results(df_stats, last_epoch)\n",
    "    \n",
    "    #--------------TRAINING-------------#\n",
    "    batch_size_cuda = 16\n",
    "    if batch_size < 16:\n",
    "        batch_size_cuda = batch_size\n",
    "        \n",
    "    num_iterations = 5\n",
    "    total_val_loss = 0.0\n",
    "    total_val_acc = 0.0\n",
    "    total_test_loss = 0.0\n",
    "    total_test_acc = 0.0\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        value = randint(0, 100)\n",
    "        seedVal = value\n",
    "        print(\"******************\")\n",
    "        print(\"This is the iteration \" + str(i))\n",
    "        \n",
    "        model_save_path = \"model_save/ideology/model_news2a_qtitle.t7\" + str(i)\n",
    "\n",
    "        model, datasetTrain, datasetVal, optimizer, scheduler = prepare_for_training(all_input_ids_Train, all_input_masks_Train, stance_labels_Train, ideology_labels_Train, all_input_ids_Val,\n",
    "                                                                                                               all_input_masks_Val, stance_labels_Val, ideology_labels_Val, model_current, batch_size_cuda, epochs, num_warmup_steps, learning_rate)    \n",
    "        training_stats, last_epoch, min_val_loss, max_val_acc = train_stance (model_save_path, model, tokenizer, datasetTrain, datasetVal, epochs, batch_size, optimizer,\n",
    "                                                                          scheduler, patience, verbose, delta, seedVal)\n",
    "\n",
    "        avg_test_loss, avg_test_acc = run_test_ideology(model_save_path, all_input_ids_Test, all_input_masks_Test, stance_labels_Test, ideology_labels_Test, batch_size)\n",
    "        df_stats = print_summary(training_stats)\n",
    "        plot_results(df_stats, last_epoch)\n",
    "        \n",
    "        total_val_loss += min_val_loss\n",
    "        total_val_acc += max_val_acc\n",
    "        total_test_loss += avg_test_loss\n",
    "        total_test_acc += avg_test_acc\n",
    "\n",
    "        print('Min Val Loss: ' + str(min_val_loss))\n",
    "        print('Max Val Acc: ' + str(max_val_acc))\n",
    "        print('Test Loss: ' + str(avg_test_loss))\n",
    "        print('Test Acc: ' + str(avg_test_acc))\n",
    "        \n",
    "        \n",
    "    print(\"******************\")\n",
    "    print('Avg Min Val Loss: ' + str(total_val_loss/num_iterations))\n",
    "    print('Avg Max Val Acc: ' + str(total_val_acc/num_iterations))\n",
    "    print('Avg Test Loss: ' + str(total_test_loss/num_iterations))\n",
    "    print('Avg Test Acc: ' + str(total_test_acc/num_iterations))\n",
    "    \n",
    "    \n",
    "    #model_to_save.save_pretrained('model_save')\n",
    "    #tokenizer.save_pretrained('model_save')\n",
    "\n",
    "    # Good practice: save your training arguments together with the trained model\n",
    "    #torch.save(args, os.path.join('model_save', 'training_args.bin'))\n",
    "    #model_args = str(max_len) + '_' + str(doc_stride) + '_' + str(batch_size) + \"_\" + str(learning_rate) + \"_warmup\" + str(num_warmup_steps) + \"_seedVal\" + str(seedVal)\n",
    "    #model_path = model_save_path + '/model_' + model_args\n",
    "    #model.save_pretrained(model_save_path)\n",
    "    #torch.save(model.state_dict(), model_path)\n",
    "\n",
    "def create_determinism(seedVal):\n",
    "    import os\n",
    "    torch.manual_seed(seedVal)\n",
    "    torch.cuda.manual_seed_all(seedVal)  \n",
    "    torch.cuda.manual_seed(seedVal)\n",
    "    np.random.seed(seedVal)\n",
    "    random.seed(seedVal)\n",
    "    #os.environ['PYTHONHASHSEED'] = str(seedVal)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    return avg_test_loss, avg_test_accuracy\n",
    "\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "def run_test_stance(model_savepath, all_input_ids_Test, all_input_masks_Test, stance_labels_Test, ideology_labels_Test, batch_size = 16):\n",
    "    #loss_fct = torch.nn.BCELoss()\n",
    "    loss_fct_relatedness = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    t_test_relatedness, t_test_stance, t_test_mmd_symbol, t_test_mmd_symbol_ = preprocess_fnc(stance_labels_Test)\n",
    "    # Create the DataLoader.\n",
    "    prediction_data = TensorDataset(all_input_ids_Test, all_input_masks_Test, t_test_relatedness, t_test_stance, t_test_mmd_symbol, t_test_mmd_symbol_)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size, num_workers=0)\n",
    "    \n",
    "    model_current = 'bert-base-uncased'\n",
    "    tokenizer = load_tokenizer(model_current)\n",
    "        \n",
    "    model = StanceDetectionClass(model_current)\n",
    "    checkpoint = torch.load(model_savepath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])    \n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  betas=(0.9, 0.999), \n",
    "                  eps=1e-08, \n",
    "                  weight_decay=1e-5,\n",
    "                  correct_bias=True\n",
    "    )\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    #model.cuda()\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_test_loss = 0.0\n",
    "    \n",
    "    total_test_accuracy = 0.0\n",
    "    predictions , true_labels = [], []\n",
    "    \n",
    "    alpha = 1.3\n",
    "    theta = 0.8\n",
    "    beta = 1e-3\n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "      #Add batch to GPU\n",
    "        \n",
    "        #batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_relatedness = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "        b_mmd_symbol = batch[4].to(device)\n",
    "        b_mmd_symbol_ = batch[5].to(device)\n",
    "  \n",
    "        # Telling the model not to compute or store gradients, saving memory and \n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():         \n",
    "            # Forward pass, calculate logit predictions\n",
    "            \n",
    "            n1 = torch.sum(b_mmd_symbol, dim=0)\n",
    "            n2 = torch.sum(b_mmd_symbol_, dim=0)\n",
    "        \n",
    "            aa = torch.reshape(b_mmd_symbol, (-1,1))\n",
    "            bb = torch.reshape(b_mmd_symbol_, (-1,1))\n",
    "            \n",
    "            theta_d_layer, P_relatedness, P_stance = model(input_ids = b_input_ids, attention_mask = b_input_mask)\n",
    "                \n",
    "            if n1 == 0:\n",
    "                d1 = torch.zeros(batch_size, 1, device = device)\n",
    "            else:\n",
    "                d1 = torch.div(torch.sum(theta_d_layer*aa, dim=1), n1)\n",
    "                \n",
    "            if n2 == 0:\n",
    "                d2 = torch.zeros(batch_size, 1, device = device)\n",
    "            else:\n",
    "                d2 = torch.div(torch.sum(theta_d_layer*bb, dim=1), n2)\n",
    "                    \n",
    "                    \n",
    "            mmd_loss = torch.sum(d1 - d2)\n",
    "                \n",
    "            \n",
    "            relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "            stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "                \n",
    "    \n",
    "            loss_test = relatedness_loss + alpha * stance_loss - beta * mmd_loss\n",
    "            total_test_loss += loss_test.item()\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            P_relatedness = P_relatedness.to('cpu')\n",
    "            b_relatedness = b_relatedness.to('cpu')\n",
    "            P_stance = P_stance.to('cpu')\n",
    "            b_labels = b_labels.to('cpu')\n",
    "\n",
    "            total_test_accuracy += predict(P_relatedness, P_stance, b_labels)\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_test_loss = total_test_loss / len(prediction_dataloader)\n",
    "    avg_test_accuracy = total_test_accuracy / len(prediction_dataloader)\n",
    "\n",
    "    return avg_test_loss, avg_test_accuracy\n",
    "\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "def run_test_ideology(model_save_path, all_input_ids_Test, all_input_masks_Test, stance_labels_Test, ideology_labels_Test, batch_size = 16):\n",
    "\n",
    "    t_ideology_labels_test = preprocess_ideology_new(stance_labels_Test, ideology_labels_Test)\n",
    "    # Create the DataLoader.\n",
    "    prediction_data = TensorDataset(all_input_ids_Test, all_input_masks_Test, t_ideology_labels_test)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "    loss_fct = torch.nn.BCELoss()\n",
    "    \n",
    "    model_current = 'bert-base-uncased'\n",
    "    tokenizer = load_tokenizer(model_current)\n",
    "        \n",
    "    model = IdeologyDetectionClass(model_current)\n",
    "    checkpoint = torch.load(model_save_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])    \n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  betas=(0.9, 0.999), \n",
    "                  eps=1e-08, \n",
    "                  weight_decay=1e-5,\n",
    "                  correct_bias=True\n",
    "    )\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "\n",
    "    model.to(device)\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_test_loss = 0.0\n",
    "    \n",
    "    total_test_accuracy = 0.0\n",
    "    predictions , true_labels = [], []\n",
    "    \n",
    "    alpha = 1.3\n",
    "    theta = 0.8\n",
    "    beta = 1e-3\n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "      #Add batch to GPU\n",
    "\n",
    "        #batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_ideologylabels = batch[2].to(device)\n",
    "  \n",
    "        # Telling the model not to compute or store gradients, saving memory and \n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():         \n",
    "            \n",
    "            \n",
    "            # Forward pass, calculate logit predictions\n",
    "            P_ideology = model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "            ideology_loss = loss_fct(P_ideology, b_ideologylabels.float())\n",
    "\n",
    "            loss = ideology_loss\n",
    "\n",
    "            #logits = outputs[0]\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            P_ideology = P_ideology.detach().cpu()\n",
    "            t_ideology_labels_test = b_ideologylabels.to('cpu')\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            total_test_loss += loss.item()\n",
    "            total_test_accuracy += predict_binary(P_ideology, t_ideology_labels_test)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_test_loss = total_test_loss / len(prediction_dataloader)\n",
    "    \n",
    "    avg_test_accuracy = total_test_accuracy / len(prediction_dataloader)\n",
    "  \n",
    "            # Store predictions and true labels\n",
    "            #predictions.append(logits)\n",
    "            #true_labels.append(label_ids)\n",
    "    #print('Test Accuracy', avg_test_accuracy)\n",
    "\n",
    "    return avg_test_loss, avg_test_accuracy\n",
    "\n",
    "### import os\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "from transformers import AutoModel\n",
    "from transformers import DistilBertModel\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "#model = \"bert-base-uncased\"\n",
    "train_path = './dataset/fnc/train'\n",
    "val_path = './dataset/fnc/test'\n",
    "\n",
    "model_save_path = './model_save/'\n",
    "model_name = 'querytitle_model_base_9'\n",
    "\n",
    "#device = run_utils()\n",
    "        \n",
    "model_base = 'bert-base-uncased'\n",
    "model_roberta = \"roberta-base\"\n",
    "model_finetuned = './models/2_a/'\n",
    "model_finetuned2 = './models/2_b/'\n",
    "model_tiny_bert = './models/tiny_bert/'\n",
    "    \n",
    "model_current = model_base\n",
    "tokenizer = load_tokenizer(model_current)\n",
    "\n",
    "max_len = 512\n",
    "doc_stride = 128\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "num_warmup_steps = 10\n",
    "learning_rate = 2e-6\n",
    "\n",
    "##-----Early Stopping\n",
    "patience = 4000\n",
    "verbose = True\n",
    "delta = 0.000001\n",
    "seedVal = 20\n",
    "\n",
    "train_flag = True\n",
    "\n",
    "if train_flag:\n",
    "    run_wholeprocess_fnc(tokenizer, model_current, train_path, val_path, max_len, doc_stride, batch_size, num_warmup_steps, learning_rate,seedVal)  \n",
    "else:\n",
    "    avg_test_loss, avg_test_acc = run_onlytest_ideology(tokenizer, model_save_path + model_name, torch.nn.BCELoss(), device)\n",
    "    print(avg_test_loss, avg_test_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
