{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4337b-c2f4-42b9-90ae-0e4a4df8f37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "        \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\",\n",
    "        \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "        \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\",\n",
    "        \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "        \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"co\",\n",
    "        \"con\", \"could\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\",\n",
    "        \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "        \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\",\n",
    "        \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\",\n",
    "        \"has\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
    "        \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\n",
    "        \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n",
    "        \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\",\n",
    "        \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"nevertheless\", \"next\", \"nine\", \"nobody\", \"now\", \"nowhere\",\n",
    "        \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
    "        \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n",
    "        \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
    "        \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\",\n",
    "        \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "        \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\",\n",
    "        \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\",\n",
    "        \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\",\n",
    "        \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
    "        \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\",\n",
    "        \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96481b79-7fac-4022-817b-78c2f108538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_train(df, dfVal, dfTest, lim_unigram):\n",
    "    claims_t = []\n",
    "    claims_track_t = {}\n",
    "    \n",
    "    id_ref = {}\n",
    "    claimID_dict = {}\n",
    "    claimIDs_t = []\n",
    "    \n",
    "    headlines_t = []\n",
    "    headlines_track_t = {}\n",
    "    \n",
    "    for index, instance in df.iterrows():\n",
    "        claim = instance['claimHeadline']\n",
    "        claim_ID = instance['claimID']\n",
    "        \n",
    "        if claim_ID not in claimID_dict:\n",
    "            claimID_dict[claim_ID] = claim\n",
    "\n",
    "    \n",
    "    for index, instance in df.iterrows():\n",
    "        claim = instance['claimHeadline']\n",
    "        claim_ID = instance['claimID']\n",
    "        \n",
    "        headline = instance['articleHeadline']\n",
    "        headline_ID = instance['articleID']\n",
    "             \n",
    "        if headline not in headlines_track_t:\n",
    "            headlines_t.append(headline)\n",
    "            headlines_track_t[headline] = 1\n",
    "        if claim_ID not in claims_track_t:\n",
    "            claims_t.append(claimID_dict[claim_ID])\n",
    "            claims_track_t[claim_ID] = 1\n",
    "            claimIDs_t.append(claim_ID)\n",
    "            \n",
    "            \n",
    "    ####################\n",
    "    claimID_dict = {}\n",
    "    for index, instance in dfVal.iterrows():\n",
    "        claim = instance['claimHeadline']\n",
    "        claim_ID = instance['claimID']\n",
    "        if claim_ID not in claimID_dict:\n",
    "            claimID_dict[claim_ID] = claim\n",
    "            \n",
    "    claims_v = []\n",
    "    claims_track_v = {}\n",
    "    \n",
    "    headlines_v = []\n",
    "    headlines_track_v = {}\n",
    "    claimIDs_v = []\n",
    "    for index, instance in dfVal.iterrows():\n",
    "        claim = instance['claimHeadline']\n",
    "        claim_ID = instance['claimID']\n",
    "        \n",
    "        headline = instance['articleHeadline']\n",
    "        headline_ID = instance['articleID']\n",
    "        \n",
    "        if headline not in headlines_track_v:\n",
    "            headlines_v.append(headline)\n",
    "            headlines_track_v[headline] = 1\n",
    "        if claim_ID not in claims_track_v:\n",
    "            claims_v.append(claimID_dict[claim_ID])\n",
    "            claims_track_v[claim_ID] = 1\n",
    "            claimIDs_v.append(claim_ID)\n",
    "            \n",
    "\n",
    "      \n",
    "    ####################\n",
    "    \n",
    "    \n",
    "    claimID_dict = {}\n",
    "    for index, instance in dfTest.iterrows():\n",
    "        claim = instance['claimHeadline']\n",
    "        claim_ID = instance['claimID']\n",
    "        if claim_ID not in claimID_dict:\n",
    "            claimID_dict[claim_ID] = claim\n",
    "            \n",
    "    claims_test = []\n",
    "    claims_track_test = {}\n",
    "    \n",
    "    headlines_test = []\n",
    "    headlines_track_test = {}\n",
    "    claimIDs_test = []\n",
    "    for index, instance in dfTest.iterrows():\n",
    "        claim = instance['claimHeadline']\n",
    "        claim_ID = instance['claimID']\n",
    "        \n",
    "        headline = instance['articleHeadline']\n",
    "        headline_ID = instance['articleID']\n",
    "            \n",
    "        if headline not in headlines_track_test:\n",
    "            headlines_test.append(headline)\n",
    "            headlines_track_test[headline] = 1\n",
    "        \n",
    "        if claim_ID not in claims_track_test:\n",
    "            claims_test.append(claimID_dict[claim_ID])\n",
    "            claims_track_test[claim_ID] = 1\n",
    "            claimIDs_test.append(claim_ID)\n",
    "\n",
    "            \n",
    "        # Create reference dictionary\n",
    "    for i, elem in enumerate(headlines_t + claimIDs_t):\n",
    "        id_ref[elem] = i\n",
    "\n",
    "        \n",
    "    # Create vectorizers and BOW and TF arrays for train set\n",
    "    bow_vectorizer = CountVectorizer(max_features=lim_unigram, stop_words=stop_words)\n",
    "    bow = bow_vectorizer.fit_transform(claims_t + headlines_t)  # Train set only\n",
    "    \n",
    "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
    "    tfreq = tfreq_vectorizer.transform(bow).toarray()  # Train set only\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=lim_unigram, stop_words=stop_words).\\\n",
    "        fit(headlines_t + claims_t + headlines_v + claims_v + headlines_test + claims_test)  # Train and test sets\n",
    "    \n",
    "    ####################\n",
    "    \n",
    "    \n",
    "    headline_tfidf_track = {}\n",
    "    claim_tfidf_track = {}\n",
    "    headline_all = []\n",
    "    claim_all = []\n",
    "    cos_track = {}\n",
    "    \n",
    "    train_set = []\n",
    "    print(len(df))\n",
    "    \n",
    "    for index, instance in df.iterrows():\n",
    "        headline = instance['articleHeadline']\n",
    "        claim = instance['claimHeadline']\n",
    "        claim_ID = instance['claimID']\n",
    "        \n",
    "        headline_all.append(headline)\n",
    "        claim_all.append(claim)\n",
    "        \n",
    "        headline_tf = tfreq[id_ref[headline]].reshape(1, -1)\n",
    "        claim_tf = tfreq[id_ref[claim_ID]].reshape(1, -1)\n",
    "        \n",
    "        if headline not in headline_tfidf_track:\n",
    "            head_tfidf = tfidf_vectorizer.transform([headline]).toarray()\n",
    "            headline_tfidf_track[headline] = head_tfidf\n",
    "        else:\n",
    "            head_tfidf = headline_tfidf_track[headline]\n",
    "            \n",
    "        if claim not in claim_tfidf_track:\n",
    "            claim_tfidf = tfidf_vectorizer.transform([claim]).toarray()\n",
    "            claim_tfidf_track[claim] = claim_tfidf\n",
    "        else:\n",
    "            claim_tfidf = claim_tfidf_track[claim]\n",
    "            \n",
    "        if (headline, claim) not in cos_track:\n",
    "            tfidf_cos = cosine_similarity(head_tfidf, claim_tfidf)[0].reshape(1, 1)\n",
    "            cos_track[(headline, claim)] = tfidf_cos\n",
    "        else:\n",
    "            tfidf_cos = cos_track[(headline, claim)]\n",
    "\n",
    "        feat_vec = np.squeeze(np.c_[headline_tf, claim_tf, tfidf_cos])\n",
    "        train_set.append(feat_vec)\n",
    "\n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, headline_all, claim_all, 'features/train_overlap.npy')\n",
    "    X_refuting = gen_or_load_feats(refuting_features, headline_all, claim_all, 'features/train_refuting.npy')\n",
    "    X_polarity = gen_or_load_feats(polarity_features, headline_all, claim_all, 'features/train_polarity.npy')\n",
    "    X_hand = gen_or_load_feats(hand_features, headline_all, claim_all, 'features/train_hand.npy')\n",
    "    \n",
    "    train_features = np.squeeze(np.c_[X_refuting, X_polarity,X_overlap,X_hand])#\n",
    "    train_set = np.squeeze(np.c_[train_set,train_features])\n",
    "    ####### preprocessing\n",
    "    train_set = np.asarray(train_set)\n",
    "    train_mean = np.mean(train_set, axis = 0)\n",
    "    train_set = train_set-train_mean\n",
    "    \n",
    "    return train_set, train_mean, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585fbf7-29a7-4401-9e4b-4c798418bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_val(dfVal, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m):\n",
    "    \n",
    "    headlines_track = {}\n",
    "    claims_track = {}\n",
    "    \n",
    "    headline_all = []\n",
    "    claim_all = []\n",
    "    cos_track = {}\n",
    "    \n",
    "    test_set = []\n",
    "    \n",
    "    for index, instance in dfVal.iterrows():\n",
    "        headline = instance['articleHeadline']\n",
    "        claim = instance['claimHeadline']\n",
    "        headline_all.append(headline)\n",
    "        claim_all.append(claim)\n",
    "        \n",
    "        if headline not in headlines_track:\n",
    "            head_bow = bow_vectorizer.transform([headline]).toarray()\n",
    "            headline_tf = tfreq_vectorizer.transform(head_bow).toarray()[0].reshape(1, -1)\n",
    "            headline_tfidf = tfidf_vectorizer.transform([headline]).toarray().reshape(1, -1)\n",
    "            headlines_track[headline] = (headline_tf, headline_tfidf)\n",
    "        else:\n",
    "            headline_tf = headlines_track[headline][0]\n",
    "            headline_tfidf = headlines_track[headline][1]\n",
    "            \n",
    "        if claim not in claims_track:\n",
    "            claim_bow = bow_vectorizer.transform([claim]).toarray()\n",
    "            claim_tf = tfreq_vectorizer.transform(claim_bow).toarray()[0].reshape(1, -1)\n",
    "            claim_tfidf = tfidf_vectorizer.transform([claim]).toarray().reshape(1, -1)\n",
    "            claims_track[headline] = (claim_tf, claim_tfidf)\n",
    "        else:\n",
    "            claim_tf = claims_track[claim][0]\n",
    "            claim_tfidf = claims_track[claim][1]\n",
    "            \n",
    "        if (headline, claim) not in cos_track:\n",
    "            tfidf_cos = cosine_similarity(headline_tfidf, claim_tfidf)[0].reshape(1, 1)\n",
    "            cos_track[(headline, claim)] = tfidf_cos\n",
    "        else:\n",
    "            tfidf_cos = cos_track[(headline, claim)]\n",
    "            \n",
    "        feat_vec = np.squeeze(np.c_[headline_tf, claim_tf, tfidf_cos])\n",
    "        test_set.append(feat_vec)\n",
    "        \n",
    "        \n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, headline_all, claim_all, 'features/val_overlap.npy')\n",
    "    X_refuting = gen_or_load_feats(refuting_features, headline_all, claim_all, 'features/val_refuting.npy')\n",
    "    X_polarity = gen_or_load_feats(polarity_features, headline_all, claim_all, 'features/val_polarity.npy')\n",
    "    X_hand = gen_or_load_feats(hand_features, headline_all, claim_all, 'features/val_hand.npy')\n",
    "    \n",
    "    test_features = np.squeeze(np.c_[X_refuting,X_polarity,X_overlap,X_hand])#\n",
    "    test_set = np.squeeze(np.c_[test_set, test_features])\n",
    "    ####### preprocessing\n",
    "    test_set = np.asarray(test_set)\n",
    "    test_set = (test_set-m)#/(std+0.0001)\n",
    "    \n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653ac4d-fa51-426f-8522-bc89eca047bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_train_serp(df, dfVal, dfTest, lim_unigram):\n",
    "    claims_t = []\n",
    "    claims_track_t = {}\n",
    "    \n",
    "    id_ref = {}\n",
    "    claimID_dict = {}\n",
    "    claimIDs_t = []\n",
    "    \n",
    "    headlines_t = []\n",
    "    headlines_track_t = {}\n",
    "    \n",
    "    for index, instance in df.iterrows():\n",
    "        claim = instance['Q']\n",
    "        claim_ID = instance['qID']\n",
    "        \n",
    "        if claim_ID not in claimID_dict:\n",
    "            claimID_dict[claim_ID] = claim\n",
    "\n",
    "    \n",
    "    for index, instance in df.iterrows():\n",
    "        claim = instance['Q']\n",
    "        claim_ID = instance['qID']\n",
    "        \n",
    "        headline = instance['title']\n",
    "             \n",
    "        if headline not in headlines_track_t:\n",
    "            headlines_t.append(headline)\n",
    "            headlines_track_t[headline] = 1\n",
    "        if claim_ID not in claims_track_t:\n",
    "            claims_t.append(claimID_dict[claim_ID])\n",
    "            claims_track_t[claim_ID] = 1\n",
    "            claimIDs_t.append(claim_ID)\n",
    "            \n",
    "            \n",
    "    ####################\n",
    "    claimID_dict = {}\n",
    "    for index, instance in dfVal.iterrows():\n",
    "        claim = instance['Q']\n",
    "        claim_ID = instance['qID']\n",
    "        if claim_ID not in claimID_dict:\n",
    "            claimID_dict[claim_ID] = claim\n",
    "            \n",
    "    claims_v = []\n",
    "    claims_track_v = {}\n",
    "    \n",
    "    headlines_v = []\n",
    "    headlines_track_v = {}\n",
    "    claimIDs_v = []\n",
    "    for index, instance in dfVal.iterrows():\n",
    "        claim = instance['Q']\n",
    "        claim_ID = instance['qID']\n",
    "        \n",
    "        headline = instance['title']\n",
    "        \n",
    "        if headline not in headlines_track_v:\n",
    "            headlines_v.append(headline)\n",
    "            headlines_track_v[headline] = 1\n",
    "        if claim_ID not in claims_track_v:\n",
    "            claims_v.append(claimID_dict[claim_ID])\n",
    "            claims_track_v[claim_ID] = 1\n",
    "            claimIDs_v.append(claim_ID)\n",
    "            \n",
    "\n",
    "      \n",
    "    ####################\n",
    "    \n",
    "    \n",
    "    claimID_dict = {}\n",
    "    for index, instance in dfTest.iterrows():\n",
    "        claim = instance['Q']\n",
    "        claim_ID = instance['qID']\n",
    "        if claim_ID not in claimID_dict:\n",
    "            claimID_dict[claim_ID] = claim\n",
    "            \n",
    "    claims_test = []\n",
    "    claims_track_test = {}\n",
    "    \n",
    "    headlines_test = []\n",
    "    headlines_track_test = {}\n",
    "    claimIDs_test = []\n",
    "    for index, instance in dfTest.iterrows():\n",
    "        claim = instance['Q']\n",
    "        claim_ID = instance['qID']\n",
    "        \n",
    "        headline = instance['title']\n",
    "            \n",
    "        if headline not in headlines_track_test:\n",
    "            headlines_test.append(headline)\n",
    "            headlines_track_test[headline] = 1\n",
    "        \n",
    "        if claim_ID not in claims_track_test:\n",
    "            claims_test.append(claimID_dict[claim_ID])\n",
    "            claims_track_test[claim_ID] = 1\n",
    "            claimIDs_test.append(claim_ID)\n",
    "\n",
    "            \n",
    "        # Create reference dictionary\n",
    "    for i, elem in enumerate(headlines_t + claimIDs_t):\n",
    "        id_ref[elem] = i\n",
    "\n",
    "        \n",
    "    # Create vectorizers and BOW and TF arrays for train set\n",
    "    bow_vectorizer = CountVectorizer(max_features=lim_unigram, stop_words=stop_words)\n",
    "    bow = bow_vectorizer.fit_transform(claims_t + headlines_t)  # Train set only\n",
    "    \n",
    "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
    "    tfreq = tfreq_vectorizer.transform(bow).toarray()  # Train set only\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=lim_unigram, stop_words=stop_words).\\\n",
    "        fit(headlines_t + claims_t + headlines_v + claims_v + headlines_test + claims_test)  # Train and test sets\n",
    "    \n",
    "    ####################\n",
    "    \n",
    "    \n",
    "    headline_tfidf_track = {}\n",
    "    claim_tfidf_track = {}\n",
    "    headline_all = []\n",
    "    claim_all = []\n",
    "    cos_track = {}\n",
    "    \n",
    "    train_set = []\n",
    "    print(len(df))\n",
    "    \n",
    "    for index, instance in df.iterrows():\n",
    "        headline = instance['title']\n",
    "        claim = instance['Q']\n",
    "        claim_ID = instance['qID']\n",
    "        \n",
    "        headline_all.append(headline)\n",
    "        claim_all.append(claim)\n",
    "        \n",
    "        headline_tf = tfreq[id_ref[headline]].reshape(1, -1)\n",
    "        claim_tf = tfreq[id_ref[claim_ID]].reshape(1, -1)\n",
    "        \n",
    "        if headline not in headline_tfidf_track:\n",
    "            head_tfidf = tfidf_vectorizer.transform([headline]).toarray()\n",
    "            headline_tfidf_track[headline] = head_tfidf\n",
    "        else:\n",
    "            head_tfidf = headline_tfidf_track[headline]\n",
    "            \n",
    "        if claim not in claim_tfidf_track:\n",
    "            claim_tfidf = tfidf_vectorizer.transform([claim]).toarray()\n",
    "            claim_tfidf_track[claim] = claim_tfidf\n",
    "        else:\n",
    "            claim_tfidf = claim_tfidf_track[claim]\n",
    "            \n",
    "        if (headline, claim) not in cos_track:\n",
    "            tfidf_cos = cosine_similarity(head_tfidf, claim_tfidf)[0].reshape(1, 1)\n",
    "            cos_track[(headline, claim)] = tfidf_cos\n",
    "        else:\n",
    "            tfidf_cos = cos_track[(headline, claim)]\n",
    "\n",
    "        feat_vec = np.squeeze(np.c_[headline_tf, claim_tf, tfidf_cos])\n",
    "        train_set.append(feat_vec)\n",
    "\n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, headline_all, claim_all, 'features_serp/train_overlap.npy')\n",
    "    X_refuting = gen_or_load_feats(refuting_features, headline_all, claim_all, 'features_serp/train_refuting.npy')\n",
    "    X_polarity = gen_or_load_feats(polarity_features, headline_all, claim_all, 'features_serp/train_polarity.npy')\n",
    "    X_hand = gen_or_load_feats(hand_features, headline_all, claim_all, 'features_serp/train_hand.npy')\n",
    "    \n",
    "    train_features = np.squeeze(np.c_[X_refuting, X_polarity,X_overlap,X_hand])#\n",
    "    train_set = np.squeeze(np.c_[train_set,train_features])\n",
    "    ####### preprocessing\n",
    "    train_set = np.asarray(train_set)\n",
    "    train_mean = np.mean(train_set, axis = 0)\n",
    "    train_set = train_set-train_mean\n",
    "    \n",
    "    return train_set, train_mean, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d587d-b040-47f8-a77c-ccf1f506c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_val_serp(dfVal, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m):\n",
    "    \n",
    "    headlines_track = {}\n",
    "    claims_track = {}\n",
    "    \n",
    "    headline_all = []\n",
    "    claim_all = []\n",
    "    cos_track = {}\n",
    "    \n",
    "    test_set = []\n",
    "    \n",
    "    for index, instance in dfVal.iterrows():\n",
    "        headline = instance['title']\n",
    "        claim = instance['Q']\n",
    "        headline_all.append(headline)\n",
    "        claim_all.append(claim)\n",
    "        \n",
    "        if headline not in headlines_track:\n",
    "            head_bow = bow_vectorizer.transform([headline]).toarray()\n",
    "            headline_tf = tfreq_vectorizer.transform(head_bow).toarray()[0].reshape(1, -1)\n",
    "            headline_tfidf = tfidf_vectorizer.transform([headline]).toarray().reshape(1, -1)\n",
    "            headlines_track[headline] = (headline_tf, headline_tfidf)\n",
    "        else:\n",
    "            headline_tf = headlines_track[headline][0]\n",
    "            headline_tfidf = headlines_track[headline][1]\n",
    "            \n",
    "        if claim not in claims_track:\n",
    "            claim_bow = bow_vectorizer.transform([claim]).toarray()\n",
    "            claim_tf = tfreq_vectorizer.transform(claim_bow).toarray()[0].reshape(1, -1)\n",
    "            claim_tfidf = tfidf_vectorizer.transform([claim]).toarray().reshape(1, -1)\n",
    "            claims_track[headline] = (claim_tf, claim_tfidf)\n",
    "        else:\n",
    "            claim_tf = claims_track[claim][0]\n",
    "            claim_tfidf = claims_track[claim][1]\n",
    "            \n",
    "        if (headline, claim) not in cos_track:\n",
    "            tfidf_cos = cosine_similarity(headline_tfidf, claim_tfidf)[0].reshape(1, 1)\n",
    "            cos_track[(headline, claim)] = tfidf_cos\n",
    "        else:\n",
    "            tfidf_cos = cos_track[(headline, claim)]\n",
    "            \n",
    "        feat_vec = np.squeeze(np.c_[headline_tf, claim_tf, tfidf_cos])\n",
    "        test_set.append(feat_vec)\n",
    "        \n",
    "        \n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, headline_all, claim_all, 'features_serp/val_overlap.npy')\n",
    "    X_refuting = gen_or_load_feats(refuting_features, headline_all, claim_all, 'features_serp/val_refuting.npy')\n",
    "    X_polarity = gen_or_load_feats(polarity_features, headline_all, claim_all, 'features_serp/val_polarity.npy')\n",
    "    X_hand = gen_or_load_feats(hand_features, headline_all, claim_all, 'features_serp/val_hand.npy')\n",
    "    \n",
    "    test_features = np.squeeze(np.c_[X_refuting,X_polarity,X_overlap,X_hand])#\n",
    "    test_set = np.squeeze(np.c_[test_set, test_features])\n",
    "    ####### preprocessing\n",
    "    test_set = np.asarray(test_set)\n",
    "    test_set = (test_set-m)#/(std+0.0001)\n",
    "    \n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439403a-b804-41db-a064-1171093c8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_test_serp(dfTest, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m):\n",
    "    \n",
    "    headlines_track = {}\n",
    "    claims_track = {}\n",
    "    \n",
    "    headline_all = []\n",
    "    claim_all = []\n",
    "    cos_track = {}\n",
    "    \n",
    "    test_set = []\n",
    "    \n",
    "    for index, instance in dfTest.iterrows():\n",
    "        headline = instance['title']\n",
    "        claim = instance['Q']\n",
    "        headline_all.append(headline)\n",
    "        claim_all.append(claim)\n",
    "        \n",
    "        if headline not in headlines_track:\n",
    "            head_bow = bow_vectorizer.transform([headline]).toarray()\n",
    "            headline_tf = tfreq_vectorizer.transform(head_bow).toarray()[0].reshape(1, -1)\n",
    "            headline_tfidf = tfidf_vectorizer.transform([headline]).toarray().reshape(1, -1)\n",
    "            headlines_track[headline] = (headline_tf, headline_tfidf)\n",
    "        else:\n",
    "            headline_tf = headlines_track[headline][0]\n",
    "            headline_tfidf = headlines_track[headline][1]\n",
    "            \n",
    "        if claim not in claims_track:\n",
    "            claim_bow = bow_vectorizer.transform([claim]).toarray()\n",
    "            claim_tf = tfreq_vectorizer.transform(claim_bow).toarray()[0].reshape(1, -1)\n",
    "            claim_tfidf = tfidf_vectorizer.transform([claim]).toarray().reshape(1, -1)\n",
    "            claims_track[headline] = (claim_tf, claim_tfidf)\n",
    "        else:\n",
    "            claim_tf = claims_track[claim][0]\n",
    "            claim_tfidf = claims_track[claim][1]\n",
    "            \n",
    "        if (headline, claim) not in cos_track:\n",
    "            tfidf_cos = cosine_similarity(headline_tfidf, claim_tfidf)[0].reshape(1, 1)\n",
    "            cos_track[(headline, claim)] = tfidf_cos\n",
    "        else:\n",
    "            tfidf_cos = cos_track[(headline, claim)]\n",
    "            \n",
    "        feat_vec = np.squeeze(np.c_[headline_tf, claim_tf, tfidf_cos])\n",
    "        test_set.append(feat_vec)\n",
    "        \n",
    "        \n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, headline_all, claim_all, 'features_serp/test_overlap.npy')\n",
    "    X_refuting = gen_or_load_feats(refuting_features, headline_all, claim_all, 'features_serp/test_refuting.npy')\n",
    "    X_polarity = gen_or_load_feats(polarity_features, headline_all, claim_all, 'features_serp/test_polarity.npy')\n",
    "    X_hand = gen_or_load_feats(hand_features, headline_all, claim_all, 'features_serp/test_hand.npy')\n",
    "    \n",
    "    test_features = np.squeeze(np.c_[X_refuting,X_polarity,X_overlap,X_hand])#\n",
    "    test_set = np.squeeze(np.c_[test_set, test_features])\n",
    "    ####### preprocessing\n",
    "    test_set = np.asarray(test_set)\n",
    "    test_set = (test_set-m)#/(std+0.0001)\n",
    "    \n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70afd89-7969-4458-aef7-6e53cd398f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_test(dfTest, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m):\n",
    "    \n",
    "    headlines_track = {}\n",
    "    claims_track = {}\n",
    "    \n",
    "    headline_all = []\n",
    "    claim_all = []\n",
    "    cos_track = {}\n",
    "    \n",
    "    test_set = []\n",
    "    \n",
    "    for index, instance in dfTest.iterrows():\n",
    "        headline = instance['articleHeadline']\n",
    "        claim = instance['claimHeadline']\n",
    "        headline_all.append(headline)\n",
    "        claim_all.append(claim)\n",
    "        \n",
    "        if headline not in headlines_track:\n",
    "            head_bow = bow_vectorizer.transform([headline]).toarray()\n",
    "            headline_tf = tfreq_vectorizer.transform(head_bow).toarray()[0].reshape(1, -1)\n",
    "            headline_tfidf = tfidf_vectorizer.transform([headline]).toarray().reshape(1, -1)\n",
    "            headlines_track[headline] = (headline_tf, headline_tfidf)\n",
    "        else:\n",
    "            headline_tf = headlines_track[headline][0]\n",
    "            headline_tfidf = headlines_track[headline][1]\n",
    "            \n",
    "        if claim not in claims_track:\n",
    "            claim_bow = bow_vectorizer.transform([claim]).toarray()\n",
    "            claim_tf = tfreq_vectorizer.transform(claim_bow).toarray()[0].reshape(1, -1)\n",
    "            claim_tfidf = tfidf_vectorizer.transform([claim]).toarray().reshape(1, -1)\n",
    "            claims_track[headline] = (claim_tf, claim_tfidf)\n",
    "        else:\n",
    "            claim_tf = claims_track[claim][0]\n",
    "            claim_tfidf = claims_track[claim][1]\n",
    "            \n",
    "        if (headline, claim) not in cos_track:\n",
    "            tfidf_cos = cosine_similarity(headline_tfidf, claim_tfidf)[0].reshape(1, 1)\n",
    "            cos_track[(headline, claim)] = tfidf_cos\n",
    "        else:\n",
    "            tfidf_cos = cos_track[(headline, claim)]\n",
    "            \n",
    "        feat_vec = np.squeeze(np.c_[headline_tf, claim_tf, tfidf_cos])\n",
    "        test_set.append(feat_vec)\n",
    "        \n",
    "        \n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, headline_all, claim_all, 'features/test_overlap.npy')\n",
    "    X_refuting = gen_or_load_feats(refuting_features, headline_all, claim_all, 'features/test_refuting.npy')\n",
    "    X_polarity = gen_or_load_feats(polarity_features, headline_all, claim_all, 'features/test_polarity.npy')\n",
    "    X_hand = gen_or_load_feats(hand_features, headline_all, claim_all, 'features/test_hand.npy')\n",
    "    \n",
    "    test_features = np.squeeze(np.c_[X_refuting,X_polarity,X_overlap,X_hand])#\n",
    "    test_set = np.squeeze(np.c_[test_set, test_features])\n",
    "    ####### preprocessing\n",
    "    test_set = np.asarray(test_set)\n",
    "    test_set = (test_set-m)#/(std+0.0001)\n",
    "    \n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f19062-bb2f-4058-a9cb-e24dd4d0584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_utils():\n",
    "    # Get the GPU device name.\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    # The device name should look like the following:\n",
    "    if device_name == '/device:GPU:0':\n",
    "        print('Found GPU at: {}'.format(device_name))\n",
    "    else:\n",
    "        raise SystemError('GPU device not found')\n",
    "\n",
    "    device = None\n",
    "    # If there's a GPU available...\n",
    "    if torch.cuda.is_available():    \n",
    "        # Tell PyTorch to use the GPU.    \n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    # If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3640605c-b740-405e-b86d-6d299fe51fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d7031-730e-4300-8bec-33c3df26fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model):\n",
    "    tokenizer = None\n",
    "    from transformers import AutoTokenizer, DistilBertTokenizer, BertTokenizer, RobertaTokenizer, AutoModelWithLMHead\n",
    "    #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    #tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a35829-89a1-43ed-8c8b-6b78ec585d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_emergent(path):\n",
    "    df = pd.read_csv(path, delimiter=',', header = 0, names=['', 'claimHeadline', 'articleHeadline', 'stance', 'articleID', 'claimID'])    \n",
    "    \n",
    "    #df[\"stance\"] = df[\"stance\"].replace({\"Pro\": \"0\", \"Agst\": \"1\", \"Neut\": \"2\", \"Not-rel\": \"3\"})\n",
    "    #df[\"stance\"] = df[\"stance\"].replace({\"Not-rel\": \"Notrel\"})\n",
    "\n",
    "    df['claimHeadline'] = df['claimHeadline'].str.lower()\n",
    "    df['articleHeadline'] = df['articleHeadline'].str.lower()\n",
    "    #df['title'] = df['title'].str.lower()\n",
    "    \n",
    "    df = df.astype({'claimHeadline': 'str'})\n",
    "    df = df.astype({'articleHeadline': 'str'})\n",
    "    df = df.astype({'articleID': 'str'})\n",
    "    df = df.astype({'claimID': 'str'})\n",
    "\n",
    "    \n",
    "    #df = df.drop('ideology', axis=1).copy(deep=True)\n",
    "    \n",
    "    print(df.iloc[0])\n",
    "    \n",
    "    print(df.dtypes)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dded75-6d7c-45ba-85c0-ad68c8383eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_stance(path):\n",
    "    df = pd.read_csv(path, delimiter='\\t', header = 0, names=['qID', 'docID', 'stance', 'ideology', 'docCont', 'Q', 'title'])    \n",
    "    \n",
    "    #df[\"stance\"] = df[\"stance\"].replace({\"Pro\": \"0\", \"Agst\": \"1\", \"Neut\": \"2\", \"Not-rel\": \"3\"})\n",
    "    #df[\"stance\"] = df[\"stance\"].replace({\"Not-rel\": \"Notrel\"})\n",
    "\n",
    "    df['docCont'] = df['docCont'].str.lower()\n",
    "    df['Q'] = df['Q'].str.lower()\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    \n",
    "    df = df.astype({'docCont': 'str'})\n",
    "    df = df.astype({'Q': 'str'})\n",
    "    df = df.astype({'title': 'str'})\n",
    "    \n",
    "    print(df['ideology'])\n",
    "    print(\"**********\")\n",
    "    \n",
    "    #df = df.drop('ideology', axis=1).copy(deep=True)\n",
    "    \n",
    "    print(df.iloc[0])\n",
    "    \n",
    "    print(df.dtypes)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a12d46-78a2-46c2-9806-d9b1adaa1531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_only_notrel_docs_emergent(df):\n",
    "    \n",
    "    df_new = df\n",
    "    \n",
    "    dict_claims = {}\n",
    "    list_claims = []\n",
    "    tot_count = df.shape[0] #old:900\n",
    "    print(tot_count)\n",
    "    for idx in range(0, tot_count):\n",
    "        my_ID = df.iloc[idx]['claimID']\n",
    "        if my_ID not in dict_claims:\n",
    "            list_claims.append(my_ID)\n",
    "            dict_claims[my_ID] = df.iloc[idx]['claimHeadline']\n",
    "    \n",
    "    distinct_claim_num = len(dict_claims)\n",
    "    for idx in range(0, tot_count):\n",
    "        random_claim_idx = randint(1, distinct_claim_num)\n",
    "        \n",
    "        old_doc_inst = df.iloc[idx]\n",
    "        \n",
    "        new_inst = old_doc_inst\n",
    "        new_claim_ID = list_claims[random_claim_idx - 1]\n",
    "\n",
    "        if new_claim_ID != old_doc_inst['claimID']:\n",
    "            new_inst['claimID'] = new_claim_ID\n",
    "            new_inst['claimHeadline'] = dict_claims[new_claim_ID]\n",
    "            new_inst['stance'] = \"not\"\n",
    "            \n",
    "            df_new = df_new.append(new_inst, ignore_index = True)\n",
    "    print(df_new.shape[0])        \n",
    "    #df_new.to_csv('./dataset/batches_cleaned/stance/Latest_Merged_Batches_NotRelatedAdded.tsv', sep='\\t', index=False)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2697cd1-a8e2-4326-a3fd-a883b032a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_more_notrel_docs(df):\n",
    "    #path = './dataset/batches_cleaned/stance/Latest_Merged_Batches.tsv'\n",
    "    \n",
    "    #df = pd.read_csv(path, delimiter='\\t', header = 0, names=['qID', 'docID', 'stance', 'docCont', 'Q', 'title'])\n",
    "    #df['docCont'] = df['docCont'].str.lower()\n",
    "    #df['Q'] = df['Q'].str.lower()\n",
    "    #df['title'] = df['title'].str.lower()\n",
    "    \n",
    "    \n",
    "    path_queries = './dataset/batches_cleaned/stance_all/queries.tsv'\n",
    "    df_queries = pd.read_csv(path_queries, delimiter='\\t', header = 0, names=['qID', 'Q'])      \n",
    "    \n",
    "    df_new = df\n",
    "    \n",
    "    tot_count = 880 #old:900\n",
    "    for idx in range(0, tot_count):\n",
    "        valueRow = randint(1, 1335)\n",
    "        valueQ = randint(1, 57)\n",
    "        \n",
    "        old_doc_inst = df.iloc[valueRow-1]\n",
    "        \n",
    "        if old_doc_inst['stance'] != \"Not-rel\":\n",
    "        \n",
    "            new_inst = old_doc_inst\n",
    "            new_q_row = df_queries.iloc[valueQ-1]\n",
    "\n",
    "            new_qID = new_q_row['qID']\n",
    "            if new_qID != old_doc_inst['qID']:\n",
    "                new_inst['qID'] = new_qID\n",
    "                new_inst['Q'] = new_q_row['Q']\n",
    "                new_inst['stance'] = \"Not-rel\"\n",
    "                new_inst['ideology'] = 'No'\n",
    "            \n",
    "                df_new = df_new.append(new_inst, ignore_index = True)\n",
    "            \n",
    "    #df_new.to_csv('./dataset/batches_cleaned/stance/Latest_Merged_Batches_NotRelatedAdded.tsv', sep='\\t', index=False)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e656c0e8-e006-426c-92f3-82d008ddd7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset_stance(df):\n",
    "    #create_determinism(seedVal)\n",
    "    \n",
    "    df_pro = df[df['stance'] == \"Pro\"]\n",
    "    df_agst = df[df['stance'] == \"Agst\"]\n",
    "    df_neut = df[df['stance'] == \"Neut\"]\n",
    "    df_na = df[df['stance'] == \"Not-rel\"]\n",
    "    \n",
    "    df_con = df[df['ideology'] == \"Con\"]\n",
    "    df_lib = df[df['ideology'] == \"Lib\"]\n",
    "    df_no = df[df['ideology'] == \"No\"]\n",
    "    \n",
    "    \n",
    "    df_neut = df_neut\n",
    "\n",
    "    \n",
    "    print(\"Pro\", df_pro.shape[0])\n",
    "    print(\"Agst\", df_agst.shape[0])\n",
    "    print(\"Neut\", df_neut.shape[0])\n",
    "    print(\"Not-rel\", df_na.shape[0])\n",
    "    \n",
    "    print(\"Con\", df_con.shape[0])\n",
    "    print(\"Lib\", df_lib.shape[0])\n",
    "    print(\"NA\", df_no.shape[0])\n",
    "    \n",
    "    df_new = df_pro.append(df_agst, ignore_index = True)\n",
    "    df_new = df_new.append(df_neut, ignore_index = True)\n",
    "    df_new = df_new.append(df_na, ignore_index = True)\n",
    "\n",
    "    y_copy = df_new['stance'].copy(deep=True)\n",
    "    X_copy = df_new.drop('stance', axis=1).copy(deep=True)\n",
    "    \n",
    "    X = pd.DataFrame (columns=['qID', 'docID', 'ideology', 'docCont' 'Q', 'title'])\n",
    "    y = pd.DataFrame (columns=['stance'])\n",
    "    \n",
    "    X = X_copy\n",
    "    y = y_copy\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
    "    \n",
    "    print(len(X_train))\n",
    "    print(len(y_train))\n",
    "    print(len(X_test))\n",
    "    print(len(y_test))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, shuffle=True)\n",
    "    \n",
    "    X_train.insert(3, \"stance\", y_train.values) \n",
    "    X_val.insert(3, \"stance\", y_val.values) \n",
    "    X_test.insert(3, \"stance\", y_test.values)\n",
    "    \n",
    "    \n",
    "    df_pro = X_train[X_train['stance'] == \"Pro\"]\n",
    "    df_agst = X_train[X_train['stance'] == \"Agst\"]\n",
    "    df_neut = X_train[X_train['stance'] == \"Neut\"]\n",
    "    df_na = X_train[X_train['stance'] == \"Not-rel\"]\n",
    "    \n",
    "    \n",
    "    print(\"****Train****\")\n",
    "    print(\"Pro\", df_pro.shape[0])\n",
    "    print(\"Agst\", df_agst.shape[0])\n",
    "    print(\"Neut\", df_neut.shape[0])\n",
    "    print(\"Not-rel\", df_na.shape[0])\n",
    "    \n",
    "    \n",
    "    df_pro = X_val[X_val['stance'] == \"Pro\"]\n",
    "    df_agst = X_val[X_val['stance'] == \"Agst\"]\n",
    "    df_neut = X_val[X_val['stance'] == \"Neut\"]\n",
    "    df_na = X_val[X_val['stance'] == \"Not-rel\"]\n",
    "    \n",
    "    print(\"****Val****\")\n",
    "    print(\"Pro\", df_pro.shape[0])\n",
    "    print(\"Agst\", df_agst.shape[0])\n",
    "    print(\"Neut\", df_neut.shape[0])\n",
    "    print(\"Not-rel\", df_na.shape[0])\n",
    "    \n",
    "    df_pro = X_test[X_test['stance'] == \"Pro\"]\n",
    "    df_agst = X_test[X_test['stance'] == \"Agst\"]\n",
    "    df_neut = X_test[X_test['stance'] == \"Neut\"]\n",
    "    df_na = X_test[X_test['stance'] == \"Not-rel\"]\n",
    "    \n",
    "    print(\"****Test****\")\n",
    "    print(\"Pro\", df_pro.shape[0])\n",
    "    print(\"Agst\", df_agst.shape[0])\n",
    "    print(\"Neut\", df_neut.shape[0])\n",
    "    print(\"Not-rel\", df_na.shape[0])\n",
    "\n",
    "    \n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb66f68-f6fe-4bbb-b514-29a470331bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset_stance_emergent(df):\n",
    "    #create_determinism(seedVal)\n",
    "    \n",
    "    df_pro = df[df['stance'] == \"for\"]\n",
    "    df_agst = df[df['stance'] == \"against\"]\n",
    "    df_neut = df[df['stance'] == \"observing\"]\n",
    "    df_na = df[df['stance'] == \"not\"]\n",
    "\n",
    "    print(\"Pro\", df_pro.shape[0])\n",
    "    print(\"Agst\", df_agst.shape[0])\n",
    "    print(\"Neut\", df_neut.shape[0])\n",
    "    print(\"Not-rel\", df_na.shape[0])\n",
    "    \n",
    "    df_new = df_pro.append(df_agst, ignore_index = True)\n",
    "    df_new = df_new.append(df_neut, ignore_index = True)\n",
    "    df_new = df_new.append(df_na, ignore_index = True)\n",
    "\n",
    "    y_copy = df_new['stance'].copy(deep=True)\n",
    "    X_copy = df_new.drop('stance', axis=1).copy(deep=True)\n",
    "    \n",
    "    X = pd.DataFrame (columns=['', 'claimHeadline', 'articleHeadline', 'articleID', 'claimID'])\n",
    "    y = pd.DataFrame (columns=['stance'])\n",
    "    \n",
    "    X = X_copy\n",
    "    y = y_copy\n",
    "    \n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
    "    \n",
    "    print(len(X_train))\n",
    "    print(len(y_train))\n",
    "    print(len(X_val))\n",
    "    print(len(y_val))\n",
    "    \n",
    "    X_train.insert(3, \"stance\", y_train.values) \n",
    "    X_val.insert(3, \"stance\", y_val.values) \n",
    "    \n",
    "    \n",
    "    df_pro = X_train[X_train['stance'] == \"for\"]\n",
    "    df_agst = X_train[X_train['stance'] == \"against\"]\n",
    "    df_neut = X_train[X_train['stance'] == \"observing\"]\n",
    "    df_na = X_train[X_train['stance'] == \"not\"]\n",
    "    \n",
    "    \n",
    "    print(\"****Train****\")\n",
    "    print(\"Pro\", df_pro.shape[0])\n",
    "    print(\"Agst\", df_agst.shape[0])\n",
    "    print(\"Neut\", df_neut.shape[0])\n",
    "    print(\"Not-rel\", df_na.shape[0])\n",
    "    \n",
    "    \n",
    "    df_pro = X_val[X_val['stance'] == \"for\"]\n",
    "    df_agst = X_val[X_val['stance'] == \"against\"]\n",
    "    df_neut = X_val[X_val['stance'] == \"observing\"]\n",
    "    df_na = X_val[X_val['stance'] == \"not\"]\n",
    "    \n",
    "    print(\"****Val****\")\n",
    "    print(\"Pro\", df_pro.shape[0])\n",
    "    print(\"Agst\", df_agst.shape[0])\n",
    "    print(\"Neut\", df_neut.shape[0])\n",
    "    print(\"Not-rel\", df_na.shape[0])\n",
    "    \n",
    "    X_train.to_csv(\"emergent_train.csv\", sep=',', index=False)\n",
    "    X_val.to_csv(\"emergent_val.csv\", sep=',', index=False)\n",
    "\n",
    "    \n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab000ae-ef96-4cb0-bb9b-cebb18ff2b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stance_emergent(s_labels):\n",
    "    t_relatedness = []\n",
    "    t_stance = []\n",
    "    \n",
    "    t_mmd_symbol = []\n",
    "    t_mmd_symbol_ = []\n",
    "    \n",
    "    #print(labels.shape)\n",
    "        \n",
    "\n",
    "    for idx, s_label in enumerate(s_labels):\n",
    "        if s_label == \"not\": #unrelated\n",
    "            t_relatedness.append([0,1])\n",
    "            t_stance.append([0,0,0,1])\n",
    "            t_mmd_symbol.append(0)\n",
    "            t_mmd_symbol_.append(1)\n",
    "        elif s_label == \"for\": #agree\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([1,0,0,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "        elif s_label == \"against\": #disagree\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([0,1,0,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "        elif s_label == \"observing\": #discuss\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([0,0,1,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "        else:\n",
    "            print(\"Error-labels\", s_label)\n",
    "            \n",
    "    \n",
    "    t_relatedness = torch.as_tensor(t_relatedness, dtype=torch.int32)\n",
    "    t_stance = torch.as_tensor(t_stance, dtype=torch.int32)\n",
    "    \n",
    "    t_mmd_symbol  = torch.as_tensor(t_mmd_symbol, dtype=torch.float32)\n",
    "    t_mmd_symbol_ = torch.as_tensor(t_mmd_symbol_, dtype=torch.float32)\n",
    "    \n",
    "    return t_relatedness, t_stance, t_mmd_symbol, t_mmd_symbol_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd23b46-9507-4cfb-bac6-4eafb2e4a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stance_emergent_test(s_labels):\n",
    "    t_relatedness = []\n",
    "    t_stance = []\n",
    "    \n",
    "    t_mmd_symbol = []\n",
    "    t_mmd_symbol_ = []\n",
    "    \n",
    "    #print(labels.shape)\n",
    "        \n",
    "\n",
    "    for idx, s_label in enumerate(s_labels):\n",
    "        if s_label == \"not\": #unrelated\n",
    "            t_relatedness.append([0,1])\n",
    "            t_stance.append([0,0,0,1])\n",
    "            t_mmd_symbol.append(0)\n",
    "            t_mmd_symbol_.append(1)\n",
    "        elif s_label == \"for\": #agree\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([1,0,0,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "        elif s_label == \"against\": #disagree\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([0,1,0,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "        elif s_label == \"observing\": #discuss\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([0,0,1,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "        else:\n",
    "            print(\"Error-labels\", s_label)\n",
    "            \n",
    "    \n",
    "    t_relatedness = torch.as_tensor(t_relatedness, dtype=torch.int32)\n",
    "    t_stance = torch.as_tensor(t_stance, dtype=torch.int32)\n",
    "    \n",
    "    t_mmd_symbol  = torch.as_tensor(t_mmd_symbol, dtype=torch.float32)\n",
    "    t_mmd_symbol_ = torch.as_tensor(t_mmd_symbol_, dtype=torch.float32)\n",
    "    \n",
    "    return t_relatedness, t_stance, t_mmd_symbol, t_mmd_symbol_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7074a9e-8601-4ed2-a074-62822c4487e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stance_serp(s_labels):\n",
    "    t_relatedness = []\n",
    "    t_stance = []\n",
    "    \n",
    "    t_mmd_symbol = []\n",
    "    t_mmd_symbol_ = []\n",
    "    \n",
    "    #print(labels.shape)\n",
    "        \n",
    "\n",
    "    for idx, s_label in enumerate(s_labels):\n",
    "        if s_label == \"Not-rel\": #unrelated\n",
    "            t_relatedness.append([0,1])\n",
    "            t_stance.append([0,0,0,-1])\n",
    "            t_mmd_symbol.append(0)\n",
    "            t_mmd_symbol_.append(1)\n",
    "        elif s_label == \"Pro\": #agree\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([1,0,0,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "        elif s_label == \"Agst\": #disagree\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([0,1,0,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "        elif s_label == \"Neut\": #discuss\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([0,0,1,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "        else:\n",
    "            print(\"Error-labels\", s_label)\n",
    "            \n",
    "    \n",
    "    t_relatedness = torch.as_tensor(t_relatedness, dtype=torch.int32)\n",
    "    t_stance = torch.as_tensor(t_stance, dtype=torch.int32)\n",
    "    \n",
    "    t_mmd_symbol  = torch.as_tensor(t_mmd_symbol, dtype=torch.float32)\n",
    "    t_mmd_symbol_ = torch.as_tensor(t_mmd_symbol_, dtype=torch.float32)\n",
    "    \n",
    "    return t_relatedness, t_stance, t_mmd_symbol, t_mmd_symbol_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a3672-b5f3-4986-b13d-fe4e1e70b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def predict_classwise_stance_emergent(P_relatedness, P_stance, stance_labels):\n",
    "\n",
    "    P_related = torch.reshape(P_relatedness[:, 0], (-1, 1))\n",
    "    P_unrelated = torch.reshape(P_relatedness[:, 1], (-1, 1))\n",
    "    \n",
    "    tmp1 = P_stance[:,:3]\n",
    "    tmp2 = torch.reshape(torch.sum(tmp1,dim=1),[-1,1])\n",
    "    tmp3 = torch.cat([tmp2,tmp2,tmp2],dim=1)\n",
    "    tmp4 = torch.cat([P_related, P_related, P_related],dim=1)\n",
    "    tmp5 = torch.div(tmp1,tmp3)\n",
    "    tmp6 = tmp5*tmp4\n",
    "    prob = torch.cat([tmp6,P_unrelated],1)#tmp6\n",
    "    \n",
    "    target_labels = torch.argmax(torch.abs(stance_labels), 1)\n",
    "    predict_labels = torch.argmax(prob, 1)\n",
    "\n",
    "\n",
    "    \n",
    "    agree_true = 0\n",
    "    agree_total = 0\n",
    "    \n",
    "    disagree_true = 0\n",
    "    disagree_total = 0\n",
    "    \n",
    "    discuss_true = 0\n",
    "    discuss_total = 0\n",
    "    \n",
    "    unrelated_true = 0\n",
    "    unrelated_total = 0\n",
    "    \n",
    "    for idx, true_label in enumerate(target_labels):\n",
    "        predict_label = predict_labels[idx] \n",
    "        if true_label == 0:\n",
    "            agree_total += 1\n",
    "            if predict_label == 0:\n",
    "                agree_true += 1\n",
    "        elif true_label == 1:\n",
    "            disagree_total += 1\n",
    "            if predict_label == 1:\n",
    "                disagree_true += 1\n",
    "        elif true_label == 2:\n",
    "            discuss_total += 1\n",
    "            if predict_label == 2:\n",
    "                discuss_true += 1\n",
    "        elif true_label == 3:\n",
    "            unrelated_total += 1\n",
    "            if predict_label == 3:\n",
    "                unrelated_true += 1\n",
    "    accuracy_agree = 0\n",
    "    if agree_true != 0:\n",
    "        accuracy_agree = agree_true/agree_total\n",
    "        \n",
    "    accuracy_disagree = 0\n",
    "    if disagree_true != 0:\n",
    "        accuracy_disagree = disagree_true/disagree_total\n",
    "    \n",
    "    accuracy_discuss = 0\n",
    "    if discuss_true != 0:\n",
    "        accuracy_discuss = discuss_true/discuss_total\n",
    "        \n",
    "    accuracy_notrelated = 0\n",
    "    if unrelated_true != 0:\n",
    "        accuracy_notrelated = unrelated_true/unrelated_total\n",
    "        \n",
    "    true_predict_count = len((torch.eq(predict_labels, target_labels)).nonzero().flatten())\n",
    "    accuracy = true_predict_count / len(predict_labels)\n",
    "    \n",
    "    true_total = agree_true + disagree_true + discuss_true + unrelated_true\n",
    "    #print(\"*************\")\n",
    "    #print(\"Total True\")\n",
    "    #print(true_total)\n",
    "    \n",
    "    return accuracy, accuracy_agree, accuracy_disagree, accuracy_discuss, accuracy_notrelated, true_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57a22e-bead-4a7a-a8d1-e7c0477c1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concanListStrings(list1, list2):\n",
    "    list3 = []\n",
    "    myLen1 = len(list1)\n",
    "    if myLen1 != len(list2):\n",
    "        print(\"Length - error\")\n",
    "    for idx in range(0, myLen1):\n",
    "        list3.append(list1[idx] + \" \" + list2[idx])\n",
    "    return list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9339360-e8a9-4c29-944a-057aeaa5c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concanListStrings_sep(list1, list2):\n",
    "    list3 = []\n",
    "    myLen1 = len(list1)\n",
    "    if myLen1 != len(list2):\n",
    "        print(\"Length - error\")\n",
    "    for idx in range(0, myLen1):\n",
    "        list3.append(list1[idx] + \" [SEP] \" + list2[idx])\n",
    "\n",
    "    return list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf991a6-52d1-4da9-bdc7-fe082106c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the datasets with the different fields.\n",
    "def generate_datasets_emergent(df):\n",
    "\n",
    "    sentencesQuery = df.claimHeadline.values #claim in this case\n",
    "    sentencesTitle = df.articleHeadline.values\n",
    "    labels = df.stance.values\n",
    "\n",
    "    sentencesQueryTitle = concanListStrings_sep(sentencesQuery, sentencesTitle)\n",
    "\n",
    "    return sentencesQueryTitle, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fae28-bae3-4af4-9e39-3fd0610c42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the datasets with the different fields.\n",
    "def generate_datasets(df):\n",
    "\n",
    "    sentencesQuery = df.Q.values #claim in this case\n",
    "    sentencesTitle = df.title.values\n",
    "    labels = df.stance.values\n",
    "    labels_ideology = df.ideology.values\n",
    "    #df.ideology.values\n",
    "    \n",
    "    sentencesCont = df.docCont.values\n",
    "\n",
    "    sentencesQueryTitle = concanListStrings_sep(sentencesQuery, sentencesTitle)\n",
    "    sentencesQueryTitleCont = concanListStringsLonger(sentencesQueryTitle, sentencesCont)\n",
    "\n",
    "    return sentencesQueryTitle, sentencesQueryTitleCont, labels, labels_ideology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8de18-a83e-4d17-a33e-b2bf639a7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sequences_longer(tokenizer, docs, labels, max_len, doc_stride):\n",
    "\n",
    "#doc_stride = 6 #same as the maxLen\n",
    "#max_length = 128\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    label_map = {label : i for i, label in enumerate(labels)}\n",
    "\n",
    "    special_tokens_count = 2 #[CLS] and [SEP]\n",
    "    # For every sentence...\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels_Transformed = []\n",
    "    labels_Transformed_Ideology = []\n",
    "\n",
    "    only_get_partial_text = False\n",
    "    if doc_stride == 0:\n",
    "        only_get_partial_text = True\n",
    "        \n",
    "    checked_doc_stride_thresh = doc_stride - special_tokens_count - 1\n",
    "        \n",
    "    allDocs_len = len(docs)\n",
    "    for doc_id in range(0, allDocs_len):\n",
    "        currDoc = docs[doc_id]\n",
    "        currLabel = labels[doc_id]       \n",
    "        \n",
    "        my_idx = 0\n",
    "        if \"GIZEM\" in currDoc:\n",
    "            doc_splitted_tokens = currDoc.split(\" \")\n",
    "            my_idx = doc_splitted_tokens.index('GIZEM')\n",
    "        else:\n",
    "            doc_splitted_tokens = currDoc.split(\" \")\n",
    "        \n",
    "        #query\n",
    "        first_part_tokens = tokenizer.tokenize(' '.join(doc_splitted_tokens[0:my_idx]))\n",
    "        myTokens = tokenizer.tokenize(' '.join(doc_splitted_tokens[my_idx+1:]))\n",
    "        mytokens_maxlen = []\n",
    "\n",
    "        first_part_len = len(first_part_tokens)\n",
    "        cur_len = len(myTokens)\n",
    "        #longer than the max-len, use doc-stride\n",
    "        taken_len = max_len - first_part_len - special_tokens_count - 1\n",
    "        \n",
    "        if only_get_partial_text:\n",
    "            mytokens_maxlen.append(first_part_tokens + myTokens[0:taken_len])\n",
    "        else:\n",
    "            checked_thresh = max_len - first_part_len - special_tokens_count\n",
    "            if cur_len > checked_thresh:\n",
    "                #get first part len\n",
    "                while cur_len > checked_thresh:\n",
    "                    partialTokens = first_part_tokens + myTokens[0:taken_len]\n",
    "                    mytokens_maxlen.append(partialTokens)\n",
    "                    del myTokens[0:checked_doc_stride_thresh]\n",
    "                    cur_len = len(myTokens)\n",
    "                if cur_len > 0:\n",
    "                    mytokens_maxlen.append(first_part_tokens + myTokens)\n",
    "            else:\n",
    "                mytokens_maxlen.append(first_part_tokens + myTokens)\n",
    "\n",
    "      #Got the partitions for the current sentence\n",
    "      #Then for each partition, repeat the same process that\n",
    "      #you will fulfill for each sentence if they were in shorter length (< our maxLen: 128)\n",
    "      #print(len(mytokens_maxlen))\n",
    "      #   (2) Prepend the `[CLS]` token to the start.\n",
    "      #   (3) Append the `[SEP]` token to the end.\n",
    "        if len(mytokens_maxlen) == 1:\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                        currDoc,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "          # Add the encoded sentence to the list.    \n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "          # And its attention mask (simply differentiates padding from non-padding).\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "            label_id = label_map[currLabel]\n",
    "            labels_Transformed.append(label_id)\n",
    "        else:\n",
    "\n",
    "            for maxTokenList in mytokens_maxlen:\n",
    "          #   (4) Map tokens to their IDs.\n",
    "          #   (5) Pad or truncate the sentence to `max_length`\n",
    "          #   (6) Create attention masks for [PAD] tokens.\n",
    "                encoded_dict = tokenizer.encode_plus(\n",
    "                    maxTokenList,                      # Sentence to encode.\n",
    "                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                    max_length = max_len,           # Pad & truncate all sentences.\n",
    "                    pad_to_max_length = True,\n",
    "                    return_attention_mask = True,   # Construct attn. masks.\n",
    "                    return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "              # Add the encoded sentence to the list.    \n",
    "                input_ids.append(encoded_dict['input_ids'])\n",
    "              #print(maxTokenList)\n",
    "              #print(tokenizer.convert_tokens_to_ids(maxTokenList))\n",
    "              #print(encoded_dictTrain['input_ids'])\n",
    "        \n",
    "    \n",
    "              # And its attention mask (simply differentiates padding from non-padding).\n",
    "                attention_masks.append(encoded_dict['attention_mask'])\n",
    "                label_id = label_map[currLabel]\n",
    "                labels_Transformed.append(label_id)\n",
    "\n",
    "\n",
    "    all_input_ids = torch.cat(input_ids, dim=0)\n",
    "    all_input_mask = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels_Transformed)\n",
    "\n",
    "    return all_input_ids, all_input_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b6fb2-646f-4d1a-834a-88c7f9b942d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, RobertaModel, DistilBertModel\n",
    "class StanceDetectionUnigramClass(torch.nn.Module):\n",
    "    def __init__(self, datasetUsed):\n",
    "        super(StanceDetectionUnigramClass, self).__init__()\n",
    "        input_size = len(datasetUsed[0])\n",
    "        hidden_size_initial = 100\n",
    "        hidden_size = 100\n",
    "        mmd_size = 10\n",
    "        dropout_prob = 0.6\n",
    "        dropout_prob2 = 0.6\n",
    "        relatedness_size = 2\n",
    "        classes_size = 4\n",
    "        ideology_class_size = 3\n",
    "        #agreement_size = 3\n",
    "        #self.input_pl = BertForPreTraining.from_pretrained(modelUsed) #input\n",
    "        #self.input_pl = BertModel.from_pretrained(modelUsed)\n",
    "        #self.input_pl = RobertaModel.from_pretrained(modelUsed)\n",
    "        #self.input_pl = DistilBertModel.from_pretrained(modelUsed)\n",
    "        self.l1 = torch.nn.Linear(input_size, hidden_size_initial)\n",
    "        self.l2 = torch.nn.Linear(hidden_size_initial, hidden_size)\n",
    "        self.l3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn1_hidden = torch.nn.BatchNorm1d(hidden_size_initial, momentum=0.05)\n",
    "        self.bn2_hidden = torch.nn.BatchNorm1d(hidden_size, momentum=0.05)\n",
    "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout_prob2)\n",
    "\n",
    "        self.theta_d = torch.nn.Linear(hidden_size, mmd_size)\n",
    "        self.bn1_theta = torch.nn.BatchNorm1d(mmd_size, momentum=0.05)\n",
    "        \n",
    "        self.probability = torch.nn.Linear(hidden_size, relatedness_size)\n",
    "        self.output_prob = torch.nn.Softmax(dim = 1)\n",
    "        \n",
    "        self.stance = torch.nn.Linear(hidden_size + relatedness_size - 1, classes_size)\n",
    "        self.ideology = torch.nn.Linear(hidden_size + classes_size - 2, ideology_class_size)\n",
    "        \n",
    "        #for param in self.input_pl.embeddings.parameters():\n",
    "            #param.requires_grad = False\n",
    "        \n",
    "        #for param in self.input_pl[2][0:5].parameters():\n",
    "            #param.requires_grad = False\n",
    "\n",
    "        #self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, mmd_pl, mmd_pl_):\n",
    "        relatedness_size = 2\n",
    "        classes_size = 4\n",
    "        ideology_class_size = 3\n",
    "        \n",
    "        #hidden layer\n",
    "        hidden_state = self.l1(input_ids)\n",
    "        hidden_state_normalized = self.bn1_hidden(hidden_state)\n",
    "        hidden_state_normalized = torch.nn.ReLU()(hidden_state_normalized)\n",
    "        hidden_layer= self.dropout2(hidden_state_normalized)\n",
    "    \n",
    "        #mmd layer        \n",
    "        theta_d = self.theta_d(hidden_layer)\n",
    "        theta_d_normalized = self.bn1_theta(theta_d)\n",
    "        theta_d_normalized = torch.nn.ReLU()(theta_d_normalized)\n",
    "        theta_d_layer= self.dropout2(theta_d_normalized)\n",
    "        \n",
    "        \n",
    "        n1 = torch.sum(mmd_pl, dim = 0) + 1e-10\n",
    "        n2 = torch.sum(mmd_pl_, dim = 0)  + 1e-10\n",
    "        aa = torch.reshape(mmd_pl, (-1,1))\n",
    "        bb = torch.reshape(mmd_pl_, (-1,1))\n",
    "        \n",
    "        #calculate mmd_loss                  \n",
    "        d1 = torch.div(torch.sum(torch.mul(theta_d_layer, aa), dim=1), n1)\n",
    "        d2 = torch.div(torch.sum(torch.mul(theta_d_layer, bb), dim=1), n2)\n",
    "                             \n",
    "        mmd_loss = torch.sum(d1 - d2)\n",
    "\n",
    "        #probability layer\n",
    "        relatedness_state = self.probability(hidden_layer)\n",
    "        relatedness_flat = self.dropout2(relatedness_state)\n",
    "        \n",
    "        relatedness_flat_reshaped = torch.reshape(relatedness_flat, (-1, relatedness_size))\n",
    "        P_relatedness = self.output_prob(relatedness_flat_reshaped)\n",
    "        #P_relatedness = relatedness_flat_reshaped\n",
    "        \n",
    "        P_related = torch.reshape(P_relatedness[:, 0], (-1, 1))\n",
    "        P_unrelated = torch.reshape(P_relatedness[:, 1], (-1, 1))\n",
    "        \n",
    "        #stance layer\n",
    "        concat_fea = torch.cat([hidden_layer, P_related], dim = 1)\n",
    "        stance_state = self.stance(concat_fea) #batch size x classes_size\n",
    "        stance_flat = self.dropout2(stance_state) #batch size x classes_size\n",
    "        \n",
    "        stance_flat_reshaped = torch.reshape(stance_flat, (-1, classes_size))\n",
    "        P_stance = self.output_prob(stance_flat_reshaped) \n",
    "        \n",
    "\n",
    "        return mmd_loss, P_relatedness, P_stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34c2f8-0059-4df1-ad2f-45a25c282a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, RobertaModel, DistilBertModel\n",
    "class StanceIdeologyDetectionClass(torch.nn.Module):\n",
    "    def __init__(self, modelUsed):\n",
    "        super(StanceIdeologyDetectionClass, self).__init__()\n",
    "        input_size = 768\n",
    "        hidden_size_initial = 512\n",
    "        hidden_size = 512\n",
    "        mmd_size = 10\n",
    "        dropout_prob = 0.6\n",
    "        dropout_prob2 = 0.6\n",
    "        relatedness_size = 2\n",
    "        classes_size = 4\n",
    "        ideology_class_size = 3\n",
    "        #agreement_size = 3\n",
    "        #self.input_pl = BertForPreTraining.from_pretrained(modelUsed) #input\n",
    "        #self.input_pl = BertModel.from_pretrained(modelUsed)\n",
    "        self.input_pl = RobertaModel.from_pretrained(modelUsed)\n",
    "        #self.input_pl = DistilBertModel.from_pretrained(modelUsed)\n",
    "        self.l1 = torch.nn.Linear(input_size, hidden_size_initial)\n",
    "        self.l2 = torch.nn.Linear(hidden_size_initial, hidden_size)\n",
    "        self.l3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn1_hidden = torch.nn.BatchNorm1d(hidden_size_initial, momentum=0.05)\n",
    "        self.bn2_hidden = torch.nn.BatchNorm1d(hidden_size, momentum=0.05)\n",
    "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout_prob2)\n",
    "\n",
    "        self.theta_d = torch.nn.Linear(hidden_size, mmd_size)\n",
    "        self.bn1_theta = torch.nn.BatchNorm1d(mmd_size, momentum=0.05)\n",
    "        \n",
    "        self.probability = torch.nn.Linear(hidden_size, relatedness_size)\n",
    "        self.output_prob = torch.nn.Softmax(dim = 1)\n",
    "        \n",
    "        self.stance = torch.nn.Linear(hidden_size + relatedness_size - 1, classes_size)\n",
    "        self.ideology = torch.nn.Linear(hidden_size + classes_size - 2, ideology_class_size)\n",
    "        \n",
    "        #for param in self.input_pl.embeddings.parameters():\n",
    "            #param.requires_grad = False\n",
    "        \n",
    "        #for param in self.input_pl[2][0:5].parameters():\n",
    "            #param.requires_grad = False\n",
    "\n",
    "        #self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, mmd_pl, mmd_pl_):\n",
    "        relatedness_size = 2\n",
    "        classes_size = 4\n",
    "        ideology_class_size = 3\n",
    "        \n",
    "        input_1 = self.input_pl(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        #for param in self.input_pl.parameters():\n",
    "            #param.requires_grad = False\n",
    "            \n",
    "        ##for name, param in model.named_parameters():\n",
    "        #    if 'classifier' not in name: # classifier layer\n",
    "        #        param.requires_grad = False\n",
    "\n",
    "        input_1 = input_1[0]\n",
    "        input_1 = input_1[:, 0]\n",
    "        \n",
    "        #hidden layer\n",
    "        hidden_state = self.l1(input_1)\n",
    "        hidden_state_normalized = self.bn1_hidden(hidden_state)\n",
    "        hidden_state_normalized = torch.nn.ReLU()(hidden_state_normalized)\n",
    "        hidden_layer= self.dropout2(hidden_state_normalized)\n",
    "    \n",
    "        #mmd layer        \n",
    "        theta_d = self.theta_d(hidden_layer)\n",
    "        theta_d_normalized = self.bn1_theta(theta_d)\n",
    "        theta_d_normalized = torch.nn.ReLU()(theta_d_normalized)\n",
    "        theta_d_layer= self.dropout2(theta_d_normalized)\n",
    "        \n",
    "        \n",
    "        n1 = torch.sum(mmd_pl, dim = 0) + 1e-10\n",
    "        n2 = torch.sum(mmd_pl_, dim = 0)  + 1e-10\n",
    "        aa = torch.reshape(mmd_pl, (-1,1))\n",
    "        bb = torch.reshape(mmd_pl_, (-1,1))\n",
    "        \n",
    "        #calculate mmd_loss                  \n",
    "        d1 = torch.div(torch.sum(torch.mul(theta_d_layer, aa), dim=1), n1)\n",
    "        d2 = torch.div(torch.sum(torch.mul(theta_d_layer, bb), dim=1), n2)\n",
    "                             \n",
    "        mmd_loss = torch.sum(d1 - d2)\n",
    "\n",
    "        #probability layer\n",
    "        relatedness_state = self.probability(hidden_layer)\n",
    "        relatedness_flat = self.dropout2(relatedness_state)\n",
    "        \n",
    "        relatedness_flat_reshaped = torch.reshape(relatedness_flat, (-1, relatedness_size))\n",
    "        P_relatedness = self.output_prob(relatedness_flat_reshaped)\n",
    "        #P_relatedness = relatedness_flat_reshaped\n",
    "        \n",
    "        P_related = torch.reshape(P_relatedness[:, 0], (-1, 1))\n",
    "        P_unrelated = torch.reshape(P_relatedness[:, 1], (-1, 1))\n",
    "        \n",
    "        #stance layer\n",
    "        concat_fea = torch.cat([hidden_layer, P_related], dim = 1)\n",
    "        stance_state = self.stance(concat_fea) #batch size x classes_size\n",
    "        stance_flat = self.dropout2(stance_state) #batch size x classes_size\n",
    "        \n",
    "        stance_flat_reshaped = torch.reshape(stance_flat, (-1, classes_size))\n",
    "        P_stance = self.output_prob(stance_flat_reshaped) \n",
    "        \n",
    "\n",
    "        return mmd_loss, P_relatedness, P_stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f0d0d-a584-4117-8144-d7bfaa2b80c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.val_acc_max_stance = -1\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, val_acc_stance, model_save_state, model_save_path, model, tokenizer):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, val_acc_stance, model_save_state, model_save_path, model, tokenizer)\n",
    "            self.val_acc_max_stance = val_acc_stance\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, val_acc_stance, model_save_state, model_save_path, model, tokenizer)\n",
    "            self.val_acc_max_stance = val_acc_stance\n",
    "            self.counter = 0\n",
    "\n",
    "            #self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, val_loss, val_acc_stance, model_save_state, model_save_path, model, tokenizer):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            print(f'Validation acc stance : ({self.val_acc_max_stance:.6f} --> {val_acc_stance:.6f}).  Saving model ...')\n",
    "        #torch.save(model.module.state_dict(), 'checkpoint.pt')\n",
    "        \n",
    "        torch.save(model_save_state, './model_save/emergent/model_emergentbert.t7')\n",
    "        \n",
    "        \n",
    "        #model.save_pretrained('model_save/')\n",
    "        #tokenizer.save_pretrained('model_save/')\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        #torch.save(model, './model_save/entire_model.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6960313-d14b-4a62-8a5e-45ad0bbd8c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "def prepare_for_training_stance_emergent_unigram(instancesTrain, labelsTrain, instancesVal, labelsVal, modelUsed, batch_size=16, epochs = 50, num_warmup_steps=0, learning_rate=5e-5):\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "\n",
    "    from transformers import BertForSequenceClassification, AdamW, BertConfig, RobertaConfig, AutoModelWithLMHead\n",
    "    from transformers import DistilBertForSequenceClassification, RobertaForSequenceClassification\n",
    "    \n",
    "    from torch.utils.data import DataLoader, RandomSampler\n",
    "    \n",
    "    \n",
    "    t_train_relatedness, t_train_stance, t_train_mmd_symbol, t_train_mmd_symbol_ = preprocess_stance_emergent(labelsTrain)\n",
    "    t_instancesTrain  = torch.as_tensor(instancesTrain, dtype=torch.float32)\n",
    "    datasetTrain = TensorDataset(t_instancesTrain, t_train_relatedness, t_train_stance, t_train_mmd_symbol, t_train_mmd_symbol_)\n",
    "\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "    t_val_relatedness, t_val_stance, t_val_mmd_symbol, t_val_mmd_symbol_ = preprocess_stance_emergent_test(labelsVal)\n",
    "    t_instancesVal = torch.as_tensor(instancesVal, dtype=torch.float32)\n",
    "    datasetVal = TensorDataset(t_instancesVal, t_val_relatedness, t_val_stance, t_val_mmd_symbol, t_val_mmd_symbol_)\n",
    "    \n",
    "    \n",
    "    model = StanceDetectionUnigramClass(instancesTrain)\n",
    "    \n",
    "\n",
    "    #print(model)\n",
    "    # Tell pytorch to run this model on the GPU.\n",
    "    #model.cuda()\n",
    "\n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "    # I believe the 'W' stands for 'Weight Decay fix\"\n",
    "    \n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  betas=(0.9, 0.999), \n",
    "                  eps=1e-08, \n",
    "                  weight_decay=1e-5,\n",
    "                  correct_bias=True\n",
    "               )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "            datasetTrain,  # The training samples.\n",
    "            sampler =  RandomSampler(datasetTrain), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size., \n",
    "            num_workers=8\n",
    "        )\n",
    "    batch_size = batch_size\n",
    "\n",
    "\n",
    "    from transformers import get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "    # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "    # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "    # training data.\n",
    "    epochs = epochs\n",
    "\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    schedulerOld = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "    \n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps, num_cycles = 5)\n",
    "    \n",
    "    return model, datasetTrain, datasetVal, optimizer, schedulerOld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3118cfdc-86eb-4b7e-8fe2-bad90f4cbfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "def prepare_for_training_stance_emergent(input_idsTrain, attention_masksTrain, labelsTrain, input_idsVal, attention_masksVal, labelsVal, modelUsed, batch_size=16, epochs = 50, num_warmup_steps=0, learning_rate=5e-5):\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "\n",
    "    from transformers import BertForSequenceClassification, AdamW, BertConfig, RobertaConfig, AutoModelWithLMHead\n",
    "    from transformers import DistilBertForSequenceClassification, RobertaForSequenceClassification\n",
    "    \n",
    "    from torch.utils.data import DataLoader, RandomSampler\n",
    "    \n",
    "    t_train_relatedness, t_train_stance, t_train_mmd_symbol, t_train_mmd_symbol_ = preprocess_stance_emergent(labelsTrain)\n",
    "    datasetTrain = TensorDataset(input_idsTrain, attention_masksTrain, t_train_relatedness, t_train_stance, t_train_mmd_symbol, t_train_mmd_symbol_)\n",
    "\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "    t_val_relatedness, t_val_stance, t_val_mmd_symbol, t_val_mmd_symbol_ = preprocess_stance_emergent(labelsVal)\n",
    "    datasetVal = TensorDataset(input_idsVal, attention_masksVal, t_val_relatedness, t_val_stance, t_val_mmd_symbol, t_val_mmd_symbol_)\n",
    "    \n",
    "    \n",
    "    model = StanceIdeologyDetectionClass(modelUsed)\n",
    "    \n",
    "\n",
    "    #print(model)\n",
    "    # Tell pytorch to run this model on the GPU.\n",
    "    #model.cuda()\n",
    "\n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "    # I believe the 'W' stands for 'Weight Decay fix\"\n",
    "    \n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  betas=(0.9, 0.999), \n",
    "                  eps=1e-08, \n",
    "                  weight_decay=1e-5,\n",
    "                  correct_bias=True\n",
    "               )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "            datasetTrain,  # The training samples.\n",
    "            sampler =  RandomSampler(datasetTrain), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size., \n",
    "            num_workers=8\n",
    "        )\n",
    "    batch_size = batch_size\n",
    "\n",
    "\n",
    "    from transformers import get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "    # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "    # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "    # training data.\n",
    "    epochs = epochs\n",
    "\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    schedulerOld = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "    \n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps, num_cycles = 5)\n",
    "    \n",
    "    return model, datasetTrain, datasetVal, optimizer, schedulerOld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c30fc-44f9-40bd-9742-1d59400d134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087276cd-c709-4d00-9033-ce1d76e1ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_batches_datasets(datasetTrain, datasetVal, batch_size = 16):\n",
    "    from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "        \n",
    "    # Create the DataLoaders for our training and validation sets.\n",
    "    # We'll take training samples in random order. \n",
    "    train_dataloader = DataLoader(\n",
    "            datasetTrain,  # The training samples.\n",
    "            sampler =  RandomSampler(datasetTrain), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size., \n",
    "            num_workers=8, drop_last=True\n",
    "        )\n",
    "\n",
    "    # For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "    validation_dataloader = DataLoader(\n",
    "            datasetVal, # The validation samples.\n",
    "            sampler = SequentialSampler(datasetVal), # Pull out batches sequentially.\n",
    "            batch_size = batch_size, # Evaluate with this batch size.\n",
    "            num_workers=8, drop_last=True\n",
    "        )\n",
    "    \n",
    "    \n",
    "    #validation_dataloader = DataLoader(\n",
    "    #        datasetVal, # The validation samples.\n",
    "    #        sampler = SequentialSampler(datasetVal), # Pull out batches sequentially.\n",
    "    #        batch_size = batch_size, # Evaluate with this batch size.\n",
    "    #        num_workers=0, drop_last=True\n",
    "    #)\n",
    "    \n",
    "    return train_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34780e2e-9b39-4740-a419-586e133b5510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "#from tensorboardX import SummaryWriter\n",
    "\n",
    "#import EarlyStopping\n",
    "def train_stance_emergent_unigram(model_save_path, model, tokenizer, datasetTrain, datasetVal, epochs, batch_size, optimizer, scheduler, patience, verbose, delta, seedVal, continue_train = False):\n",
    "    writer = SummaryWriter()\n",
    "    min_val_loss = 100\n",
    "    \n",
    "    relatedness_size = 2\n",
    "    classes_size = 4\n",
    "    loss_fct_relatedness = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    loss_fct_stance = torch.nn.CrossEntropyLoss()\n",
    "    #loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    alpha = 1.5\n",
    "    beta = 1e-3\n",
    "    theta = 0\n",
    "    gamma = 0\n",
    "    \n",
    "    batch_size_max_once = batch_size\n",
    "\n",
    "    if batch_size < batch_size_max_once:\n",
    "        batch_size_max_once = batch_size\n",
    "        \n",
    "    accumulation_steps = batch_size/batch_size_max_once\n",
    "    \n",
    "    es = EarlyStopping(patience,verbose, delta)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    train_dataloader, validation_dataloader = return_batches_datasets(datasetTrain, datasetVal, batch_size_max_once)\n",
    "    \n",
    "    epoch_start = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        #multi-gpu\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            \n",
    "    print(device)\n",
    "    \n",
    "    \n",
    "            \n",
    "    if continue_train:\n",
    "        checkpoint = torch.load('./model_save/fnc/model_fncbert.t7')\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    \n",
    "     #pos_weight=torch.FloatTensor ([28.36 / 0.5090]\n",
    "    \n",
    "     #pos_weight = torch.tensor([1.0, 1.0, 1.0])\n",
    "     #pos_weight = pos_weight.to(device)\n",
    "     #criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    weights_ideology = torch.tensor([10, 5, 1.5]).to(device)   \n",
    "    weights_stance = torch.tensor([0.82, 0.85, 0.67, 0.3]).to(device) \n",
    "    loss_fct_relatedness_weighted = torch.nn.BCEWithLogitsLoss(pos_weight = weights_stance)\n",
    "    \n",
    "    # For each epoch...\n",
    "    batch_epoch_count = 1\n",
    "    for epoch_i in range(epoch_start, epoch_start + epochs):\n",
    "        \n",
    "        print(\"---------Epoch----------\" + str(epoch_i))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "    \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        #print(\"\")\n",
    "        #print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        #print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        # For each batch of training data...\n",
    "        mini_batch_avg_loss = 0\n",
    "        #train_size = len(train_dataloader)\n",
    "        \n",
    "        if batch_epoch_count % 500 == 0:\n",
    "            batch_size = batch_size*2\n",
    "            accumulation_steps = int(batch_size/batch_size_max_once)\n",
    "        batch_epoch_count = batch_epoch_count + 1\n",
    "\n",
    "        #train_size = len(train_dataloader) / float(accumulation_steps)\n",
    "        \n",
    "        print(\"Batch Size: \" + str(batch_size))\n",
    "        print(float(accumulation_steps))\n",
    "        \n",
    "        #print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            with torch.autograd.detect_anomaly():\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_relatedness = batch[1].to(device)\n",
    "                b_labels = batch[2].to(device)\n",
    "                b_mmd_symbol = batch[3].to(device)\n",
    "                b_mmd_symbol_ = batch[4].to(device)\n",
    "        \n",
    "            \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "            \n",
    "\n",
    "                mmd_loss, P_relatedness, P_stance = model(input_ids = b_input_ids, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "\n",
    "            \n",
    "                relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "                stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "\n",
    "            \n",
    "                loss = relatedness_loss + alpha * stance_loss - beta * mmd_loss\n",
    "                total_train_loss += loss.item()\n",
    "            \n",
    "            \n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                #for parameter in model.parameters(): print(parameter.grad.norm())\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "                #torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "                \n",
    "                # Update parameters and take a step using the computed gradient.\n",
    "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "                # modified based on their gradients, the learning rate, etc.\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the learning rate.\n",
    "                scheduler.step()\n",
    "                \n",
    "                #for param_group in optimizer.param_groups:\n",
    "                #print(\"Learning Rate: \", optimizer.param_groups[\"lr\"])\n",
    "                \n",
    "                                \n",
    "                # Always clear any previously calculated gradients before performing a\n",
    "                # backward pass. PyTorch doesn't do this automatically because \n",
    "                # accumulating the gradients is \"convenient while training RNNs\". \n",
    "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                #total_train_loss = 0\n",
    "\n",
    "                #print(\"Iter: %02d, Loss: %4.4f\" % (step, mini_batch_avg_loss))                \n",
    "    \n",
    "        print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        #print(\"\")\n",
    "        print(\"  Average training loss: {0:.6f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        #print(\"\")\n",
    "        #print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy_stance = 0\n",
    "        total_eval_accuracy_ideology = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        \n",
    "        agree_test_accuracy = 0\n",
    "        disagree_test_accuracy = 0 \n",
    "        discuss_test_accuracy = 0 \n",
    "        unrelated_test_accuracy = 0\n",
    "        \n",
    "        con_test_acc = 0\n",
    "        lib_test_acc = 0\n",
    "        na_test_acc = 0\n",
    "        \n",
    "        total_true = 0\n",
    "        \n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "        \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_relatedness = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            b_mmd_symbol = batch[3].to(device)\n",
    "            b_mmd_symbol_ = batch[4].to(device)\n",
    "        \n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "            \n",
    "                mmd_loss, P_relatedness, P_stance = model(input_ids = b_input_ids, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "                \n",
    "                #CrossEntropy Loss\n",
    "                relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "                stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "\n",
    "                \n",
    "    \n",
    "                loss_val = relatedness_loss + alpha * stance_loss - beta * mmd_loss\n",
    "                total_eval_loss += loss_val.item()\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                P_relatedness = P_relatedness.to('cpu')\n",
    "                b_relatedness = b_relatedness.to('cpu')\n",
    "                P_stance = P_stance.to('cpu')\n",
    "                b_labels = b_labels.to('cpu')\n",
    "                \n",
    "\n",
    "                # Calculate the accuracy for this batch of test sentences, and\n",
    "                # accumulate it over all batches.\n",
    "                #total_eval_accuracy += predict(P_relatedness, P_stance, b_labels)\n",
    "                \n",
    "                \n",
    "                acc_list = predict_classwise_stance_emergent(P_relatedness, P_stance, b_labels)\n",
    "                total_eval_accuracy_stance += acc_list[0]\n",
    "                ###\n",
    "                agree_test_accuracy += acc_list[1]\n",
    "                disagree_test_accuracy += acc_list[2]\n",
    "                discuss_test_accuracy += acc_list[3]\n",
    "                unrelated_test_accuracy += acc_list[4]\n",
    "                total_true += acc_list[5]\n",
    "        \n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy_stance = total_eval_accuracy_stance / len(validation_dataloader)\n",
    "        print(\"Avg Val Accuracy Stance: {0:.6f}\".format(avg_val_accuracy_stance))\n",
    "        print(\"Total True\")\n",
    "        print(total_true)\n",
    "        print(\"*************\")\n",
    "        avg_val_agree_accuracy = agree_test_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Agree Accuracy: {0:.6f}\".format(avg_val_agree_accuracy))\n",
    "        avg_val_disagree_accuracy = disagree_test_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Disagree Accuracy: {0:.6f}\".format(avg_val_disagree_accuracy))\n",
    "        avg_val_discuss_accuracy = discuss_test_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Discuss Accuracy: {0:.6f}\".format(avg_val_discuss_accuracy))\n",
    "        avg_val_unrelated_accuracy = unrelated_test_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Unrelated Accuracy: {0:.6f}\".format(avg_val_unrelated_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        \n",
    "        if avg_val_loss < min_val_loss:\n",
    "            min_val_loss = avg_val_loss\n",
    "    \n",
    "        print(\"Avg Validation Loss: {0:.6f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Stance Accur.': avg_val_accuracy_stance,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        model_save_state = {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "    \n",
    "        es.__call__(avg_val_loss, avg_val_accuracy_stance, model_save_state, model_save_path, model, tokenizer)\n",
    "        last_epoch = epoch_i + 1\n",
    "        if es.early_stop == True:\n",
    "            break  # early stop criterion is met, we can stop now\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    \n",
    "    \n",
    "    min_val_loss = es.val_loss_min\n",
    "    max_val_acc = es.val_acc_max_stance\n",
    "\n",
    "    return training_stats, last_epoch, min_val_loss, max_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d450ae5-318c-41c3-94df-d1493118cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "#from tensorboardX import SummaryWriter\n",
    "\n",
    "#import EarlyStopping\n",
    "def train_stance_emergent(model_save_path, model, tokenizer, datasetTrain, datasetVal, epochs, batch_size, optimizer, scheduler, patience, verbose, delta, seedVal, continue_train = False):\n",
    "    writer = SummaryWriter()\n",
    "    min_val_loss = 100\n",
    "    \n",
    "    relatedness_size = 2\n",
    "    classes_size = 4\n",
    "    loss_fct_relatedness = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    loss_fct_stance = torch.nn.CrossEntropyLoss()\n",
    "    #loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    alpha = 1.3\n",
    "    beta = 1e-3\n",
    "    theta = 0\n",
    "    gamma = 0\n",
    "    \n",
    "    batch_size_max_once = 16\n",
    "\n",
    "    if batch_size < batch_size_max_once:\n",
    "        batch_size_max_once = batch_size\n",
    "        \n",
    "    accumulation_steps = batch_size/batch_size_max_once\n",
    "    \n",
    "    es = EarlyStopping(patience,verbose, delta)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    train_dataloader, validation_dataloader = return_batches_datasets(datasetTrain, datasetVal, batch_size_max_once)\n",
    "    \n",
    "    epoch_start = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        #multi-gpu\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            \n",
    "    print(device)\n",
    "    \n",
    "    \n",
    "            \n",
    "    if continue_train:\n",
    "        checkpoint = torch.load('./model_save/fnc/model_fncbert.t7')\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    \n",
    "     #pos_weight=torch.FloatTensor ([28.36 / 0.5090]\n",
    "    \n",
    "     #pos_weight = torch.tensor([1.0, 1.0, 1.0])\n",
    "     #pos_weight = pos_weight.to(device)\n",
    "     #criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    weights_ideology = torch.tensor([10, 5, 1.5]).to(device)   \n",
    "    weights_stance = torch.tensor([0.82, 0.85, 0.67, 0.3]).to(device) \n",
    "    loss_fct_relatedness_weighted = torch.nn.BCEWithLogitsLoss(pos_weight = weights_stance)\n",
    "    \n",
    "    # For each epoch...\n",
    "    batch_epoch_count = 1\n",
    "    for epoch_i in range(epoch_start, epoch_start + epochs):\n",
    "        \n",
    "        print(\"---------Epoch----------\" + str(epoch_i))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "    \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        #print(\"\")\n",
    "        #print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        #print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        # For each batch of training data...\n",
    "        mini_batch_avg_loss = 0\n",
    "        #train_size = len(train_dataloader)\n",
    "        \n",
    "        if batch_epoch_count % 500 == 0:\n",
    "            batch_size = batch_size*2\n",
    "            accumulation_steps = int(batch_size/batch_size_max_once)\n",
    "        batch_epoch_count = batch_epoch_count + 1\n",
    "\n",
    "        #train_size = len(train_dataloader) / float(accumulation_steps)\n",
    "        \n",
    "        print(\"Batch Size: \" + str(batch_size))\n",
    "        print(float(accumulation_steps))\n",
    "        \n",
    "        #print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            with torch.autograd.detect_anomaly():\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_input_mask = batch[1].to(device)\n",
    "                b_relatedness = batch[2].to(device)\n",
    "                b_labels = batch[3].to(device)\n",
    "                b_mmd_symbol = batch[4].to(device)\n",
    "                b_mmd_symbol_ = batch[5].to(device)\n",
    "        \n",
    "            \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "            \n",
    "\n",
    "                mmd_loss, P_relatedness, P_stance = model(input_ids = b_input_ids, attention_mask = b_input_mask, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "\n",
    "            \n",
    "                relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "                stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "\n",
    "            \n",
    "                loss = relatedness_loss + alpha * stance_loss - beta * mmd_loss\n",
    "                total_train_loss += loss.item()\n",
    "            \n",
    "            \n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                #for parameter in model.parameters(): print(parameter.grad.norm())\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "                #torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "                \n",
    "                # Update parameters and take a step using the computed gradient.\n",
    "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "                # modified based on their gradients, the learning rate, etc.\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the learning rate.\n",
    "                scheduler.step()\n",
    "                \n",
    "                #for param_group in optimizer.param_groups:\n",
    "                #print(\"Learning Rate: \", optimizer.param_groups[\"lr\"])\n",
    "                \n",
    "                                \n",
    "                # Always clear any previously calculated gradients before performing a\n",
    "                # backward pass. PyTorch doesn't do this automatically because \n",
    "                # accumulating the gradients is \"convenient while training RNNs\". \n",
    "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "                model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                #total_train_loss = 0\n",
    "\n",
    "                #print(\"Iter: %02d, Loss: %4.4f\" % (step, mini_batch_avg_loss))                \n",
    "    \n",
    "        print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        #print(\"\")\n",
    "        print(\"  Average training loss: {0:.6f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        #print(\"\")\n",
    "        #print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy_stance = 0\n",
    "        total_eval_accuracy_ideology = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        \n",
    "        agree_test_accuracy = 0\n",
    "        disagree_test_accuracy = 0 \n",
    "        discuss_test_accuracy = 0 \n",
    "        unrelated_test_accuracy = 0\n",
    "        \n",
    "        con_test_acc = 0\n",
    "        lib_test_acc = 0\n",
    "        na_test_acc = 0\n",
    "        \n",
    "        total_true = 0\n",
    "        \n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "        \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_relatedness = batch[2].to(device)\n",
    "            b_labels = batch[3].to(device)\n",
    "            b_mmd_symbol = batch[4].to(device)\n",
    "            b_mmd_symbol_ = batch[5].to(device)\n",
    "        \n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "            \n",
    "                mmd_loss, P_relatedness, P_stance = model(input_ids = b_input_ids, attention_mask = b_input_mask, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "                \n",
    "                #CrossEntropy Loss\n",
    "                relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "                stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "\n",
    "                \n",
    "    \n",
    "                loss_val = relatedness_loss + alpha * stance_loss - beta * mmd_loss\n",
    "                total_eval_loss += loss_val.item()\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                P_relatedness = P_relatedness.to('cpu')\n",
    "                b_relatedness = b_relatedness.to('cpu')\n",
    "                P_stance = P_stance.to('cpu')\n",
    "                b_labels = b_labels.to('cpu')\n",
    "                \n",
    "\n",
    "                # Calculate the accuracy for this batch of test sentences, and\n",
    "                # accumulate it over all batches.\n",
    "                #total_eval_accuracy += predict(P_relatedness, P_stance, b_labels)\n",
    "                \n",
    "                \n",
    "                acc_list = predict_classwise_stance_emergent(P_relatedness, P_stance, b_labels)\n",
    "                total_eval_accuracy_stance += acc_list[0]\n",
    "                ###\n",
    "                agree_test_accuracy += acc_list[1]\n",
    "                disagree_test_accuracy += acc_list[2]\n",
    "                discuss_test_accuracy += acc_list[3]\n",
    "                unrelated_test_accuracy += acc_list[4]\n",
    "                total_true += acc_list[5]\n",
    "        \n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy_stance = total_eval_accuracy_stance / len(validation_dataloader)\n",
    "        print(\"Avg Val Accuracy Stance: {0:.6f}\".format(avg_val_accuracy_stance))\n",
    "        print(\"Total True\")\n",
    "        print(total_true)\n",
    "        print(\"*************\")\n",
    "        avg_val_agree_accuracy = agree_test_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Agree Accuracy: {0:.6f}\".format(avg_val_agree_accuracy))\n",
    "        avg_val_disagree_accuracy = disagree_test_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Disagree Accuracy: {0:.6f}\".format(avg_val_disagree_accuracy))\n",
    "        avg_val_discuss_accuracy = discuss_test_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Discuss Accuracy: {0:.6f}\".format(avg_val_discuss_accuracy))\n",
    "        avg_val_unrelated_accuracy = unrelated_test_accuracy / len(validation_dataloader)\n",
    "        print(\"Avg Val Unrelated Accuracy: {0:.6f}\".format(avg_val_unrelated_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        \n",
    "        if avg_val_loss < min_val_loss:\n",
    "            min_val_loss = avg_val_loss\n",
    "    \n",
    "        print(\"Avg Validation Loss: {0:.6f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Stance Accur.': avg_val_accuracy_stance,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        model_save_state = {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "    \n",
    "        es.__call__(avg_val_loss, avg_val_accuracy_stance, model_save_state, model_save_path, model, tokenizer)\n",
    "        last_epoch = epoch_i + 1\n",
    "        if es.early_stop == True:\n",
    "            break  # early stop criterion is met, we can stop now\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    \n",
    "    \n",
    "    min_val_loss = es.val_loss_min\n",
    "    max_val_acc = es.val_acc_max_stance\n",
    "\n",
    "    return training_stats, last_epoch, min_val_loss, max_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ceceb-46d8-4555-b6ae-cfb555b18ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(training_stats):\n",
    "    # Display floats with two decimal places.\n",
    "    pd.set_option('precision', 4)\n",
    "    \n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "\n",
    "    # Create a DataFrame from our training statistics.\n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "    # Use the 'epoch' as the row index.\n",
    "    df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "    # A hack to force the column headers to wrap.\n",
    "    #df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "\n",
    "    # Display the table.\n",
    "    print(df_stats)\n",
    "    return df_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f72eb30-7193-4ffa-811e-c6cef0f6bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df_stats, last_epoch):\n",
    "    # Use plot styling from seaborn.\n",
    "    sns.set(style='darkgrid')\n",
    "\n",
    "    # Increase the plot size and font size.\n",
    "    sns.set(font_scale=1.5)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "    \n",
    "    plot1 = plt.figure(1)\n",
    "    \n",
    "    plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training_Loss\")\n",
    "    plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Val_Loss\")\n",
    "\n",
    "    # Label the plot.\n",
    "    plt.title(\"Training & Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    #plt.autoscale(enable=True, axis='x')\n",
    "    \n",
    "    plot2 = plt.figure(2)\n",
    "\n",
    "    x_ticks = []\n",
    "    for currEpoch in range(1, last_epoch+1):\n",
    "        x_ticks.append(currEpoch)\n",
    "    #plt.xticks(x_ticks)\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.plot(df_stats['Valid. Stance Accur.'], 'b-o', label=\"Valid. Stance Accur.\")\n",
    "\n",
    "    # Label the plot.\n",
    "    plt.title(\"Val Stance & Ideology Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Acc\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd466e82-19bb-4f8e-a04a-7537a1884a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wholeprocess_stance_serp_emergent_unigram(train_path, val_path, max_len, doc_stride, batch_size, num_warmup_steps, learning_rate, seedVal):\n",
    "    device = run_utils()\n",
    "    model_save_path = './model_save/emergent/model_emergentbert.t7'  \n",
    "    #model_fnc = './model_save/fnc/model_fnc.t7'    \n",
    "        \n",
    "    #model_current = 'distilbert-base-uncased'\n",
    "    #model_current = 'roberta-base'\n",
    "    model_current = 'bert-base-uncased'\n",
    "    #model_current = './models/tiny_bert/'\n",
    "    tokenizer = load_tokenizer(model_current)\n",
    "\n",
    "#--------------LOAD DATASETS--------------#\n",
    "    lim_unigram = 3000\n",
    "    \n",
    "\n",
    "    train_path = 'emergent_train.csv'\n",
    "    val_path = 'emergent_val.csv'\n",
    "    test_path = 'emergent_test.csv'\n",
    "\n",
    "    trainPer = 1.2\n",
    "    valPer = 0.2\n",
    "    testPer = 0.2\n",
    "    \n",
    "    #df_train = load_dataset_emergent(train_path)\n",
    "    #df_test = load_dataset_emergent(test_path)\n",
    "    \n",
    "    #df_added = create_only_notrel_docs_emergent(df_train)\n",
    "    #df, dfVal = sample_dataset_stance_emergent(df_added)\n",
    "    #dfTest = create_only_notrel_docs_emergent(df_test)\n",
    "    \n",
    "    #dfTest.to_csv(\"emergent_test.csv\", sep=',', index=False)\n",
    "    \n",
    "    #df = load_dataset_emergent(train_path)\n",
    "    #dfVal = load_dataset_emergent(val_path)\n",
    "    #dfTest = load_dataset_emergent(test_path)\n",
    "    \n",
    "    df_all = load_dataset_stance('./dataset/batches_cleaned/stance/Latest_Merged_Batches_Corrected.tsv')\n",
    "    #df_all_notadded = not_take_notrel_docs(df_all)\n",
    "    df_all_added = create_more_notrel_docs(df_all)\n",
    "    df, dfVal, dfTest = sample_dataset_stance(df_all_added)\n",
    "    \n",
    "    # Report the number of sentences.\n",
    "    print('Number of training sentences: {:,}'.format(df.shape[0]))\n",
    "    print('Number of val sentences: {:,}'.format(dfVal.shape[0]))\n",
    "    print('Number of test sentences: {:,}'.format(dfTest.shape[0]))\n",
    "    \n",
    "    train_set, train_mean, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer = preprocess_data_train_serp(df, dfVal, dfTest, lim_unigram)\n",
    "    val_set = preprocess_data_val_serp(dfVal, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m = 0.0001)\n",
    "    test_set = preprocess_data_test_serp(dfTest, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m = 0.0001)\n",
    "\n",
    "    labelsTrain = df['stance']\n",
    "    labelsVal = dfVal['stance']\n",
    "    labelsTest = dfTest['stance']\n",
    "    #--------------TRAINING-------------#\n",
    "    \n",
    "    model, train_dataloader, validation_dataloader, optimizer, scheduler = prepare_for_training_stance_emergent_unigram(train_set, labelsTrain, val_set, labelsVal, model_current, 16, epochs, num_warmup_steps, learning_rate)    \n",
    "    training_stats, last_epoch, min_val_loss, max_val_acc = train_stance_emergent_unigram (model_save_path, model, tokenizer, train_dataloader, validation_dataloader, epochs, batch_size, optimizer,\n",
    "                                                                          scheduler, patience, verbose, delta, seedVal, False)\n",
    "    \n",
    "    test_loss, test_acc, avg_agree_test_acc, avg_disagree_test_acc, avg_discuss_test_acc, avg_unrelated_test_acc = run_test_stance(model_save_path, test_set, labelsTest, batch_size)\n",
    "        \n",
    "    df_stats = print_summary(training_stats)\n",
    "    plot_results(df_stats, last_epoch)\n",
    "\n",
    "    print('Min Val Loss: ' + str(min_val_loss))\n",
    "    print('Max Val Acc: ' + str(max_val_acc))\n",
    "    \n",
    "    print('Test Loss: ' + str(test_loss))\n",
    "    print('Test Stance Acc: ' + str(test_acc))\n",
    "    \n",
    "    print('Test Agree Class Acc: ' + str(avg_agree_test_acc))\n",
    "    print('Test Disagree Class Acc: ' + str(avg_disagree_test_acc))\n",
    "    print('Test Discuss Class Acc: ' + str(avg_discuss_test_acc))\n",
    "    print('Test Unrelated Class Acc: ' + str(avg_unrelated_test_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e422c78-0cbf-46e5-855e-1e3bd19c413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "def run_test_stance(model_savepath, testData, stance_labels_Test, batch_size):    \n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #loss_fct = torch.nn.BCELoss()\n",
    "    loss_fct_relatedness = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    t_test_relatedness, t_test_stance, t_test_mmd_symbol, t_test_mmd_symbol_ = preprocess_stance_emergent_test(stance_labels_Test)\n",
    "    t_instancesTest  = torch.as_tensor(testData, dtype=torch.float32)\n",
    "\n",
    "    \n",
    "\n",
    "    # Create the DataLoader.\n",
    "    prediction_data = TensorDataset(t_instancesTest, t_test_relatedness, t_test_stance, t_test_mmd_symbol, t_test_mmd_symbol_)\n",
    "    #prediction_data = TensorDataset(all_input_ids_Test, all_input_masks_Test, t_test_stance)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size, num_workers=8, drop_last=True)\n",
    "    \n",
    "    #model_current = 'bert-base-uncased'\n",
    "    model_current = 'roberta-large'\n",
    "    tokenizer = load_tokenizer(model_current)\n",
    "        \n",
    "    model = StanceDetectionUnigramClass(testData)\n",
    "    checkpoint = torch.load(model_savepath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])    \n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  betas=(0.9, 0.999), \n",
    "                  eps=1e-08, \n",
    "                  weight_decay=0,\n",
    "                  correct_bias=True\n",
    "    )\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    #model.cuda()\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_test_loss = 0.0\n",
    "    \n",
    "    total_test_accuracy = 0.0\n",
    "    ####\n",
    "    agree_test_accuracy = 0.0\n",
    "    disagree_test_accuracy = 0.0\n",
    "    discuss_test_accuracy = 0.0\n",
    "    unrelated_test_accuracy = 0.0\n",
    "    predictions , true_labels = [], []\n",
    "    \n",
    "    alpha = 1.3\n",
    "    theta = 1.1\n",
    "    beta = 1e-3\n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "      #Add batch to GPU\n",
    "        \n",
    "        #batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_relatedness = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_mmd_symbol = batch[3].to(device)\n",
    "        b_mmd_symbol_ = batch[4].to(device)\n",
    "  \n",
    "\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and \n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():         \n",
    "            # Forward pass, calculate logit predictions\n",
    "            \n",
    "            n1 = torch.sum(b_mmd_symbol, dim=0)\n",
    "            n2 = torch.sum(b_mmd_symbol_, dim=0)\n",
    "        \n",
    "            aa = torch.reshape(b_mmd_symbol, (-1,1))\n",
    "            bb = torch.reshape(b_mmd_symbol_, (-1,1))\n",
    "            \n",
    "            theta_d_layer, P_relatedness, P_stance = model(input_ids = b_input_ids, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "            #P_stance = model(input_ids = b_input_ids, attention_mask = b_input_mask)\n",
    "                \n",
    "            if n1 == 0:\n",
    "                d1 = torch.zeros(batch_size, 1, device = device)\n",
    "            else:\n",
    "                d1 = torch.div(torch.sum(theta_d_layer*aa, dim=1), n1)\n",
    "                \n",
    "            if n2 == 0:\n",
    "                d2 = torch.zeros(batch_size, 1, device = device)\n",
    "            else:\n",
    "                d2 = torch.div(torch.sum(theta_d_layer*bb, dim=1), n2)\n",
    "                    \n",
    "                    \n",
    "            mmd_loss = torch.sum(d1 - d2)\n",
    "            #mmd_loss = 0\n",
    "                \n",
    "            \n",
    "            relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "            #relatedness_loss = 0\n",
    "            stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "                \n",
    "    \n",
    "            loss_test = relatedness_loss + alpha * stance_loss - beta * mmd_loss\n",
    "            #loss_test = alpha * stance_loss\n",
    "            total_test_loss += loss_test.item()\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            P_relatedness = P_relatedness.to('cpu')\n",
    "            b_relatedness = b_relatedness.to('cpu')\n",
    "            P_stance = P_stance.to('cpu')\n",
    "            b_labels = b_labels.to('cpu')\n",
    "            \n",
    "            acc_list = predict_classwise_stance_emergent(P_relatedness, P_stance, b_labels)\n",
    "            total_test_accuracy += acc_list[0]\n",
    "            ###\n",
    "            agree_test_accuracy += acc_list[1]\n",
    "            disagree_test_accuracy += acc_list[2]\n",
    "            discuss_test_accuracy += acc_list[3]\n",
    "            unrelated_test_accuracy += acc_list[4]\n",
    "            \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_test_loss = total_test_loss / len(prediction_dataloader)\n",
    "    avg_test_accuracy = total_test_accuracy / len(prediction_dataloader)\n",
    "    \n",
    "    avg_agree_test_acc = agree_test_accuracy / len(prediction_dataloader)\n",
    "    avg_disagree_test_acc = disagree_test_accuracy / len(prediction_dataloader)\n",
    "    avg_discuss_test_acc = discuss_test_accuracy / len(prediction_dataloader)\n",
    "    avg_unrelated_test_acc = unrelated_test_accuracy / len(prediction_dataloader)\n",
    "\n",
    "    return avg_test_loss, avg_test_accuracy, avg_agree_test_acc, avg_disagree_test_acc, avg_discuss_test_acc, avg_unrelated_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66175972-d594-46dd-9418-5a2f808fcbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wholeprocess_stance_serp_emergent(train_path, val_path, max_len, doc_stride, batch_size, num_warmup_steps, learning_rate, seedVal):\n",
    "    device = run_utils()\n",
    "    model_save_path = './model_save/emergent/model_emergentbert.t7'  \n",
    "    #model_fnc = './model_save/fnc/model_fnc.t7'    \n",
    "        \n",
    "    #model_current = 'distilbert-base-uncased'\n",
    "    model_current = 'roberta-base'\n",
    "    #model_current = 'bert-base-uncased'\n",
    "    #model_current = './models/tiny_bert/'\n",
    "    tokenizer = load_tokenizer(model_current)\n",
    "\n",
    "#--------------LOAD DATASETS--------------#\n",
    "\n",
    "\n",
    "    train_path = 'url-versions-2015-06-14-clean-train.csv'\n",
    "    test_path = 'url-versions-2015-06-14-clean-test.csv'\n",
    "\n",
    "    trainPer = 1.2\n",
    "    valPer = 0.2\n",
    "    testPer = 0.2\n",
    "    \n",
    "    df_train = load_dataset_emergent(train_path)\n",
    "    df_test = load_dataset_emergent(test_path)\n",
    "    \n",
    "    df_added = create_only_notrel_docs_emergent(df_train)\n",
    "    df, dfVal = sample_dataset_stance_emergent(df_added)\n",
    "    dfTest = create_only_notrel_docs_emergent(df_test)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Report the number of sentences.\n",
    "    print('Number of training sentences: {:,}'.format(df.shape[0]))\n",
    "    print('Number of val sentences: {:,}'.format(dfTest.shape[0]))\n",
    "    print('Number of test sentences: {:,}'.format(dfVal.shape[0]))\n",
    "\n",
    "    sentencesQueryCont_Train = []\n",
    "    labelsTrain = []\n",
    "\n",
    "    sentencesQueryTitle_Train, labelsTrain = generate_datasets_emergent (df)\n",
    "    \n",
    "    #print(labelsTrain)\n",
    "\n",
    "    sentencesQueryCont_Val = []\n",
    "    labelsVal = []\n",
    "\n",
    "    #--------------DATASETS-------------#\n",
    "    \n",
    "    sentencesQueryTitle_Val, labelsVal = generate_datasets_emergent (dfVal)\n",
    "    \n",
    "    \n",
    "    sentencesQueryCont_Test = []\n",
    "    labelsTest = []\n",
    "    \n",
    "    \n",
    "    sentencesQueryTitle_Test, labelsTest = generate_datasets_emergent (dfTest)\n",
    "    \n",
    "\n",
    "    all_input_ids_Train, all_input_masks_Train, labels_Train = transform_sequences_longer(tokenizer, sentencesQueryTitle_Train, labelsTrain, max_len, doc_stride) #train\n",
    "    all_input_ids_Val, all_input_masks_Val, labels_Val = transform_sequences_longer(tokenizer, sentencesQueryTitle_Val, labelsVal, max_len, doc_stride) #val\n",
    "    all_input_ids_Test, all_input_masks_Test, labels_Test = transform_sequences_longer(tokenizer, sentencesQueryTitle_Test, labelsTest, max_len, doc_stride) #test\n",
    "\n",
    "    print(labels_Train.shape)\n",
    "    print(labels_Val.shape)\n",
    "    print(labels_Test.shape)\n",
    "\n",
    "    \n",
    "    #--------------TRAINING-------------#\n",
    "    \n",
    "    model, train_dataloader, validation_dataloader, optimizer, scheduler = prepare_for_training_stance_emergent(all_input_ids_Train, all_input_masks_Train, labelsTrain, all_input_ids_Val,\n",
    "                                                                                                               all_input_masks_Val, labelsVal, model_current, 16, epochs, num_warmup_steps, learning_rate)    \n",
    "    training_stats, last_epoch, min_val_loss, max_val_acc = train_stance_emergent (model_save_path, model, tokenizer, train_dataloader, validation_dataloader, epochs, batch_size, optimizer,\n",
    "                                                                          scheduler, patience, verbose, delta, seedVal, False)\n",
    "\n",
    "    df_stats = print_summary(training_stats)\n",
    "    plot_results(df_stats, last_epoch)\n",
    "\n",
    "    print('Min Val Loss: ' + str(min_val_loss))\n",
    "    print('Max Val Acc: ' + str(max_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d14f58-3f42-415a-9660-b077ae597537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install import_ipynb\n",
    "#!pip install nltk\n",
    "#import nltk\n",
    "#nltk.download('popular')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
